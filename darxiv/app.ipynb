{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert md -> file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target: Convert markdown file -> vector database\n",
    "\n",
    "paper:{meta-json} --- pdf file_id --- paper md --- vector embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chivier/opt/miniconda3/envs/darxiv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/home/chivier/opt/miniconda3/envs/darxiv/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/chivier/opt/miniconda3/envs/darxiv/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/chivier/opt/miniconda3/envs/darxiv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9769]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "from transformers import logging\n",
    "\n",
    "# logging.set_verbosity_warning()\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "sentences = ['That is a happy person', 'That is a very happy person']\n",
    "model = SentenceTransformer('Alibaba-NLP/gte-large-en-v1.5', trust_remote_code=True)\n",
    "embeddings = model.encode(sentences)\n",
    "print(cos_sim(embeddings[0], embeddings[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_path = \"../data/ocr/ocr_2024-06-03/2402-04264_1/2402-04264_1.md\"\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def get_embeddings(sentences):\n",
    "    model = SentenceTransformer('Alibaba-NLP/gte-large-en-v1.5', trust_remote_code=True)\n",
    "    embeddings = model.encode(sentences)\n",
    "    return embeddings\n",
    "\n",
    "def convert_markdown_to_chunks(md_file, chunk_size=8000, overlap=100):\n",
    "    separators = ['\\\\n\\\\n', '\\\\n', ' ']\n",
    "    splitter = RecursiveCharacterTextSplitter(separators, chunk_size, overlap)\n",
    "    with open(md_file, 'r') as f:\n",
    "        text = f.read()\n",
    "    chunks = splitter.split_text(text)\n",
    "    return chunks\n",
    "    \n",
    "\n",
    "chunks = convert_markdown_to_chunks(md_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['# Analysis Of Hopfield Model As Associative Memory Silvestri Matteo\\n\\nDepartment of Mathematics, University of Rome \"La Sapienza\"\\n\\n## Abstract\\n\\nThis article delves into the Hopfield neural network model, drawing inspiration from biological neural systems. The exploration begins with an overview of the model\\'s foundations, incorporating insights from mechanical statistics to deepen our understanding.\\n\\nFocusing on *audio retrieval*, the study demonstrates the Hopfield model\\'s associative memory capabilities. Through practical implementation, the network is trained to retrieve different patterns.\\n\\n## 1 An Overview Of Neuroscience\\n\\nThe Hopfield model finds inspiration in the intricate connections among biological neurons in the human brain, mimicking nature\\'s efficiency in information processing. Similar to the synaptic communication in biological systems, the Hopfield network utilizes connections to store and retrieve patterns. Before delving further, we\\'ll provide a brief overview of biological neurons, shedding light on their fundamental structures.\\n\\n## 1.1 Biological Neurons\\n\\nA neuron, the fundamental building block of our nervous system, consists of three main parts:\\n\\n![0_image_0.png](0_image_0.png)\\n\\nthe dendrites, the cell body (soma), and the axon. Dendrites receive signals from other neurons, transmitting these signals to the cell body. The Cell body processes these signals and, if the input is sufficient, generates an electrical impulse. This impulse travels down the Axon, a long, slender projection, to communicate with other neurons or muscles. Synapses, the junctions between neurons, facilitate this communication by transmitting electrochemical signals to other cells. This intricate architecture enables neurons to form complex networks, laying the foundation for the remarkable functionality of our nervous system.\\n\\nEmbarking on the marvels of neural networks, the sheer scale of these intricate systems is staggering. In the human brain alone, an astounding *86 billion neurons* form a vast web of connections, orchestrating the symphony of thoughts and actions. Comparatively, the cognitive prowess of elephants shines through their expansive neural landscapes, boasting approximately 257 billion neurons—more than three times that of humans. Dolphins, renowned for their intelligence, navigate the seas with brains equipped with tens billion of neurons, contributing to their advanced problem-solving abilities. These examples illuminate the remarkable diversity and complexity of neural networks across species, and moreover show the huge quantity of neurons that are used in daily actions. In the forthcoming section, we delve into a concise overview of a neuron\\'s behavior, unraveling the fundamental processes that underlie its intricate functioning.\\n\\n## 1.2 Action Potential\\n\\nWhile our focus is on the Hopfield network model, a brief foray into the neuroscience background\\n\\n![1_image_0.png](1_image_0.png)\\n\\nis essential. In this section, we won\\'t delve into the depths of neuroscience, but rather aim to illuminate a fundamental concept: *action potential*. Understanding the behavior of neurons and the process that gives rise to a spike lays a crucial foundation. This rudimentary insight serves as a key building block, enriching our comprehension of the Neural Network models and its mathematical underpinnings. The action potential AP, a pivotal concept in neuronal function, is a neuronal phenomenon in which we see the neuron fires. Transmission of a neuronal signal is entirely dependent of the movement of ions, such as Sodium (Na+), Potassium (K+) and Chloride (CI), that are unequally distributed between the inside and the outside of the cell body. The presence and the movement of these ions creates a chemical gradient across the membrane which we define as electro-chemical gradient ECG.', 'At resting state, ECG hovers around −70mV . However, when a neuron receives a stimulus, the action potential experiences a tendency to increase. Within the biological neuron, a critical threshold exists, typically around −55mV . If ECG exceeds this threshold, then the neuron activates and the process of generating a nerve signal begins. Otherwise, the neuron is unable to fire and tends to return to its resting state. Let\\'s focus on the case where the Stimulus is strong enough to cause ECG to exceed threshold. Then the neuron activates, and the Depolarization process begins. In this state, the neuron begins to interact with ions present inside and outside the membrane in such a way that ECG continues to grow up to +40mV ;\\nthis is called an overshoot. At this point, the membrane begins to expel positive ions in order to do ECG decrease; this process is called Repolarization. Following these processes, the neuron is able to generate an electrical signal that is sent through the axon to reach the target cell. In more detail, after the depolarization process there is the so-called Refractory period.\\n\\nDuring this time segment, ECG drops below the resting-state value. This happens because the channels present in the membrane that allow the ions to cross it do not close instantaneously and therefore allow values smaller than −70mV to be reached. The neuron subsequently restabilizes and returns to a resting state. As we can see in figure 2 the trend of the ECG takes on a shape of a spike and allows us to imagine the production of an electrical signal. Navigating the intricacies of neuroscience, especially outside one\\'s specialization, can be challenging. To enhance clarity, I\\'ve included a link to a video explanation. However, two basic concepts on which the associated mathematical models are based should be clear:\\n- Cognitive capacity does not depend on any intensity, but only on *binary values* (and more specifically, by frequency).\\n\\n- There should exists a *threshold* of the network that allows to activate or not neurons.\\n\\n## 2 Artificial Neurons\\n\\nAs we transition into the realm of \"Artificial Neurons\", our focus shifts from the intricate workings of biological neurons to their mathematical counterparts. These artificial neurons serve as the foundational units in computational models, mirroring the neurological properties we\\'ve explored in the preceding section. Embodying the essence of their biological counterparts, artificial neurons encapsulate key features like activation thresholds and the generation of binary outputs, all within a mathematical framework. This section unravels the fundamental principles behind these artificial neurons, bridging the gap between neuroscience and mathematical modeling in the pursuit of understanding neural networks.\\n\\n## 2.1 Mcculloch-Pitts Model\\n\\nThe McCulloch-Pitts model, known as MP Neuron, stands as the epitome of simplicity in neural network modeling. Comprising inputs, weights, and a threshold, this foundational model captures the essence of a neuron\\'s basic functionality. Inputs convey signals, each associated with a weight, which collectively influence the neuron\\'s behavior. The critical threshold, akin to the activation threshold in biological neurons, determines whether the neuron fires or remains at rest. As we can see from the figure 3, the model can be briefly summarized in the equation\\n\\n$$y=\\\\Theta\\\\left(\\\\sum_{k=1}^{N}J_{k}S_{k}-U^{*}\\\\right)$$\\n$$\\\\mathrm{(1)}$$\\n\\nwhere Θ is the *Heaviside function*, N is number of inputs Sk, Jk are the synaptic weights and U\\n∗is the neuron threshold. This general model obviously embodies the main behavioral\\n\\n![3_image_0.png](3_image_0.png)', \"![3_image_0.png](3_image_0.png)\\n\\ncharacteristics of biological neurons. More specifically, a network of MP neurons can perform any single-out mapping M : Ω ⊂ {0, 1}\\nN → {0, 1}; for instance, any boolean function of N\\nvariables, can be expressed in terms of AND,OR,NOT operations (∧, ∨, ¬). These function can be easily implemented with a network of two MP neurons (only one in the case of ¬).\\n\\nHowever, if N is greater than 1, there is a counterexample that shows how a single layer of MP neurons is not sufficient to approximate any objective function M : XOR operation. For this reason, scientific attention has shifted to MP Multilayer Networks. Indeed, the XOR\\nfunction can be performed starting from a neural network composed of two layers, each with two MP neurons. If now we assume that we don't know the function M but we have a *training* set T S = {xi, M(xi)}\\nP\\ni=1 ⊂ Ω × {0, 1}, then there exists an algorithm knows as Perceptron that allow us to train an MP neuron in order to emulate as best as possible the target value M(·):\\nAlgorithm 1 Perceptron Learning 1: Define y = y(x) = Θ(J · x − U\\n∗) with randomly chosen parameters {Jk}\\nN\\nk=1 and U\\n∗\\n2: i = 1 3: while termination condition not reached do 4: if M(xi) = y(xi) then keep going 5: if M(xi) = 0 ∧ y(xi) = 1 then U\\n∗ = U\\n∗ + 1 , J = J − xi 6: if M(xi) = 1 ∧ y(xi) = 0 then U\\n∗ = U\\n∗ − 1 , J = J + xi 7: i = i + 1 8: end while A possible termination condition is, for example, y(xi) = M(xi) ∀xi ∈ T S.\\n\\nTherefore the algorithm ends successfully only if Ω is *linearly separable*; if not, there are methods that transform Ω into a linearly separable space in order to train the neuron correctly.\\n\\n## 2.2 Neural Networks\\n\\nFrom now on, we want to study the model of neural networks, in which there is mutual information between inputs and outputs. Let's consider a network with N neurons S1*...S*N ; we denote with Jij the synaptic weight between the neurons Si and Sj. Since the network is no longer feed-forward, we are interested in expressing the state of each individual neuron as a function of time t. More specifically, if we assume that we know the neuron states S1*(t)...S*N (t)\\nthen the behavior of i-th neuron can be expressed by the equation\\n\\n$$S_{i}(t+\\\\Delta t)=\\\\Theta\\\\left(\\\\sum_{k=1}^{N}J_{i k}S_{k}(t)-U_{i}^{*}\\\\right)$$\\n$$\\\\quad(2)$$\\n\\nwhere U\\n∗\\niis the threshold of the neuron Si. However, this modeling turns out to be a bit unrealistic, as it does not take into account the fact that the neuron's threshold could vary over time. Indeed, we consider the Stochastic Neurons\\n\\n$$\\\\left\\\\{\\\\begin{array}{l}U_{i}^{*}(t)=U_{i}^{*}-\\\\frac{T}{2}z_{i}(t)\\\\\\\\ S_{i}(t+\\\\Delta t)=\\\\Theta\\\\left(\\\\sum_{k=1}^{N}J_{i k}S_{k}(t)-U_{i}^{*}(t)\\\\right)\\\\end{array}\\\\right.\\\\tag{3}$$\\n\\nwith the *noise term* such that E (zi(t)) = 0 and E (zi(t)\\n2) = 1; the *temperature* T is a very important control parameter, which plays a central role in the computational and convergence properties of the Hopfield model. A second convenient traslation is to redefine neurons such that they have values in {+1, −1}, so called Ising Neurons\\n\\n$$\\\\left\\\\{\\\\begin{array}{l}\\\\sigma_{i}(t)=2S_{i}(t)-1\\\\\\\\ U_{i}^{*}=\\\\frac{1}{2}\\\\left(\\\\sum_{k=1}^{N}J_{i k}-h_{i}\\\\right)\\\\end{array}\\\\right.\\\\tag{4}$$\\n\\nwhere σi(t) *∈ {−*1, +1} and {hi}\\nN\\ni=1 are the biases. Let's denote the *local field* acting on the neuron σi as\\n\\n$$\\\\varphi_{i}(t):=\\\\sum_{k=1}^{N}J_{i k}\\\\sigma_{k}(t)+h_{i}$$\\n$$\\\\left({\\\\overline{{5}}}\\\\right)$$\\n$$\\\\quad(6)$$\\n\\nAfter some simple calculations, we obtain the generic formula σi(t + ∆t) = sgn (φi(t) + T zi(t)) (5)\\nwith sgn(x) = 2Θ(x) − 1; this will be the notation we will use as the article continues. The probability to find a neuron state σi(t + ∆t) can be expressed in terms of the noise distribution P(z); in the case of *symmetric distribution*, we have\\n\\n$$\\\\mathbb{P}\\\\left(\\\\sigma_{i}(t+\\\\Delta t)=\\\\pm1\\\\right)=g\\\\left(\\\\pm{\\\\frac{\\\\varphi_{i}(t)}{T}}\\\\right)\\\\equiv\\\\int_{-\\\\infty}^{\\\\pm{\\\\frac{\\\\varphi_{i}(t)}{T}}}\\\\mathcal{P}(z)\\\\,d z$$\", \"$$\\\\mathbb{P}\\\\left(\\\\sigma_{i}(t+\\\\Delta t)=\\\\pm1\\\\right)=g\\\\left(\\\\pm{\\\\frac{\\\\varphi_{i}(t)}{T}}\\\\right)\\\\equiv\\\\int_{-\\\\infty}^{\\\\pm{\\\\frac{\\\\varphi_{i}(t)}{T}}}\\\\mathcal{P}(z)\\\\,d z$$\\n\\nwhere g is the *cumulative distribution function*. A natural choice is to consider the distribution of a Standard Gaussian, whose associated cdf is g(x) = 12\\n(1 + erf( x\\n√2\\n)). Another plausible choice is\\n\\n$$\\\\left\\\\{\\\\begin{aligned}\\\\mathcal{P}(z)&=\\\\frac{1}{2}(1-\\\\operatorname{tanh}^{2}(z))\\\\\\\\ \\\\mathcal{g}(z)&=\\\\frac{1}{2}(1+\\\\operatorname{tanh}(z))\\\\end{aligned}\\\\right.$$\\n$$\\\\left(7\\\\right)$$\\n\\nAs we can see from the figure 4, the two possible choices are very similar on a numerical level.\\n\\nNotice that T controls the impact of the noise on the model; in fact, if T = 0 then the process is deterministic, while if T → ∞ then\\n\\n$$\\\\mathbb{P}\\\\left(\\\\sigma_{i}(t+\\\\Delta t)=\\\\pm1\\\\right)=g(0)={\\\\frac{1}{2}}$$\\n\\n![5_image_0.png](5_image_0.png)\\n\\nthat is, the process is fully-random. If we assume T ̸= 0, we can see from equation 6 that the microscopic laws governing the *spin vector* σ = (σ1*...σ*N ) are defined as a stochastic alignment to the local field φ = (φ1(σ)*...φ*N (σ)); as a matter of fact, if φ1(σ) > 0 then P (σ1(t + ∆t) = 1) >\\n1 2\\n, while if φ1(σ) < 0 then P (σ1(t + ∆t) = 1) <\\n1 2\\n.\\n\\n## 2.3 Noiseless Networks\\n\\nNow we focus on noiseless dynamics, which can be divided into two types:\\n- Parallel dynamics, represented by\\n\\n$$\\\\sigma_{i}(t+\\\\Delta t)=\\\\text{sgn}\\\\left(\\\\sum_{k=1}^{N}J_{ik}\\\\sigma_{k}(t)+h_{i}\\\\right)\\\\quad\\\\forall i\\\\in\\\\{1..N\\\\}\\\\tag{8}$$\\n\\n- Sequential dynamics, represented by\\n\\n$$\\\\left\\\\{\\\\begin{array}{l}\\\\mbox{choose randomly}i\\\\mbox{in}\\\\{1..N\\\\}\\\\\\\\ \\\\\\\\ \\\\sigma_{i}(t+\\\\Delta t)=\\\\mbox{sgn}\\\\left(\\\\sum_{k=1}^{N}J_{ik}\\\\sigma_{k}(t)+h_{i}\\\\right)\\\\end{array}\\\\right.\\\\tag{9}$$\\n$\\\\left(10\\\\right)^3$\\nTherefore, starting from an initial configuration σ(0), the following sequence is obtained\\n\\n$$\\\\sigma(0)\\\\to\\\\sigma(1)\\\\to\\\\sigma(2)\\\\to\\\\dots$$\\nσ(0) → σ(1) → σ(2) → ... (10)\\nand we are hopeful that the sequence tends towards an *attractor* σ\\n∗ or in a limit cycle. Parallel dynamics and sequential dynamics (with some improvements) always evolve into an attractor, that is a limit cycle of period less than or equal to 2N . However, we will focus on sequential dynamics since it is evident that, for very large N, parallel dynamics turns out to be very expensive from a computational point of view.\\n\\nProposition 1. *Let's consider a noiseless sequential dynamic that has the following properties:*\\n(i) Symmetric interactions, i.e. Jik = Jki ∀i, k ∈ {1*...N*}\\n(ii) Non-negative self-interactions, i.e. Jii ≥ 0 ∀i ∈ {1*...N*}\\n(iii) *Stationary external field* h = (h1*, ..., h*N )\\nThen the function\\n\\n$$L(\\\\mathbf{\\\\sigma};N,J,\\\\mathbf{h}):=-\\\\frac{1}{2}\\\\sum_{k,l=1}^{N}\\\\sigma_{k}J_{kl}\\\\sigma_{j}-\\\\sum_{k=1}^{N}h_{k}\\\\sigma_{k}\\\\tag{11}$$\\n7\\nis a Ljapunov function *with respect to the dynamic written above.*\\nProof. We have to show that ∆L(σ) = L(σ\\n′) − L(σ) ≤ 0 ∀σ,σ\\n′, where the two configurations can only be different on the state of neuron i. Wlog, we can consider σ\\n′ ̸= σ with σ\\n′\\ni = −σi.\\n\\nThen we have\\n\\n$$\\\\Delta L(\\\\mathbf{\\\\sigma})=-\\\\frac{1}{2}\\\\sum_{k=1}^{N}J_{ki}(\\\\sigma_{k}^{\\\\prime}\\\\sigma_{i}^{\\\\prime}-\\\\sigma_{k}\\\\sigma_{i})-\\\\frac{1}{2}\\\\sum_{l=1}^{N}J_{il}(\\\\sigma_{i}^{\\\\prime}\\\\sigma_{l}^{\\\\prime}-\\\\sigma_{i}\\\\sigma_{l})-h_{i}(\\\\sigma_{i}^{\\\\prime}-\\\\sigma_{i})$$ $$=\\\\sum_{k=1}^{N}J_{ki}\\\\sigma_{k}\\\\sigma_{i}+\\\\sum_{l=1}^{N}J_{il}\\\\sigma_{l}\\\\sigma_{l}-2J_{ii}+2h_{i}\\\\sigma_{i}$$ $$=2\\\\sigma_{i}\\\\left(\\\\sum_{k=1}^{N}J_{ki}\\\\sigma_{k}+h_{i}\\\\right)-2J_{ii}=-2\\\\bigg{|}\\\\sum_{k=1}^{N}J_{ki}\\\\sigma_{k}\\\\bigg{|}-2J_{ii}\\\\leq0$$  In the form above, the estimation is between $i$ from $i$ and $j$.  \\nwhere we used the fact that ψ sgn(ψ) = |ψ| with ψ generic function; in our case, we have ψ =PN\\nk=1 Jkiσk + hi and sgn(ψ) = σ\\n′\\ni = −σi hence the thesis.\", 'The result just demonstrated shows a key concept that underlies spin-glass models of neural networks and their application in patterns recognition: we want an *energy function* which has fixed points (previously called attractors) corresponding to the patterns we want the network to memorize. In the case of a single Pattern ξ = (ξ1, ..., ξN ) *∈ {−*1, +1}\\nN , we define the synaptic weights according to the *Hebbian rule*\\n\\n$$J_{ij}=\\\\frac{\\\\xi_{i}\\\\xi_{j}}{N}\\\\quad\\\\forall i,j=1...N\\\\tag{12}$$\\n\\nwhich is connected to the concept \"cells that fire together they wire together\". Moreover, we can assume that hi = h ∀i, because there is no reason why different neurons should be feel different external fields. Let\\'s introduce the new variables τi = ξiσi and νi = ξih; multiplying the equation 9 by ξi we obtain the dynamic\\n\\n$$\\\\tau_{i}(t+1)=\\\\mathrm{sgn}\\\\left(\\\\frac{1}{N}\\\\sum_{k=1}^{N}\\\\tau_{k}+\\\\nu_{k}\\\\right)$$\\n\\nand summing over i, we can rewrite it in terms of the *average activity* m(t) := 1N\\n\\n$$i t y~m(t):={\\\\frac{1}{N}}\\\\sum_{i=1}^{N}\\\\tau_{i}(t)$$\\n$$m(t+1)=\\\\frac{N_{+}}{N}\\\\operatorname{sgn}(m(t)+|h|)+\\\\frac{N-N_{+}}{N}\\\\operatorname{sgn}(m(t)-|h|)$$\\n\\nwhere N+ is the number of positive entries of the pattern ξ. This modeling fits perfectly with the retrieval task; indeed, the network have three possible behavior:\\n- if m(0) > |h|, m(t) = 1 then the network retrieve in 1 step the pattern ξ\\n\\n- if m(0) < −|h|, m(t) = −1 then we have retrieval in 1 step of the pattern −ξ\\n- if |m(0)| < |h|, σ tends to a configuration ξ 0 where all neurons have either + or − sign, depending on which sign prevails in the pattern; so ξ 0 = sgn PN\\ni=1 ξi 1.\\nIn other words, the function m measures the alignment between the neuronal configuration σ and the target pattern ξ. Notice that this model can reconstruct the pattern only if the initial state σ(0) is sufficiently close to the associated fixed point, i.e. m(σ(0)) > |h|; otherwise, the network finds two possible configurations associated with the other two fixed points.\\n\\nIn the case of multiple Patterns {ξ µ}\\nP\\nµ=1 with ξ µ *∈ {−*1, +1}\\nN and P > 1, we want a Ljapunov function that have stationary states corresponding to the patterns we want to store.\\n\\nFor perfect retrieval, we assume that the patterns are orthogonal, i.e. ξ µ· ξ ν = 0 ∀µ ̸= ν; therefore, if we assume that there are no self-interactions and no external fields, Hebb\\'s rule becomes\\n\\n$$J_{i k}=\\\\frac{1}{N}(1-\\\\delta_{i k})\\\\sum_{\\\\mu=1}^{P}\\\\xi_{i}^{\\\\mu}\\\\xi_{k}^{\\\\mu}\\\\tag{13}$$\\n\\nthus obtaining the Lyapunov function\\n\\n$$L(\\\\mathbf{\\\\sigma};N,P,J,\\\\mathbf{h}=0)={\\\\frac{P}{2}}-{\\\\frac{1}{2}}\\\\sum_{\\\\mu=1}^{P}\\\\left[{\\\\frac{1}{\\\\sqrt{N}}}\\\\sum_{i=1}^{N}\\\\xi_{i}^{\\\\mu}\\\\sigma_{i}\\\\right]$$\\n\\nsuch that L(σ) ≥ L(±ξ µ) ∀µ = 1*...P*, or rather all patterns (and their opposites) are stationary points of dynamics. This will be the initial setting of the Hopfield model, which is known for its associative memory capacity with P > 1 patterns.\\n\\n## 2.4 Neural Processes As Markov Chains\\n\\nNow, we want to analyse the previously dynamics in probabilistic terms using *Markov Chains*.\\n\\nIn this section, we will only state the definitions and results that are of interest to us for the purpose of translating neural dynamics in terms of Markov chains.\\n\\n## 2.4.1 Brief Overview Of Markov Chains\\n\\nDefinition 1. A discrete time Markov chain at values in a discrete set S *is a sequence of* random variables X = {Xt}t≥1 *such that*\\n\\n$$\\\\mathbb{P}(X_{t+1}=j|X_{t}=i,X_{t-1}=i_{t-1}...,X_{0}=i_{0})=\\\\mathbb{P}(X_{t+1}=j|X_{t}=i)=:W_{i j}$$\\n\\ni.e. that the probability of finding oneself in state j depends solely on the state at the previous time i.\\n\\nThe probability Wij can be interpreted as the component of a *stochastic transfer matrix* W\\nwhere\\n\\n$$\\\\sum_{j\\\\in{\\\\mathcal{S}}}W_{i j}=1\\\\;\\\\;\\\\forall i\\\\;\\\\;\\\\mathrm{and}\\\\;\\\\;W_{i j}\\\\in[0,1].$$\\n\\nTherefore, the Markov chain is in bi-univocal correspondence with its transfer matrix. Furthermore, we have', '$$\\\\sum_{j\\\\in{\\\\mathcal{S}}}W_{i j}=1\\\\;\\\\;\\\\forall i\\\\;\\\\;\\\\mathrm{and}\\\\;\\\\;W_{i j}\\\\in[0,1].$$\\n\\nTherefore, the Markov chain is in bi-univocal correspondence with its transfer matrix. Furthermore, we have\\n\\n$$p_{t}(X=j|X_{0})=\\\\left(p_{t-1}(X=j|X_{0})W\\\\right)_{i}=\\\\ldots=\\\\left(p_{0}(X=j|X_{0})W^{t}\\\\right)_{i}$$\\n\\nDefinition 2. A MC is said ergodic if there exists an integer τ *such that for all pairs of states*\\n(i, j) ∈ S × S *we have that* pt(X = j|X0 = i) > 0 ∀*t > τ.*\\nDefinition 3. A distribution p(X|X0) is invariant if p(X|X0)W = p(X|X0).\\n\\nTheorem 1. For any ergodic Markov chain X*, there exists an unique invariant distribution* p∞(X|X0) that is the principal left eigenvector of W, such that if ν(i, t) *is the number of visits* of state i in t *steps then* limt→∞\\nν(i, t)\\nt\\n= p∞ .\\n\\nTheorem 2. Let X be an ergodic MC with invariant distribution p∞*. Then*\\n\\n$$p_{t}(X|X_{0})=p_{0}(X|X_{0})W^{t}\\\\ {\\\\xrightarrow{t\\\\to\\\\infty}}\\\\ p_{\\\\infty}(X)$$\\n\\nindependently of the initial distribution p0(X|X0).\\n\\nDefinition 4. A stochastic matrix W *and a measure* p(X) *are said to be in detailed balance if* p(X = i)Wij = Wjip(X = j) ∀i, j ∈ S.\\n\\n2.4.2 Translating Neural dynamics in terms of Markov Chains As analysed in section 2.2, we have that sequential dynamics with noise can be expressed by\\n\\n$$\\\\begin{cases}{\\\\mathrm{choose~randomly~}}i{\\\\mathrm{~in~}}\\\\{1..N\\\\}}\\\\\\\\ \\\\mathbb{P}(\\\\mathbf{\\\\sigma}(t+\\\\Delta t))={\\\\frac{1}{2}}+{\\\\frac{1}{2}}\\\\sigma_{i}(t+\\\\Delta t)\\\\operatorname{tanh}(\\\\beta\\\\varphi_{i}(\\\\mathbf{\\\\sigma}(t)))\\\\end{cases}$$\\n$$\\\\left(14\\\\right)$$\\n\\nwhere β =\\n1 Tand we use the fact that tanh(σiβφi(σ(t))) = σi tanh(βφi(σ(t))) since σi takes values in {−1, +1} and tanh is odd. If σ(t) is given, then equation 14 becomes\\n\\n$$p_{t+1}(\\\\mathbf{\\\\sigma})={\\\\frac{1}{N}}\\\\sum_{i=1}^{N}\\\\left[\\\\left({\\\\frac{1}{2}}+{\\\\frac{1}{2}}\\\\sigma_{i}\\\\operatorname{tanh}(\\\\beta\\\\varphi_{i}(\\\\mathbf{\\\\sigma}(t)))\\\\right)\\\\prod_{j\\\\neq i}\\\\delta_{\\\\sigma_{j},\\\\sigma_{j}(t)}\\\\right]$$\\n\\nwhere the production tells us that all neurons remain unaffected except neuron i, while the summation tells us that during sequential dynamics, (almost) all neurons are given the opportunity to be flipped. On the other hand, if probability pt(σ) is given, then we get\\n\\n$$p_{t+1}(\\\\sigma)=\\\\sum_{\\\\sigma^{\\\\prime}}W[\\\\sigma;\\\\sigma^{\\\\prime}]p_{t}(\\\\sigma^{\\\\prime})$$\\n$$\\\\left(15\\\\right)$$\\n′) (15)\\nwith the 2N × 2 N transfer matrix given by\\n\\n$$W[\\\\mathbf{\\\\sigma};\\\\mathbf{\\\\sigma}^{\\\\prime}]=\\\\frac{1}{N}\\\\sum_{i=1}^{N}\\\\left[\\\\left(\\\\frac{1}{2}+\\\\frac{1}{2}\\\\sigma_{i}^{\\\\prime}\\\\tanh(\\\\beta\\\\varphi_{i}(\\\\mathbf{\\\\sigma}^{\\\\prime})\\\\right)\\\\delta_{\\\\mathbf{\\\\sigma},\\\\mathbf{\\\\sigma}^{\\\\prime}}+\\\\left(\\\\frac{1}{2}-\\\\frac{1}{2}\\\\sigma_{i}^{\\\\prime}\\\\tanh(\\\\beta\\\\varphi_{i}(\\\\mathbf{\\\\sigma}^{\\\\prime})\\\\right)\\\\delta_{F_{i}(\\\\mathbf{\\\\sigma}),\\\\mathbf{\\\\sigma}^{\\\\prime}}\\\\right]$$  where $F_{i}(\\\\mathbf{\\\\sigma})=(\\\\sigma_{1},...,\\\\sigma_{i-1},-\\\\sigma_{i},\\\\sigma_{i+1},...,\\\\sigma_{N})$ is the **flipping operator**. Equation 15 is the \\nMarkov equation corresponding to the sequential process σ(t) → σ(t + 1).\\nProposition 2. The process described above by W *is ergodic. Then there exists a unique* stationary distribution p∞ to which it will converge from any initial distributions over states.\\n\\nThis distribution is determined by the stationary condition\\n\\n$$p_{\\\\infty}(\\\\mathbf{\\\\sigma})=\\\\sum_{\\\\mathbf{\\\\sigma}^{\\\\prime}}W[\\\\mathbf{\\\\sigma};\\\\mathbf{\\\\sigma}^{\\\\prime}]p_{\\\\infty}(\\\\mathbf{\\\\sigma}^{\\\\prime})\\\\;\\\\;\\\\forall\\\\mathbf{\\\\sigma}\\\\in\\\\{-1,+1\\\\}^{N}.$$\\n\\nWe observe that, to calculate p∞(σ), we have to solve a system of 2N linear equations for 2 N values of p∞, which would be a very difficult job. This is why we want to impose a strong condition that would simplify the calculations, the Detailed Balance :\\n\\n$$W[\\\\mathbf{\\\\sigma};\\\\mathbf{\\\\sigma^{\\\\prime}}]p_{\\\\infty}(\\\\mathbf{\\\\sigma^{\\\\prime}})=W[\\\\mathbf{\\\\sigma^{\\\\prime}};\\\\mathbf{\\\\sigma}]p_{\\\\infty}(\\\\mathbf{\\\\sigma})\\\\;\\\\;\\\\forall\\\\mathbf{\\\\sigma},\\\\mathbf{\\\\sigma^{\\\\prime}}\\\\in\\\\{-1,+1\\\\}^{N}.$$\\nN . (16)\\n$$(16)$$', \"Theorem 3. *Let's consider sequential dynamics without self-interactions (*Jii = 0 ∀i). Then the detailed equilibrium is equivalent to the symmetry of the interactions, i.e.\\n\\n$J_{ij}=J_{ji}\\\\ \\\\ \\\\forall i,j\\\\ \\\\Longleftrightarrow\\\\ W[\\\\mathbf{\\\\sigma};\\\\mathbf{\\\\sigma}^{\\\\prime}]p_{\\\\infty}(\\\\mathbf{\\\\sigma}^{\\\\prime})=W[\\\\mathbf{\\\\sigma}^{\\\\prime};\\\\mathbf{\\\\sigma}]p_{\\\\infty}(\\\\mathbf{\\\\sigma})\\\\ \\\\ \\\\forall\\\\mathbf{\\\\sigma},\\\\mathbf{\\\\sigma}^{\\\\prime}\\\\in\\\\{-1,+1\\\\}^{N}$\\nN .\\nMoreover, if detailed balance holds then the equilibrium distribution is given by p∞(σ) ∝ e\\n−H(σ)\\nT (17)\\nwhere H(σ) *is the Ljapunov function of the noiseless dynamics given by*\\n\\n$${\\\\mathcal{H}}(\\\\mathbf{\\\\sigma})=-{\\\\frac{1}{2}}\\\\sum_{i,j=1}^{N}\\\\sigma_{i}J_{i j}\\\\sigma_{j}-\\\\sum_{i=1}^{N}h_{i}\\\\sigma_{i}.$$\\n$$\\\\left(17\\\\right)$$\\n\\nNotice that p∞(σ) corresponds to the Boltzmann-Gibbs distribution for (σ*, J,* h) .\\n\\nProof. Wlog we can suppose σ\\n′ ̸= σ; moreover, we assume σ\\n′ = Fi(σ) where Fiis the flipping operator. In this case, the DB-condition is equivalent to\\n\\n$${\\\\frac{p_{\\\\infty}(\\\\mathbf{\\\\sigma}^{\\\\prime})e^{-\\\\beta\\\\sigma_{i}^{\\\\prime}\\\\varphi_{i}(\\\\mathbf{\\\\sigma}^{\\\\prime})}}{\\\\cosh(\\\\beta\\\\varphi_{i}(\\\\mathbf{\\\\sigma}^{\\\\prime}))}}={\\\\frac{p_{\\\\infty}(\\\\mathbf{\\\\sigma})e^{-\\\\beta\\\\sigma_{i}\\\\varphi_{i}(\\\\mathbf{\\\\sigma})}}{\\\\cosh(\\\\beta\\\\varphi_{i}(\\\\mathbf{\\\\sigma}))}}$$\\n\\nNotice that φi(σ) = φi(σ\\n′) because of no self-interaction; hence the DB-condition becomes p∞(σ\\n′)e\\n−βσ′iφi(σ) = p∞(σ)e\\n−βσiφi(σ)\\nRecall that the process is ergodic, so p∞(σ) > 0 ∀σ; therefore, we can express the limit distrubution in terms of the exponential function\\n\\n$$p_{\\\\infty}(\\\\mathbf{\\\\sigma})=\\\\exp\\\\left\\\\{\\\\beta\\\\left(\\\\sum_{k}h_{k}\\\\sigma_{k}+\\\\frac{1}{2}\\\\sum_{k\\\\neq l}\\\\sigma_{k}J_{k l}\\\\sigma_{l}+K(\\\\mathbf{\\\\sigma})\\\\right)\\\\right\\\\}$$\\n\\nwhere K(σ) is the *implicit term*. Combining the two previous equations, we obtain that the DB condition equals\\n\\n$$\\\\left\\\\{\\\\begin{array}{l l}{\\\\exp(g_{i}(\\\\mathbf{\\\\sigma}^{\\\\prime}))=\\\\exp(g_{i}(\\\\mathbf{\\\\sigma}))}\\\\\\\\ {{\\\\frac{g_{i}(\\\\mathbf{\\\\sigma})}{\\\\beta}}=-\\\\sigma_{i}\\\\varphi_{i}(\\\\mathbf{\\\\sigma})+\\\\sum_{k=1}^{N}h_{k}\\\\sigma_{k}+{\\\\frac{1}{2}}\\\\sum_{k\\\\neq l}J_{k l}\\\\sigma_{k}\\\\sigma_{l}+K(\\\\mathbf{\\\\sigma})}\\\\end{array}\\\\right.$$\\n\\nWe observe that, by explicating the local field, we obtain\\n\\ngi(σ) β = −σi  X N k=1 Jikσk + hi ! + X N k=1 hkσk + 1 2 X k̸=l Jklσkσl + K(σ) =   −σihi + X N k=1 hkσk ! +   − X N k=1 Jikσkσi + 1 2 X k̸=l Jklσkσl ! + K(σ) k̸=i hkσk +   − X k̸=i Jikσkσi + 1 2 X k̸=l,k̸=i,l̸=i Jklσkσl + 1 2 X k̸=i Jkiσkσi + 1 2 X l̸=i Jilσiσl ! + K(σ) = X = {terms non involving index i} + 1 2 X k̸=i (Jki − Jik)σiσk + K(σ)\\nIn conclusion, these calculations show that the DB condition holds if and only if there exists a function K(·) such that K(σ\\n′) − K(σ) = σiPk̸=i\\n(Jik − Jki)σk. We can now demonstrate the two implications. Let's now assume that the balance condition applies. Thus, we consider σ = Fj (σ) with j ̸= i so we obtain\\n\\n$$K(F_{i}F_{j}\\\\sigma)-K(F_{j}\\\\sigma)=\\\\sigma_{i}\\\\sum_{k\\\\neq i}(J_{i k}-J_{k i})F_{j}\\\\sigma_{k}$$ $$=\\\\left(\\\\sigma_{i}\\\\sum_{k\\\\neq i}(J_{i k}-J_{k i})\\\\sigma_{k}\\\\right)-2\\\\sigma_{i}(J_{i j}-J_{j i})\\\\sigma_{j}$$\\n\\nwhence\\n$$K(F_{i}F_{j}\\\\pmb{\\\\sigma})-K(F_{j}\\\\pmb{\\\\sigma})-K(F_{i}\\\\pmb{\\\\sigma})+K(\\\\pmb{\\\\sigma})=-2\\\\sigma_{i}(J_{i j}-J_{j i})\\\\sigma_{j}$$\\n\\n## And Since The Left-Hand Member Is Invariant With Respect To Permutation (*I, J*), The Right-Hand\\nMember Must Necessarily Be Invariant, I.E. There Must Be Symmetrical Interaction.\\nOn The Other Hand, If We Assume Symmetrical Interaction, Then The Db Condition Is Equivalent\\nTo The Existence Of A Function K Such That K(Fiσ) − K(Σ) = 0 And This Is Easily Verified By\\nTaking, For Example, Constant K(Σ) = K. 3 Statistical Mechanics Background\\n\\nStatistical mechanics is a branch of Physics that elucidates the collective behavior of macroscopic systems through the analysis of statistical properties at the microscopic level.\", \"Statistical mechanics is a branch of Physics that elucidates the collective behavior of macroscopic systems through the analysis of statistical properties at the microscopic level.\\n\\nSpin models form a foundational framework in statistical mechanics, offering a conceptual lens to investigate the collective behavior of magnetic systems. At their core, these models represent the angular momentum of atomic spins, influencing the material's magnetic properties. Notable among them is the *spin glass model*, introducing disorder for complex behaviors. A specific variant, the *mean-field spin glass*, simplifies the description for tractable analyses. These models illuminate the dynamics of magnetic materials, serving as invaluable tools to unveil the intricate interplay between microscopic spins and macroscopic magnetic phenomena. Within the domain of neural networks, spin models find a unique application in unraveling the intricate dynamics governing these complex systems. By adapting the principles of statistical mechanics to model the interactions between spins, analogous to neurons in a network, researchers can gain valuable insights into the emergent behaviors, phase transitions, and information processing mechanisms within neural networks. These spin models offer a conceptual bridge, allowing us to draw parallels between the collective behavior of spins in magnetic systems and the dynamic interactions of neurons in a network. In this interdisciplinary approach, spin models prove to be versatile tools, shedding light on the nuanced dynamics that define the computational prowess of neural networks. In this section, we provide a concise overview of key statistical mechanics concepts essential for understanding the Hopfield model. We explore principles directly relevant to this neural network, distilling the foundational elements needed to navigate the interplay between statistical mechanics and the Hopfield model's dynamics. Our aim is to offer a focused and accessible entry into the world of statistical mechanics tailored to the study of neural networks.\\n\\nIn general, we have a mean-field spin model with σ *∈ {−*1, +1}\\nN , J ∈ R\\nN×N symmetric and the Hamiltonian\\n\\n$${\\\\cal H}(\\\\mathbf{\\\\sigma};N,J,\\\\mathbf{h})=-\\\\frac{1}{2}\\\\sum_{i,j=1}^{N}\\\\sigma_{i}J_{ij}\\\\sigma_{j}-\\\\sum_{i=1}^{N}h_{i}\\\\sigma_{i}\\\\tag{18}$$  we on the probability distribution \\n\\nalthough our focus will be on the probability distribution\\n\\n$$\\\\rho(\\\\mathbf{\\\\sigma};\\\\beta,N,J,\\\\mathbf{h})={\\\\frac{\\\\exp\\\\left(-\\\\beta{\\\\mathcal{H}}(\\\\mathbf{\\\\sigma};N,J,\\\\mathbf{h})\\\\right)}{Z_{\\\\beta,N,J,\\\\mathbf{\\\\sigma}}}}$$\\n$$\\\\left(19\\\\right)$$\\n\\n11\\n\\nwhere\\n$$Z_{\\\\beta,N,J,\\\\mathbf{h}}=\\\\sum_{\\\\sigma^{\\\\prime}}\\\\exp\\\\left(-\\\\beta{\\\\mathcal{H}}(\\\\mathbf{\\\\sigma}^{\\\\prime};N,J,\\\\mathbf{h})\\\\right)$$\\n$$(20)$$\\n′; *N, J,* h)) (20)\\n\\nis the Partition function. We observe that we are exactly in the context described by Theorem 3 and this shows the close connection between associative neural network models and those of statistical mechanics. The level of description of statistical mechanics is the Mesoscopic level.\\n\\nFor each mesoscopic state i ∈ T , we consider its *energy* Ei; then, a *thermodynamic state* of the system is described by statistical set {ρi}i∈T interpreted as a probability distribution over the set of states T . Let us now give some definitions. Definition 5. *For all* j = 1...|T | *we call a pure state* ρ\\n(j)if ρ\\n(j)\\ni = δij ∀i ∈ T .\\n\\nNotice that the set S of thermodynamic state is a simplex, so each state ρ ∈ S can be expressed as a combination of pure states.\\n\\nDefinition 6. We define the following functions where kB is the Boltzmann constant :\\n(i) Internal Energy *U(E, ρ*) = Pi∈T ρiEi\\n(ii) *Gibbs Entropy* S(ρ) = −kBPi∈T ρilog(ρi)\\n(iii) Free Energy *F(E, ρ, T*) = U(E, ρ) − *T S(ρ*)\\nDefinition 7. We define a thermodynamic equilibrium,or *Boltzmann-Gibbs distribution*,\\nthe state ρ¯ *that minimizes the free energy* F.\\n\\nTheorem 4. *It holds that*\", \"Theorem 4. *It holds that*\\n\\n$\\\\mathfrak{h}$\\n$$\\\\forall i\\\\in T$$\\n$\\\\hat{\\\\mathbf{r}}$\\n$\\\\infty_{12}$\\n$$U\\\\left(E,\\\\rho\\\\right)=$$\\n$$\\\\left\\\\{\\\\begin{array}{l l}{{\\\\bar{\\\\rho}_{i}=\\\\frac{e^{-\\\\frac{E_{i}}{k_{B}T}}}{Z}}}\\\\\\\\ {{Z=\\\\sum_{i\\\\in\\\\mathcal{T}}e^{-\\\\frac{E_{i}}{k_{B}T}}=e^{-\\\\frac{F(E,T)}{k_{B}T}}}}\\\\\\\\ {{\\\\bar{F}(E,T)=\\\\operatorname*{inf}_{\\\\rho\\\\in\\\\mathcal{S}}F(E,\\\\rho,T)=F(E,\\\\bar{\\\\rho},T)\\\\ .}}\\\\end{array}\\\\right.$$\\n.$(q)\\\\,=\\\\,\\\\sum\\\\,_{\\\\pi}\\\\,q$\\nBefore moving on to the study of the simplest neural network model, i.e. the Curie Weiss model, let us define other functions that will be studied next. Definition 8. *We define the following functions :*\\n(i) Intensive Free Energy f*N,β,J,*h = −\\nT\\nN\\nlogZ*β,N,J,*h\\n(ii) Intensive Pressure A*N,β,J,*h =\\n1 N\\nlogZ*β,N,J,*h\\n(iii) Thermal average of observable g ω*N,β,J,*h(g) = Pσ g(σ)ρ*N,β,J,*h(σ)\\nFurthermore, we will say that one of the defined functions is in the Thermodynamic limit\\n(TDL) if we let N tend to infinity. This concept will be fundamental in the continuation of the article, as we will engage in finding solutions in this context because it would be more simple.\\n\\nHowever, the existence of this limit will not be analysed, which is a very complicated analytical problem.\\n\\n## 3.1 Curie-Weiss Model\\n\\nLet us now turn our attention to a very simple neural network model, the Curie-Weiss model.\\n\\nThis model can be described as a system made of N spins σi *∈ {−*1, +1} that can interact pairwise and with an external field according to the Hamiltonian\\n\\n$${\\\\mathcal{H}}_{N,J,h}(\\\\sigma)=-\\\\sum_{(i,j)}\\\\sigma_{i}J_{i j}\\\\sigma_{j}-\\\\sum_{i=1}^{N}h_{i}\\\\sigma_{i}$$\\n\\n$$(21)$$\\n\\nWe shall consider a homogenous coupling, i.e. Jij =JN with J constant, and homogeneous external field, i.e. hi = h ∀i = 1*...N*. The *order parameter* of the model is the empirical Magnetization\\n\\n$$m_{N}(\\\\mathbf{\\\\sigma})={\\\\frac{1}{N}}\\\\sum_{i=1}^{N}\\\\sigma_{i}$$\\n$$(22)$$\\n\\nwhich expresses the percentage of spin pointing upwards or downwards. In fact, if m = 1 then there is all positive spin while if m = −1 all negative spin. In fact, we can rewrite the Hamiltonian as a function of m as follows\\n\\n$${\\\\mathcal{H}}_{N,J,h}(\\\\mathbf{\\\\sigma})=-{\\\\frac{J}{N}}\\\\sum_{i>j}\\\\sum_{j}\\\\sigma_{i}\\\\sigma_{j}-h\\\\sum_{i}\\\\sigma_{i}=-{\\\\frac{N J}{2}}(m_{N}(\\\\mathbf{\\\\sigma}))^{2}-h N m_{N}(\\\\mathbf{\\\\sigma})+{\\\\frac{J}{2}}$$\\n\\nwhere the last term is the diagonal element that we added to introduce m and therefore we have to subtract it; clearly this term will be left out as it does not affect the minimisation process. This is the reason why we refer to these models as *mean field models*. This allows us to apply the so-called Coarse-Graining process, which consists of simplifying the expression for the partition function and rewriting it as\\n\\n$$\\\\begin{array}{l l}{{Z_{N}=\\\\sum_{\\\\sigma}e^{-\\\\beta{\\\\mathcal H}_{N}(\\\\sigma)}=\\\\sum_{m\\\\in{\\\\mathcal M}}e^{-\\\\beta{\\\\mathcal H}_{N}(m)\\\\Omega_{N}(m)}}}\\\\\\\\ {{\\\\quad}}&{{\\\\approx\\\\sum_{m\\\\in{\\\\mathcal M}}e^{-\\\\beta F_{N}(m)}\\\\approx\\\\sum_{k}e^{-\\\\beta N f_{N}(m_{k}^{*})}\\\\rho_{N}^{(k)}}}\\\\end{array}$$\\n\\nwhere ΩN (m) is the number of configurations with magnetization equal to m and m∗is the argmin of the function [UN (m)−T SN (m)]. The approximations have been made assuming that ΩN is equal to Gibbs entropy (and not Boltzmann entropy) and that N is very large. Now we want to derive an explicit formula for free energy via Laplace's method.\\n\\nTheorem 5. Let g ∈ C2([a, b]) and x0 ∈ (a, b) *be the only point such that* g(x0) = maxx∈[a,b] g(x)\\nand g\\n′′(x0) < 0*. Then*\\n\\n$$\\\\operatorname*{lim}_{N\\\\to\\\\infty}{\\\\frac{\\\\int_{a}^{b}e^{N g(x)}\\\\,d x}{e^{N g(x_{0})}{\\\\sqrt{\\\\frac{2\\\\pi}{N(-g^{\\\\prime\\\\prime}(x_{0}))}}}}}=1.$$\\n$$\\\\left(23\\\\right)$$\\nThis holds also for a = −∞ *and/or* b = +∞ *and for* x ∈ R\\n$$u d\\\\;f o r\\\\;x\\\\in\\\\mathbb{R}^{K}\\\\;\\\\;w i t h\\\\;\\\\mathrm{lim}_{N\\\\to\\\\infty}\\\\;\\\\frac{K}{N}=0.$$\\n\\nCorollary 1. Let be g *a convex function. Then we have*\\n\\n$$\\\\operatorname*{lim}_{N\\\\to\\\\infty}-{\\\\frac{1}{N}}\\\\log\\\\int_{-\\\\infty}^{+\\\\infty}d x\\\\ e^{N g(x)}=\\\\operatorname*{min}_{x}g(x).$$\\n$$\\\\left(24\\\\right)$$\", \"$$\\\\operatorname*{lim}_{N\\\\to\\\\infty}-{\\\\frac{1}{N}}\\\\log\\\\int_{-\\\\infty}^{+\\\\infty}d x\\\\ e^{N g(x)}=\\\\operatorname*{min}_{x}g(x).$$\\n$$\\\\left(24\\\\right)$$\\n\\nWe observe that we can use the theorem to rewrite the distribution function as\\n\\n$$\\\\begin{array}{c}{{Z_{N}=\\\\int d m\\\\ Z_{N}(m)=\\\\int d m\\\\ e^{-N\\\\beta f_{N}(m)}}}\\\\\\\\ {{\\\\stackrel{N>>1}{\\\\approx}\\\\int d m\\\\ e^{-N\\\\beta f_{N}(m)e^{-\\\\frac{N\\\\beta}{2}f_{N}^{\\\\prime\\\\prime}(m^{*})(m-m^{*})^{2}}}}}\\\\\\\\ {{=e^{-N\\\\beta f_{N}(m^{*})}\\\\sqrt{\\\\frac{2\\\\pi}{N\\\\beta f_{N}^{\\\\prime\\\\prime}(m^{*})}}}}\\\\end{array}$$\\n\\nfrom which we obtain that\\n\\n$$f_{N}=-\\\\frac{1}{N\\\\beta}l o g Z_{N}\\\\stackrel{N\\\\geq\\\\geq1}{\\\\approx}^{1}f_{N}(m^{*})+\\\\frac{l o g\\\\left(\\\\frac{2\\\\pi}{\\\\beta f^{\\\\prime\\\\prime}(m^{*})}\\\\right)-l o g N}{2N\\\\beta}\\\\stackrel{N\\\\to\\\\infty}{\\\\longrightarrow}f(m^{*}).$$\\n\\nThus we have obtained that, under TDL, the intensive free energy is simply f(m) calculated at its minimum m∗, which in turn provides the expectation of the magnetization; so we want an explicit formula for f. Notice that\\n\\n$$f(m)=-\\\\lim_{N\\\\rightarrow\\\\infty}\\\\frac{T}{N}logZ_{N}(m)=\\\\frac{1}{2}Jm^{2}-hm-Ts(m)\\\\tag{25}$$\\n\\nwhere s(m) = limN→∞\\n1 N\\nlogΩN (m) is the entropy per spin. We will now calculate s(m) using the integral representation of Dirac's delta. Indeed, we have\\n\\n$$\\\\delta(m-m_{N}(\\\\mathbf{\\\\sigma}))=\\\\int_{-\\\\infty}^{+\\\\infty}dx\\\\ \\\\frac{N}{2\\\\pi}e^{iNx[m-m_{N}(\\\\mathbf{\\\\sigma})]}=\\\\int_{-\\\\infty}^{+\\\\infty}dx\\\\ \\\\frac{N}{2\\\\pi}e^{iNxm-ix\\\\sum_{j}\\\\sigma_{j}}$$ $$=\\\\int_{-\\\\infty}^{+\\\\infty}dx\\\\ \\\\frac{N}{2\\\\pi}e^{iNxm}\\\\prod_{j=1}^{N}e^{-i\\\\sigma_{i}x}$$\\n\\nthus\\n\\n$$\\\\Omega_{N}(m)=\\\\sum_{\\\\sigma}\\\\delta(m-m_{N}(\\\\sigma))=\\\\sum_{\\\\sigma}\\\\int_{-\\\\infty}^{+\\\\infty}\\\\,dx\\\\ \\\\frac{N}{2\\\\pi}e^{iNxm}\\\\prod_{j=1}^{N}e^{-i\\\\sigma_{i}x}$$ $$=\\\\int_{-\\\\infty}^{+\\\\infty}dx\\\\ \\\\frac{N}{2\\\\pi}e^{iNxm}\\\\prod_{j=1}^{N}\\\\sum_{\\\\sigma_{j}=\\\\pm1}e^{-i\\\\sigma_{i}x}$$ $$=\\\\int_{-\\\\infty}^{+\\\\infty}dx\\\\ \\\\frac{N}{2\\\\pi}e^{iNxm}\\\\left(2\\\\cos(x)\\\\right)^{N}=\\\\int_{-\\\\infty}^{+\\\\infty}dx\\\\ \\\\frac{N}{2\\\\pi}e^{iNxm}e^{N\\\\log(2\\\\cos(x))}.$$  rollary 1, we get \\n$$s(x))]=i m$$\\n\\nFrom Corollary 1, we get d dx[imx−log(2 cos(x))] = im−tan(x) = 0 ⇐⇒ x\\n∗ = atan(im) = i atanh(m) = i2\\n\\n$$)=i\\\\operatorname{atanh}(m)={\\\\frac{i}{2}}\\\\log\\\\left({\\\\frac{1+m}{1-m}}\\\\right)$$\\n\\nfrom which\\n\\ns(m) = −m tanh(m) + log(2 cosh(tanh(m))) = −\\n$$\\\\operatorname{sh}(\\\\operatorname{tanh}(m)))=-{\\\\frac{1+m}{2}}\\\\log\\\\!\\\\left({\\\\frac{1+m}{2}}\\\\right)-{\\\\frac{1-m}{2}}\\\\log\\\\!\\\\left({\\\\frac{1-m}{2}}\\\\right)$$\\nwhere where we put m = x\\n∗. In conclusion, we obtain the explicit formula for the coarsegrained intensive free energy in TDL given by\\n\\n$$f(m)=-\\\\frac{J}{2}m^{2}-hm+T\\\\left[\\\\frac{1+m}{2}\\\\log\\\\biggl{(}\\\\frac{1+m}{2}\\\\biggr{)}+\\\\frac{1-m}{2}\\\\log\\\\biggl{(}\\\\frac{1-m}{2}\\\\biggr{)}\\\\right]\\\\tag{26}$$\\n\\n![14_image_0.png](14_image_0.png)\\n\\nwhere the first two addends correspond to the energy contribution, while the third addend is the entropy contribution governed by the control parameter T. Figure 5 shows the plot of f(m) at different temperature levels. We observe that if J is less than zero, i.e. we are in the presence of *paramagnetic* material, the coarse-grained free energy has a single minimum at m = 0. Whereas if J is greater than zero, i.e. there is *ferromagnetic* behaviour, then the minimum points vary as the temperature varies. This is a direct consequence of the fact that\\n\\n$$f^{\\\\prime}(m)=0\\\\implies Jm^{*}\\\\beta+\\\\beta h=\\\\mbox{atanh}(m^{*})=0\\\\implies m^{*}=\\\\mbox{tanh}(\\\\beta(Jm^{*}+h))\\\\tag{27}$$\\n\\nso the minima correspond to the solutions of the Self-Consistency equation which clearly vary as β varies. See Appendix A for further details.\\n\\n## 3.2 Phase Transitions And Ergodicity Breaking\\n\\nA *phase transition* occurs when there is a singularity in the free energy or in one of its derivative.\", \"## 3.2 Phase Transitions And Ergodicity Breaking\\n\\nA *phase transition* occurs when there is a singularity in the free energy or in one of its derivative.\\n\\nThis aspect is related to a sharp change in the properties of a substance: for example liquid/gas or paramagnetic/ferromagnetic. It is possible to classify these transitions. For istance, if there is a finite discontinuity in one or more of the prime derivatives, we will say that we are in the presence of a first-order phase transition; if, on the other hand, the prime derivatives are continuous but the second derivatives are discontinuous or finite, then the transition will be of the second order. Phase transitions often involves a Symmetry Breaking process; in fact, the Hamiltonian's system and the equations of the dynamics are invariant under the action of a symmetry group, but the system is not. Tipically, the high-temperature phase contains more symmetries than the low-temperature phase. We denote by Tc a critical temperature associated with a phase transition which is sensitively to the interatomic interactions. When *T > T*c ,\\nthen the free energy has a single minimum at m∗ = 0 and thus the system explores the entire admissible phase space. If *T < T*c, then the free energy has two symmetric minima ±m∗ ̸= 0 and the order of the system is restricted to an appropriate part of the phase space Ω+ or Ω− with Ω = Ω+ ∪ Ω\\n−. In the first case the system is ergodic, while in the second case ergodicity is broken. If T=0, the system just moves toward configuration leading to energy minimization so also in this case ergodicity is broken. But in section 2.4.2 we said that the neural dynamics is ergodic; so what is wrong? The fact is that in the TDL N → ∞ the energy barriers cannot be overcome and then ergodicity is broken. It is possible to see this from the point of view of Markov chains; in particular, we denote with ρj (t) as the probability that the system is in state j at time t and ρ = (ρ1*...ρ*M) where M = 2N as all the possible states. Then we can express the probability of being at time t + 1 in state i as\\n\\n$$\\\\rho_{i}(t+1)=\\\\sum_{j=1}^{M}W(i\\\\mid j)\\\\rho_{j}(t).$$\\n\\nwith W transition matrix. Let us consider its the spectral expansion\\n\\n$$W(i\\\\mid j)=\\\\sum_{k=1}^{M}\\\\lambda_{k}v_{k}^{L}(j)v_{k}^{R}(i)$$\\n\\nfrom which we obtain\\n\\n$$\\\\rho_{i}(t)=W^{t}\\\\rho_{i}(0)=\\\\sum_{j=1}^{M}\\\\sum_{k=1}^{M}\\\\lambda_{k}^{t}v_{k}^{L}(j)v_{k}^{R}(i)\\\\rho_{j}(0).$$\\n\\nRecall that left and right eigenvectors are orthogonal if they belong to different eigenvalues. Theorem 6. Let W *be a stochastic matrix. Then*\\n\\n$$(i)\\\\ |\\\\lambda|\\\\leq1\\\\ {\\\\mathrm{for~all~}}\\\\lambda\\\\in\\\\sigma(W)$$\\n(ii) $\\\\lambda_{max}=1$ and $\\\\mathbf{v}^{L}=(1,...,1)$  This implies that \\n$$\\\\rho_{i}(t)\\\\stackrel{t\\\\to\\\\infty}{\\\\longrightarrow}\\\\sum_{j=1}^{M}v_{k^{*}}^{R}(i)\\\\rho_{j}(0)=v_{\\\\lambda_{\\\\mathrm{max}}}^{R}(i)$$\\n\\nthen , after a long enough time, memory of the initial state is lost and , for any initial configuration, the asymptotic distribution is reached corresponding to the right eigenvector of the unique largest eigenvalue. Thus, as long as N is finite and T > 0, the system is ergodic.\\n\\nSo how is it possible to break the ergodicity? As M increases, the spectral gap between the maximum and minimum eigenvalue may decrease; then in the limit M → ∞, i.e. in the TDL,\\nthere may be an asymptotic degeneration and ergodicity is broken. For istance, let us denote v R\\n1,2\\n, vL\\n1,2 the right and left eigenvector associated to the max eigenvalue with degeneration 2.\\n\\nThen we obtain\\n\\n$$\\\\rho_{i}(t)\\\\stackrel{t\\\\rightarrow\\\\infty}{\\\\longrightarrow}\\\\sum_{k=1}^{2}\\\\sum_{j=1}^{M}v_{k}^{L}(j)v_{k}^{R}(i)\\\\rho_{j}(0)=\\\\sum_{k=1}^{2}[v_{k}^{L}\\\\mathbf{\\\\rho}(0)]v_{k}^{R}$$\\n\\nand since ρ(0) still appears in the equation, the process will lead to different asymptotic trajectories depending on the initial condition.\", 'and since ρ(0) still appears in the equation, the process will lead to different asymptotic trajectories depending on the initial condition.\\n\\nThe Curie Weiss model, although we have comprehensive knowledge, is a very simple neural network model. In fact, we have seen how the cost function ( intensive free energy or the Hamiltonian) has, at best, three points of minimum: m = 0 in correspondence of a completely disordered pattern, m = ±m∗in correspondence of a ±ξ pattern. Thus the model can store with N neurons only one pattern, which is why we will now analyse in detail the much better performing *Hopfield model*.\\n\\n## 4 Hopfield Model\\n\\nLet us now analyse a more complex model. We want to obtain a neural network consisting of N neurons, which is characterised by a cost function representing P local minima at the patterns that we want to store in the network. Indeed, let us consider the patterns\\n\\n$$\\\\xi^{\\\\mu}=(\\\\xi_{1}^{\\\\mu},...,\\\\xi_{N}^{\\\\mu})\\\\in\\\\{-1,+1\\\\}^{N}\\\\;\\\\;\\\\forall\\\\mu=1...P.$$\\n\\nLet us consider the standard Hamiltonian of equation 11, where now Hebb\\'s rule is given by\\n\\n$$J_{i j}=\\\\sum_{\\\\mu=1}^{P}\\\\frac{\\\\xi_{i}^{\\\\mu}\\\\xi_{j}^{\\\\mu}}{N}\\\\;\\\\;\\\\forall i,j=1...N.$$\\n\\nAs we did for the Curie-Weiss model, we rewrite the Hamiltonian as a function of magnetization. In this case, however, we do not use the average magnetization but the so-called Mattis Magnetization expressed by\\n\\n$$m_{N,\\\\mu}(\\\\mathbf{\\\\sigma};\\\\mathbf{\\\\xi})={\\\\frac{1}{N}}\\\\sum_{i=1}^{N}\\\\xi_{i}^{\\\\mu}\\\\sigma_{i}\\\\;\\\\;\\\\;\\\\forall\\\\mu=1...P$$\\n$$\\\\left(28\\\\right)$$\\n\\ni.e. the vector (mN,1, ..., mN,P ). We observe that m represents the percentage of equal spins between the σ configuration and the ξ µ pattern; indeed, if mN,µ(σ) = 1 this means that the configuration is exactly identical to the stored pattern. If we assume that we have no external field, then we get\\n\\n$${\\\\mathcal{H}}_{N,P,\\\\xi}(\\\\mathbf{\\\\sigma})=-{\\\\frac{1}{2}}\\\\sum_{(i,j)}\\\\sum_{\\\\mu=1}^{P}{\\\\frac{\\\\xi_{i}^{\\\\mu}\\\\xi_{j}^{\\\\mu}}{N}}\\\\sigma_{i}\\\\sigma_{j}=-{\\\\frac{N}{2}}\\\\sum_{\\\\mu=1}^{P}(m_{N,\\\\mu}(\\\\mathbf{\\\\sigma}))^{2}+{\\\\frac{P}{2}}$$\\n$$(29)$$\\n$$(30)$$\\n\\nwhere the second addend, which we shall ignore, is linked to the diagonal term. In the remainder of the chapter, we will analyse the solution of the Hopfield model in two separate cases:\\n- *Low-load* case, where P is finite (will be the case we will implement in Python)\\n\\n- *High-load* case, where P ∝ N and limN→∞\\nP\\nN\\n> 0\\n\\n## 4.1 Solution In The Low-Load Regime\\n\\nUsing Laplace\\'s method, we want to obtain an explicit expression for the free energy. In more detail, we analyse the so-called Quenched Intensive Free-energy\\n\\n$$f_{N,\\\\beta,J}^{Q}=-\\\\frac{1}{\\\\beta N}\\\\mathbb{E}[\\\\log Z_{N,\\\\beta,\\\\xi}]$$\\n\\nwhere the average is a *quenched average* over possible realisations of patterns given by\\n\\n$$\\\\mathbb{E}[\\\\,\\\\cdot\\\\,]=2^{-N P}\\\\sum_{\\\\xi\\\\in\\\\{-1,+1\\\\}^{N\\\\times P}}[\\\\,\\\\cdot\\\\,]$$\\n$$(31)$$\\n\\nalthough, to lighten the notation, we will avoid emphasising that all the functions we are now going to calculate are quenched. We observe that the partition function can be written as\\n\\nσ exp β 2N X i,j,µ ξ µ i ξ µ j σiσj ! = X σ Z\"Y P ξ µ i σi N ) # exp βN 2 X P µ=1 m2µ ! ZN,β,ξ = X µ=1 dmµδ(mµ − X i Z Z Y P µ=1 dmµ N 2π dm˜ µ ! exp iN X µ m˜ µmµ − i X j,µ m˜ µξ µ j σj + βN 2 X µ m2µ ! = X σ = Z Z Y P µ=1 dmµ N 2π dm˜ µ ! exp \"iN X µ m˜ µmµ + X j log 2 cos X µ m˜ µξ µ j !! + βN 2 X µ m2µ #\\nso that the partition function has a linear spin-dependency and can therefore sum directly over configurations. Notice that the extremality conditions in order to use Laplace\\'s method are\\n\\n$\\\\tilde{m}_{\\\\mu}=i\\\\beta m_{\\\\mu}$ with respect to $\\\\ m_{\\\\mu}$ and $\\\\ m_{\\\\mu}^{*}=m_{\\\\mu}=\\\\dfrac{i}{N}\\\\sum_{j}\\\\xi_{j}^{\\\\mu}\\\\tanh\\\\bigl(\\\\hat{m}_{\\\\mu}\\\\xi_{j}^{\\\\mu}\\\\bigr)$ wrt to $\\\\ \\\\hat{m}_{\\\\mu}$. \\nj\\nfrom which we obtain\\n\\n$$f_{\\\\beta,\\\\mathbf{\\\\xi}}=\\\\operatorname*{min}_{m}\\\\left[{\\\\frac{1}{2}}\\\\sum_{\\\\mu}m_{\\\\mu}^{2}-{\\\\frac{1}{\\\\beta}}\\\\mathbb{E}\\\\left(\\\\log\\\\left(2\\\\cosh\\\\beta\\\\sum_{\\\\mu}m_{\\\\mu}\\\\mathbf{\\\\xi}^{\\\\mu}\\\\right)\\\\right)\\\\right]$$', \"where we use the fact that, for all ξj *∈ {−*1, +1}\\nP\\n\\n$$\\\\frac{1}{N}\\\\sum_{j}\\\\log\\\\left(2\\\\cos\\\\left(\\\\sum_{\\\\mu}\\\\hat{m}_{\\\\mu}\\\\xi_{j}^{\\\\mu}\\\\right)\\\\right)=\\\\frac{1}{N}\\\\sum_{j}g(\\\\xi_{j})\\\\stackrel{N\\\\rightarrow\\\\infty}{\\\\longrightarrow}\\\\mathbb{E}[g(\\\\xi_{j})].$$\\n\\nTheorem 7. *The quenched free energy of the Hopfield model in the thermodynamic limit and* in low-load regime is\\n\\n$$f_{\\\\beta}^{Q}=\\\\frac{1}{2}\\\\sum_{\\\\mu}(m_{\\\\mu}^{*})^{2}-\\\\frac{1}{\\\\beta}\\\\mathbb{E}\\\\left[\\\\log2\\\\cosh\\\\!\\\\left(\\\\beta\\\\sum_{\\\\mu}m_{\\\\mu}^{*}\\\\mathbf{\\\\xi}^{\\\\mu}\\\\right)\\\\right]\\\\tag{32}$$\\n\\nwhere the Mattis magnetization satisfy the self-consistency equations\\n\\n$$m_{\\\\mu}^{*}=\\\\mathbb{E}\\\\left[\\\\mathbf{\\\\xi}^{\\\\mu}\\\\tanh\\\\left(\\\\beta\\\\sum_{\\\\nu}m_{\\\\nu}^{*}\\\\mathbf{\\\\xi}^{\\\\nu}\\\\right)\\\\right].\\\\tag{33}$$\\n\\nWe can see from equation 33 how the Curie-Weiss model is a special case of the Hopfield model. In fact, if we assume that only one pattern is the candidate to be retrieved, for istance ξ 1, then we have m1 ̸= 0 while mµ = 0 ∀µ > 1 so\\n\\n$$m_{\\\\mu}^{*}=\\\\mathbb{E}\\\\left[\\\\mathbf{\\\\xi}^{\\\\mu}\\\\operatorname{tanh}\\\\left(\\\\beta m_{1}^{*}\\\\mathbf{\\\\xi}^{1}\\\\right)\\\\right]=\\\\operatorname{tanh}(\\\\beta m_{1}^{*})\\\\ \\\\mathbb{E}\\\\left[\\\\mathbf{\\\\xi}^{\\\\mu}\\\\mathbf{\\\\xi}^{1}\\\\right]=\\\\operatorname{tanh}(\\\\beta m_{1}^{*})\\\\delta_{\\\\mu,1}$$\\n\\nwhich is exactly the Curie-Weiss law. Also in this case, there exists a critical βc = 1 such that if *β < β*c then there is a paramagnetic behavior and if *β > β*c then there is a ferromagnetic behavior. The most important difference between the Curie-weiss model and the Hopfield model in low-load regime is the possibility of encountering the system in a Spurious state: it can be interpreted as a system error during the retrieval process, where m(n) = mn(1...1, *0...*0) i.e. the magnetization is a vector with the first n components equal to 1 and the remaining equal to 0.\\n\\nThis solution is compatible with equation 33; in fact, if *µ > n*, then the operator E factorizes because the argument of tanh(·) is independent of ξ µso E[ξ µ] · E[tanh βPν m\\n(n)\\nν ξ ν] = 0; while if µ ≤ n, then the equation has non-zero solution for β > 1. However, the solution associated with a spurious state is simply a free energy extremal point and not a global minimum point.\\n\\nIn more detail, deriving an explicit expression of the Hessiana derivative of fβ, we obtain that if n is odd then there exists T\\n(n)\\nc such that for T < T(n)\\nc the spurious states m(n) are local minima, while for n even there are saddle points.\\n\\n## 4.2 Signal-To-Noise Analysis\\n\\nWe now ask whether Hebb's rule stabilises the stored pattern ξ µ. For example, will the configuration σ given by σi = ξ µ i ∀i = 1*...N* be dynamically stable? If we assume the absence of external fields and noise, the stability condition is equivalent to saying that σiφi(σ) > 0 ∀i = 1*...N*.\\n\\nIn this way, the configuration does not vary during neural dynamics according to eq.5, hence the pattern is a fixed point for the model. For istance, let's consider σ = ξ 1; recall that\\n\\n$$\\\\varphi_{i}(\\\\mathbf{\\\\sigma})=\\\\sum_{j=1}^{N}J_{i j}\\\\sigma_{j}(t)={\\\\frac{1}{N}}\\\\sum_{j\\\\neq i}\\\\sum_{\\\\mu=1}^{P}\\\\xi_{i}^{\\\\mu}\\\\xi_{j}^{\\\\mu}\\\\sigma_{j}$$\\n\\nthus we obtain\\n\\n$$\\\\sigma_{i}\\\\varphi_{i}(\\\\mathbf{\\\\sigma})=\\\\xi_{i}^{1}\\\\varphi_{i}(\\\\mathbf{\\\\xi}^{1})=\\\\frac{N-1}{N}+\\\\frac{1}{N}\\\\sum_{j\\\\neq i}^{P}\\\\xi_{i}^{1}\\\\xi_{j}^{\\\\mu}\\\\xi_{j}^{\\\\mu}\\\\xi_{j}^{1}\\\\tag{34}$$\\n\\nwhere the first addend is associated with the *signal* and the second with *noise*. We observe that the signal term is equal to 1 in the TDL, while the noise term for very large N, denoted by R, verifies\\n\\n$$|R|\\\\stackrel{N>>1}{\\\\approx}\\\\sqrt{\\\\frac{P}{N}}$$\", \"$$|R|\\\\stackrel{N>>1}{\\\\approx}\\\\sqrt{\\\\frac{P}{N}}$$\\n\\nso if P is finite and N very large, the noise becomes negligible in relation to the signal and thus each pattern is effectively a fixed point. This result remains valid even if a finite fraction of spins is flipped away at random from one of the patterns. Although our aim was to build a model and its cost function in such a way as to have minima in correspondence with patterns, the non-linearity of the dynamics means that additional attractors are created. Indeed, let's consider a configuration given by\\n\\n$$\\\\sigma_{i}^{(3)}=\\\\mathrm{sgn}(\\\\xi_{i}^{1}+\\\\xi_{i}^{2}+\\\\xi_{i}^{3})\\\\;\\\\;\\\\forall i=1...N$$\\n\\nwhose Mattis magnetization equals\\n\\n$$m_{\\\\mu}^{(3)}=\\\\frac{1}{N}\\\\sum_{i=1}^{N}\\\\mathrm{sgn}(\\\\xi_{i}^{1}+\\\\xi_{i}^{2}+\\\\xi_{i}^{3})\\\\xi_{i}^{\\\\mu}\\\\stackrel{N>>1}{\\\\approx}\\\\mathbb{E}[\\\\mathrm{sgn}(\\\\xi_{i}^{1}+\\\\xi_{i}^{2}+\\\\xi_{i}^{3})\\\\xi_{i}^{\\\\mu}]=\\\\frac{1}{2}\\\\;\\\\;\\\\forall\\\\mu=1,2,3$$\\n\\nand m\\n(3)\\nµ = 0 ∀µ > 3. Then we have\\n\\n$$\\\\sigma_{i}^{(3)}\\\\varphi_{i}(\\\\mathbf{\\\\sigma}^{(3)})=\\\\sigma_{i}^{(3)}\\\\sum_{\\\\mu=1}^{3}m_{\\\\mu}^{(3)}\\\\xi_{i}^{\\\\mu}+\\\\frac{1}{N}\\\\sum_{j=1}^{N}\\\\sum_{\\\\mu>3}\\\\sigma_{i}^{(3)}\\\\xi_{i}^{\\\\mu}\\\\xi_{j}^{\\\\mu}\\\\sigma_{j}^{(3)}$$\\n\\nand we can establish that this configuration is also stable since, for very large N, we have the first term S =\\n1 2 |ξ 1 i + ξ 2 i + ξ 3 i | and the second term R such that R2 ≈\\nP −3 Nso as before, the noise term is negligible. Now, we want to use these results to obtain a Statistical Estimate of the Storage, i.e. the number of patterns Pc = max{P s.t. we have retrieval} = αN.\\n\\nWe observe that the S,R terms in equation 34 verify\\n\\n$$S\\\\stackrel{N\\\\to\\\\infty}{\\\\longrightarrow}1\\\\ ,\\\\,R\\\\stackrel{N\\\\to\\\\infty}{\\\\longrightarrow}{\\\\mathcal{N}}(0,\\\\alpha)$$\\n\\nthus we obtain\\n\\n$$\\\\mathbb{P}[\\\\sigma_{i}=\\\\xi_{i}^{1}{\\\\mathrm{~stable}}]=\\\\mathbb{P}[\\\\xi_{i}^{1}\\\\varphi_{i}(\\\\mathbf{\\\\xi}^{1})>0]=\\\\mathbb{P}[R>-1]={\\\\frac{1}{2}}\\\\left[1+\\\\operatorname{erf}({\\\\frac{1}{2\\\\alpha}})\\\\right].$$\\n\\nIf we assume *α <<* 1, then we can approximate the error function as erf(x)\\n$$(x)\\\\stackrel{x>>1}{\\\\approx}1-{\\\\frac{\\\\exp\\\\!\\\\left(-x^{2}\\\\right)}{\\\\sqrt{x\\\\pi}}}$$\\nthus achieving that\\n$$\\\\mathbb{P}[\\\\mathbf{\\\\sigma}=\\\\mathbf{\\\\xi}^{1}\\\\mathrm{\\\\boldmath~stable}]\\\\approx\\\\left[1-\\\\sqrt{\\\\frac{\\\\alpha}{2\\\\pi}}\\\\exp^{-\\\\frac{1}{2\\\\alpha}}\\\\right]^{N}\\\\approx1-N\\\\sqrt{\\\\frac{\\\\alpha}{2\\\\pi}}\\\\exp^{-\\\\frac{1}{2\\\\alpha}}=1-N_{\\\\mathrm{err}}.$$\\nSo let us impose the condition Nerr << 1, which is satisfied for α =1 2 log N\\n. In conclusion, the critical number of patterns to guarantee stability of pattern ξ 1is\\n\\n$$P_{c}={\\\\frac{N}{2\\\\log N}}$$\\n\\n$$(35)$$\\n\\nand if we want stability also for the other patterns, one reaches Pc =N\\n4 log N\\n. In the particular case of the model implementation that we shall see in Chapter 5, we have N = 513 and thus Pc ≈ 0.1 .\\n\\n## 4.3 Solution In The High-Load Regime\\n\\nIn this section, we will analyse the model in the case of *High-load*, where P ∝ N and\\n\\n$$\\\\alpha:=\\\\operatorname*{lim}_{N\\\\to\\\\infty}{\\\\frac{P}{N}}>0.$$\\n$$(36)$$\\n> 0. (36)\\nOne possible approach is to use the so-called Replica trick. This method uses the following identity\\n\\n$$\\\\log(x)=\\\\operatorname*{lim}_{n\\\\to0}{\\\\frac{x^{n}-1}{n}}$$\\nin order to calculate the quenched pressure\\n$$A_{\\\\beta}^{Q}=-\\\\beta f_{\\\\beta}^{Q}=\\\\operatorname*{lim}_{N\\\\to\\\\infty}\\\\operatorname*{lim}_{n\\\\to0}{\\\\frac{\\\\mathbb{E}Z_{N,\\\\beta,J}^{n}-1}{N n}}.$$\\n\\nThe trick is to consider distinct replicas σ\\n(a),σ\\n(b)that have the same initial distribution. This leads us to introduce the new order parameter called overlap\\n\\n$$q_{a b}=\\\\frac{1}{N}\\\\sum_{i=1}^{N}\\\\sigma_{i}^{(a)}\\\\sigma_{i}^{(b)}\\\\tag{1}$$\\n$$\\\\left(37\\\\right)$$\\n$$(38)$$\\n\\nwhich measures the correlation between the two replicas. This method is very efficient for solving the *Sherrington-Kirkpatrick model*, however it is more complicated to apply it to the Hopfield model (see appendix C for more informations about the SK model).\", 'An alternative and much more sophisticated approach is the so-called Intepolation technique. The main idea is to introduce an interpolating pressure AN (t) that recovers the original model for t = 1, while for t = 0 it corresponds to the pressure of a simpler model analytically addressable. Then, the expression for AN (t) is obtained by exploiting the fundamental theorem of calculus\\n\\n$$A_{N,\\\\beta}^{Q}=A_{N}(1)=A_{N}(0)+\\\\int_{0}^{1}\\\\frac{d}{dt}\\\\ A_{N}(t^{\\\\prime})\\\\ dt^{\\\\prime}.$$  In two starting assumptions. The first consists of the so-called\\n$$(39)$$\\n$$(40)$$\\n\\nThe resolution is based on two starting assumptions. The first consists of the so-called Replica Symmetry Ansatz, in which it is assumed that qab = q ∀a ̸= b. The second, consists of considering the patterns in the following way: the target pattern ξ 1is a Rademarcher pattern, while all others {ξ µ}µ=2*...P* are distributed as a standard Gaussian. Recall that the partition function is\\n\\n$$\\\\begin{array}{r l}{Z_{N,\\\\beta,\\\\xi}=\\\\sum_{\\\\sigma}\\\\exp(-\\\\beta{\\\\mathcal{H}}_{N,\\\\beta,\\\\sigma\\\\xi}(\\\\sigma))}&{{}=\\\\sum_{\\\\sigma}\\\\exp\\\\left({\\\\frac{\\\\beta}{2N}}\\\\sum_{i,j}\\\\xi_{i}^{1}\\\\xi_{j}^{1}\\\\sigma_{i}\\\\sigma_{j}+{\\\\frac{\\\\beta}{2N}}\\\\sum_{\\\\mu>1}\\\\sum_{i,j}\\\\xi_{i}^{\\\\mu}\\\\xi_{j}^{\\\\mu}\\\\sigma_{i}\\\\sigma_{j}\\\\right)}\\\\end{array}$$\\n\\nand using the identity\\nZdz exp−Az2 + Bz= rπ A expB2 4A 1 2 and B = β N (Pi ξ µ i σi) 2 we obtain\\n(40)\\non the second term with A =\\n$$Z_{N,\\\\beta,\\\\xi}=\\\\sum_{\\\\sigma}\\\\int d\\\\mu(z)\\\\exp\\\\left(\\\\frac{\\\\beta}{2N}\\\\sum_{i,j}\\\\xi_{i}^{1}\\\\xi_{j}^{1}\\\\sigma_{i}\\\\sigma_{j}+\\\\sqrt{\\\\frac{\\\\beta}{N}}\\\\sum_{\\\\mu>1}\\\\sum_{i}\\\\xi_{i}^{\\\\mu}\\\\sigma_{i}z_{\\\\mu}\\\\right)$$\\nwhere µ(z) is the Gaussian measure and zµ is a real variable. The orders parameters shall be the overlap q12, the Mattis magnetization m1 and also\\n\\n$$r_{12}=\\\\frac{1}{P-1}\\\\sum_{\\\\mu}z_{\\\\mu}^{(1)}z_{\\\\mu}^{(2)}\\\\ ,\\\\ r_{11}=\\\\frac{1}{P-1}\\\\sum_{\\\\mu}z_{\\\\mu}^{(1)}z_{\\\\mu}^{(1)}.$$\\n\\nNow, we will simply illustrate the results required to arrive at the solution of the Hopfield model, leaving the very tiring calculations to the reader. Let be t ∈ R\\n+ , *A, B, C, D* constants to be set a posteriori and Ji, J˜µ ∼ N (0, 1); then the partition function is\\n\\n$$Z_{N,\\\\beta,\\\\xi}(t;J,\\\\bar{J})=\\\\sum_{\\\\sigma}\\\\int d\\\\mu(z)\\\\exp\\\\left\\\\{\\\\frac{t}{2}\\\\beta Nm_{1}^{2}+\\\\sqrt{\\\\frac{t\\\\beta}{N}}\\\\sum_{\\\\mu,i}\\\\xi_{i}^{\\\\mu}\\\\sigma_{i}z_{\\\\mu}\\\\right.\\\\tag{4}$$ $$\\\\left.+\\\\left(1-t\\\\right)NDM_{1}+(1-t)\\\\frac{C}{2}\\\\sum_{\\\\mu}z_{\\\\mu}^{2}\\\\right.$$\\n\\n$$\\\\left(41\\\\right)$$\\n\\n$$\\\\mu$$ $$+\\\\;\\\\sqrt{1-t}A\\\\sum_{i}J_{i}\\\\sigma_{i}+\\\\sqrt{1-t}B\\\\sum_{\\\\mu}\\\\tilde{J}_{\\\\mu}z_{\\\\mu}\\\\Biggr\\\\}$$\\n\\nand the corresponding quenched statistical pressure is\\n\\n$$A_{N}(t)=\\\\frac{1}{N}\\\\mathbb{E}\\\\log Z_{N,\\\\beta,\\\\xi}(t)\\\\ ,\\\\ A(t)=\\\\operatorname*{lim}_{N\\\\rightarrow\\\\infty}A_{N}(t).$$\\n\\nRecall that the ⟨·⟩ average of the observable O is\\n\\n$$\\\\langle{\\\\cal O}\\\\rangle=\\\\mathbb{E}[\\\\omega_{N,\\\\beta,J}({\\\\cal O})]=\\\\mathbb{E}\\\\left[\\\\frac{\\\\sum_{\\\\sigma}{\\\\cal O}(\\\\sigma)\\\\exp(-\\\\beta{\\\\mathcal H}(\\\\sigma))}{\\\\sum_{\\\\sigma}\\\\exp(-\\\\beta{\\\\mathcal H}(\\\\sigma))}\\\\right].$$\\nexp(−βH(σ)) . (42)\\n\\nProposition 3. *The derivative of the quenched statistical pressure at finite* N is\\n\\n$$\\\\frac{d}{d t}A_{N}(t)=\\\\frac{\\\\beta}{2}\\\\langle m_{1}^{2}\\\\rangle+\\\\frac{\\\\beta P}{2N}(\\\\langle r_{11}\\\\rangle-\\\\langle r_{12}q_{12}\\\\rangle)-D\\\\langle m_{1}\\\\rangle-\\\\frac{A^{2}}{2}(1-\\\\langle q_{12}\\\\rangle)-\\\\frac{\\\\beta^{2}}{2}(\\\\langle r_{11}\\\\rangle-\\\\langle r_{12}\\\\rangle)-\\\\frac{C}{2}\\\\langle r_{11}\\\\rangle$$\\n\\nand in the thermodynamic limit we have\\n\\n$$\\\\frac{d}{d t}A(t)=\\\\frac{\\\\beta^{2}}{2}\\\\overline{{{m}}}-\\\\frac{\\\\beta\\\\alpha}{2}\\\\overline{{{r}}}(1-\\\\overline{{{q}}})$$  _with some fixed $\\\\overline{{{m}}},\\\\overline{{{q}}},\\\\overline{{{r}}}$ and $A=\\\\beta\\\\alpha\\\\overline{{{r}}},B=\\\\beta\\\\overline{{{q}}},C=\\\\beta(1-\\\\overline{{{q}}}),D=\\\\beta\\\\overline{{{m}}}$._\\nTheorem 8. In the TDL and under RS assumption, the quenched statistical pressure of the Hopfiel model is', '$$A_{\\\\beta,\\\\alpha}(\\\\mathbf{m},q,r)=-\\\\frac{\\\\beta}{2}\\\\mathbf{m}^{2}+\\\\log2-\\\\frac{\\\\alpha\\\\beta}{2}-\\\\beta\\\\alpha\\\\overline{r}(1-\\\\overline{q})+\\\\frac{\\\\beta\\\\alpha\\\\overline{q}}{2(1-\\\\beta(1-\\\\overline{q}))}\\\\tag{43}$$ $$-\\\\frac{\\\\alpha}{2}\\\\log(1-\\\\beta(1-\\\\overline{q}))+\\\\mathbb{E}\\\\left[\\\\log2\\\\cosh\\\\Bigl{(}\\\\beta\\\\overline{m}+J\\\\sqrt{\\\\beta\\\\alpha\\\\overline{r}}\\\\Bigr{)}\\\\right]$$\\n\\nwhere m, q, r *fulfill the conditions*\\n\\n$$\\\\left\\\\{\\\\begin{array}{l l}{{\\\\overline{{{q}}}=\\\\mathbb{E}\\\\left[\\\\operatorname{tanh}^{2}(\\\\beta\\\\overline{{{m}}}+J\\\\sqrt{\\\\beta\\\\alpha\\\\overline{{{r}}}})\\\\right]}}\\\\\\\\ {{\\\\overline{{{m}}}=\\\\mathbb{E}\\\\left[\\\\operatorname{tanh}\\\\!\\\\left(\\\\beta\\\\overline{{{m}}}+J\\\\sqrt{\\\\beta\\\\alpha\\\\overline{{{r}}}}\\\\right)\\\\right]}}\\\\\\\\ {{\\\\overline{{{r}}}=\\\\frac{\\\\beta\\\\overline{{{q}}}}{(1-\\\\beta(1-\\\\overline{{{q}}}))^{2}}}}\\\\end{array}\\\\right.$$\\n\\nIn conclusion, the free energy of the system is given by fβ,α(m, q) = −\\nAβ,α(m,q)\\nβand the extremality conditions are\\n\\n$$\\\\begin{split}\\\\mathbf{m}&=\\\\int d\\\\mu(z)\\\\mathbb{E}\\\\left\\\\{\\\\mathbf{\\\\xi}\\\\tanh\\\\left[\\\\beta\\\\left(\\\\mathbf{m}\\\\cdot\\\\mathbf{\\\\xi}+\\\\frac{\\\\sqrt{\\\\alpha q}}{1-\\\\beta(1-q)}z\\\\right)\\\\right]\\\\right\\\\}\\\\\\\\ &q=\\\\int d\\\\mu(z)\\\\mathbb{E}\\\\left\\\\{\\\\mathbf{\\\\xi}\\\\tanh^{2}\\\\left[\\\\beta\\\\left(\\\\mathbf{m}\\\\cdot\\\\mathbf{\\\\xi}+\\\\frac{\\\\sqrt{\\\\alpha q}}{1-\\\\beta(1-q)}z\\\\right)\\\\right]\\\\right\\\\}.\\\\end{split}\\\\tag{44}$$\\n\\nSee Appendix A for some graphic results.\\n\\n## 4.4 Phase Diagram\\n\\nWe now have all the tools to analyse the phase diagram of the Hopfield model in detail. As can be seen from the figure 6, four different states can be verified:\\n- *Retrieval state*, i.e. ⟨m⟩ ̸= 0, ⟨q⟩ ̸= 0.\\n\\n$$\\\\bullet~R e t r i e v a l~S p u r i o u s~s t a t e,\\\\;\\\\mathrm{i.e.}~\\\\langle m\\\\rangle\\\\neq0,\\\\;\\\\langle q\\\\rangle\\\\neq0.$$\\n\\n- *Spin-glass state*, i.e.⟨m⟩ = 0, ⟨q⟩ ̸= 0.\\n\\n- *Paramagnetic state*, i.e.⟨m⟩ = 0, ⟨q⟩ = 0.\\nFirstly, we observe that the retrieval state is characterised by a non-zero magnetization, while the non-retrieval state is characterised by a null, which means that the final configuration is completely random. Secondly, the Retrieval state may be perfect and thus have overlap equal to 1, or it may be in a Spurious state and retrieve only part of the pattern, thus having the\\n\\n$$T_{g}=1+{\\\\sqrt{\\\\alpha}}$$\\n\\n![22_image_0.png](22_image_0.png)\\n\\noverlap between different replicas be less than 1. However, this subdivision is more important from the point of view of model applications than from an analytical point of view.. The separation lines between the different states were derived by analytically solving the model for each grid point in order to divide the space (*α, T*) into the four regions. In more detail, we have that Tg = 1 + √α (45)\\nwhile Tc and TM were derived numerically by comparing the free energies of the pure states and those of the spin-glass states for the same α value.\\n\\n## 5 Audio Retrieval\\n\\nA practical application of the Hopfield model to a real dataset is now illustrated. The dataset consists of 81 voice recordings, in which all numbers from 0 to 80 are said. The format of these recordings is \".wav\" and the Code 1 in Python was used to transform voice patterns into binary patterns for the network to store. We note that the *Fourier transform* was used to reduce the dimensionality of the audio data; the parameters used (n-fft,hop-length) were set to (1024, 512) to maintain good audio quality and are powers of two because this provides an enormous computational advantage. Subsequently, an average was taken over each time instant so that a numerical vector of length 513 representing our audio data could be obtained. In the figure 7 the vector of the number 65 is represented. The second function of the algorithm creates a data structure in which the true audio signal with its sampling rate, matrix and mean vector of the Fourier transform is saved for each audio date. We can clearly see that the vector of coefficients is centred in 0, so the third function takes care of transforming the vector into a pattern with values in +1, −1 so that it can be handled by the Hopfield network.', 'This binarization method, although very crude, turns out to be sufficient for audio retrieval.\\n\\nHowever, the patterns do not turn out to be orthogonal: in fact, on average, each pattern has Figure 7: Representation of the average vector of Fourier coefficients of the number 65\\n\\n![23_image_0.png](23_image_0.png)\\n\\n1 import numpy as np 2 import librosa 4 def fft_signal ( audio_path ):\\n5 audio , sr = librosa . load ( audio_path ) 6 stft_signal = librosa . stft ( audio , n_fft =1024 , hop_length =512)\\n7 stft_coeff = np . mean ( stft_signal , axis =1) 8 return audio , sr , stft_signal , stft_coeff 10 def audio_importation (): 11 audio_objects = [] 12 for i in range (80): 13 file_path = f \\'/ Users / silver22 / registrazioni /{ i }. wav \\' 14 audio , sr , stft_signal , stft_coeff = fft_signal ( file_path ) 15 audio_object = { \\' audio \\': audio , \\' sr \\': sr , \\' stft_signal \\': \\\\ 16 stft_signal , \\' stft_coeff \\': stft_coeff }\\n17 audio_objects . append ( audio_object ) 18 return audio_objects 20 def audio_binarization ( stft_coeff ): 21 binary = ( stft_coeff > 0). astype (int ) 22 binary = 2 * binary - 1 23 return binary Listing 1: Dataset construction 19 1 import numpy as np 2 from tqdm import tqdm 3 import random 4 5 class HopfieldNetwork ( object ):\\n6 7 def train_weights ( self , train_data ):\\n8 print ( \" Start to train weights ... \" )\\n9 self . num_neuron = train_data . shape [1]\\n10 self . num_patterns = train_data . shape [0]\\n11 self . patterns = train_data 12 J = np . zeros (( self . num_neuron , self . num_neuron ))\\n13 for i in tqdm ( range (0 , self . num_neuron )):\\n14 for j in range ( i + 1 , self . num_neuron ): 15 for mu in range (0 , self . num_patterns ):\\n16 J [i , j ] += train_data [ mu , i ] * \\\\ 17 train_data [ mu , j ]\\n18 J = ( J + J . T ) / self . num_neuron 19 self . J = J\\n20 21 def predict ( self , test_data , temperature ):\\n22 sigma = test_data . copy (). T 23 sigma = sigma . T\\n24 N = self . num_neuron 25 K = self . num_patterns 26 alpha = K / N 27 T = temperature 28 beta = 1.0 / T 29 MCstat_step =50 30 MCrelax_step =1 31 magn_mattis_matrix = np . zeros (( self . num_patterns , \\\\ 32 MCstat_step )) 33 for stat in range (0 , MCstat_step ):\\n34 for step in range (0 , MCrelax_step ): 35 for i in range (0 , N ):\\n36 k = np . random . randint (0 , N ) 37 deltaE =2* sigma [ k ]* np . dot ( sigma , self . J [: , k ]) 38 ratio = np . exp ( - beta * deltaE ) 39 gamma = np . minimum ( ratio ,1) 40 if np .any ( random . uniform (0 ,1) < gamma ): 41 sigma [ k ] = - sigma [ k ] \\\\# flipping 42 for mu in range (0 , self . num_patterns ): 43 magn_mattis_matrix [ mu , stat ]= np . dot ( sigma , \\\\\\n44 self . patterns [ mu ,:])/ N 45 predicted = sigma 46 return predicted , magn_mattis_matrix Listing 2: Hopfield Network implementation 50% of the components equal to the other patterns. Let us now move on to the construction of the model. As we can see from Code 6, the Hopfield model can be implemented with two simple functions: the first (train-weights) takes as input a matrix containing the patterns to be stored as rows, and creates the J matrix of synaptic weights according to Hebb\\'s rule.\\n\\nThe second (predict), simulates the process of equation 10 using *Monte Carlo simulations* (see appendix B for further details). Now, let us analyse the performance of the pattern by doing the following test: we randomly take a pattern from among those stored, and corrupt it by inverting a percentage of components equal to a randomness r. We then feed this configuration to the model and compare the Mattis magnetisation relative to that specific pattern. Clearly, we will have that the model has successfully retrieved if m is about 1, while the prediction has failed if m is less than 0.5. We will use this heuristic to create graphs that play the same role as the phase diagram, i.e. we will say that we are in a\\n- Retrieval state if mµ is greater than 0.9\\n\\n- Spurious state if mµ is between 0.6 and 0.9\\n\\n## - Non-Retrieval State If Mµ Is Less Than 0.6.', '- Spurious state if mµ is between 0.6 and 0.9\\n\\n## - Non-Retrieval State If Mµ Is Less Than 0.6.\\n\\nIn more detail, we will analyse this process iteratively. That is, at each step a pattern is added\\n\\n![25_image_0.png](25_image_0.png) to the model and tested; in this way, we want to test the performance of the model as the load changes. In order to make the test as generic as possible, and considering that the voice patterns that make up the dataset have a strong correlation between them (think, for example, of the numbers 21, 22, etc...), we will perform this pattern addition randomly. At each step, i.e.\\n\\nat a fixed number of patterns, the model is made to work with a temperature ranging from 0.01 to 2. In the colourplots shown in the figure, the magnetization value at the end of the Monte Carlo simulation is used. In particular, we show the phase diagram with respect to different randomness values with which to corrupt the test data.\\n\\n![26_image_0.png](26_image_0.png)\\n\\n![26_image_1.png](26_image_1.png)\\n\\n![27_image_0.png](27_image_0.png)\\n\\n![27_image_1.png](27_image_1.png)\\n\\n1 import numpy as np 2 import matplotlib . pyplot as plt 3 from tqdm import tqdm 4 import random 5 def get_corrupted ( self , pattern , r ) : 6 sample_size = int( self . num_neuron * r )\\n7 I = np . random . choice (len( pattern ) , size = sample_size , \\\\ 8 replace = False )\\n9 corrupted = pattern . copy ()\\n10 for i in range (len( I ) ) : 11 corrupted [ I [ i ]] = -1* corrupted [ I [ i ]] 12 return corrupted 13 14 audio_object = audio_importation ()\\n15 num_patterns = 81 16 I = np . arange (81)\\n17 np . random . shuffle ( I ) 18 I_new = I . copy ()\\n19 patterns_bin_total_list = [] 20 for iter in range (2 , num_patterns +1) :\\n21 binary_list =[] 22 patterns_bin = np . zeros (( iter ,len ( audio_object [0] \\\\ 23 [ \\' stft_coeff \\' ]) ) ) 24 for i in range (0 , iter ) :\\n25 stft_coeff = audio_object [ I_new [ i ]][ \\' stft_coeff \\'] 26 binary = audiobin2 . audio_binarization ( stft_coeff )\\n27 patterns_bin [i ,:] = binary 28 patterns_bin_total_list . append ( patterns_bin )\\n29 30 model = HopfieldNetwork ()\\n31 T = T = np . linspace (0.01 , 2 , 80) 32 A = np . zeros (80) 33 for i in range (0 ,80) : 34 A [ i ] = ( i +2) /513 35 magns = np . zeros (( len( T ) ,len( A ) ) ) 36 for iter in range (0 , num_patterns -1) :\\n37 model . train_weights ( patterns_bin_total_list [ iter ]) 38 print ( \" We \\' re using \" ,iter +2 , \" patterns \" )\\n39 rand_test = np . random . choice ( range ( model . num_patterns ) ) 40 randomness = 0.2 41 test = patterns_bin [ rand_test ,:] 42 test_corrupted = get_corrupted ( test , randomness ) 43 for t in tqdm ( range (0 , len( T ) ) ) : 44 predicted , magnetization = model . predict ( test_corrupted , \\\\\\n45 temperature = T [ t ])\\n46 magns [t , iter ] = np .abs( magnetization [ rand_test , -1])\\n47 48 plt . pcolormesh (A , T , magns , cmap = \\' plasma \\')\\n49 plt . colorbar ()\\n50 plt . grid ( True , linestyle = \\' dashed \\' , linewidth =0.5)\\n51 plt . title ( \\' Phase Diagram \\')\\n52 plt . tight_layout ()\\n53 plt . show ()\\nListing 3: Creation of one colourplot with r=0.2\\n\\n## 6 Conclusion', \"## 6 Conclusion\\n\\nIn this last section we want to draw conclusions about the results obtained in the previous chapter. In particular, we want to highlight the limitations and strengths of the Hopfield model. Firstly, we can say that this model achieves astonishing results compared with its simplicity from both an implementation and a numerical point of view. However, the fact that one must necessarily work with patterns with values in {−1, +1} can be a problem in real life, as the binarization of real data may result in a significant loss of the information contained by them. In our case, it was possible to work well with audio data thanks to the Fourier transform, but this may not be the case with other types of data. Secondly, the 'empirical' phase diagrams obtained in Chapter 5 and the theoretical one obtained in Chapter 4 show a property of the model that is also common to the biological brain: there is a maximum limit of information that the network can store while maintaining good performance. This can be a great limitation in today's world, since the amount of data is increasingly large and therefore it would be computationally unsustainable to store an NxN matrix with *N >>* 1. Thirdly, we have seen from the various graphs in Chapter 5 that the model can recognise corrupted patterns up to a certain threshold of randomness; this means that if the network encounters a pattern that is excessively corrupted, it will not be possible to recover the original pattern. This may also be a limitation of the model, because there are currently other neural network models that are able to clean a data item from noise more efficiently (e.g. Autoencoders). Following the analysis of these model limitations, further models much more complex than Hopfield's are already being developed, such as Dense Associative Memories. In my humble opinion, these neural network models are fascinating and will continue to be in the spotlight of many researchers around the world, as, unlike almost all Deep Learning models, they have a very detailed mathematical background that allows for a theoretical analysis of the model's capabilities. Having a theory behind a model is very advantageous from a practical as well as an economic point of view, since it would be possible to choose the hyperparameters a priori and not having to test the model using many GPUs. In conclusion, the Hopfield model performs well as an associative memory model for image or audio data and is capable of recovering significantly corrupted patterns.\\n\\n## Appendix A: Self-Consistency Equations\\n\\nLet us analyse the self-consistency equations that characterise the calculation of the extreme points of free energy in the case of the Curie-Weiss model. The equation is m = tanh[β(Jm + h)]\\n\\n![29_image_1.png](29_image_1.png)\\n\\nand we show the solutions as the parameters *β, J, h* vary using the fixed point method. Notice\\n\\n![29_image_0.png](29_image_0.png)\\n\\nthat if β > 1 then T < 1 and we have more solutions, while if β ≤ 1 we have a unique solution.\\n\\n![30_image_0.png](30_image_0.png)\\n\\n![30_image_1.png](30_image_1.png)\\n\\n![30_image_2.png](30_image_2.png)\\n\\n## Appendix B: Monte Carlo Simulations\\n\\nWe explain step by step the Monte Carlo simulation used in the *predict* function in Code 6. We observe that the following process was used to perform sequential dynamics while optimising the computational cost.. Let's consider the simulation\\n\\n![31_image_0.png](31_image_0.png)\", '![31_image_0.png](31_image_0.png)\\n\\nThe simulation hyper-parameters are MCstat−*step* and MCrelax−*step*. The former indicates the number of steps to be taken before considering the final value as a statistic. The second, indicates how much you want to relax the process, i.e. how many steps you have to take so that the distribution associated with the network state is close to the Boltzmann-Gibbs distribution, which therefore minimises the free energy. These parameters must be chosen a posteriori, testing the convergence of the magnetisation as the parameters change. In our code, we decided not to relax the process and it turns out that 50 Monte Carlo steps are sufficient for the model to have convergence in the case of retrieval. The third for loop with respect to variable i corresponds to a single simulation step. The variable i takes values between 1 and N in such a way that all neurons in the network can be inverted according to the following strategy. At each i-step, a neuron is randomly chosen; then the energy contribution *deltaE* associated with that neuron is calculated. If *deltaE* is less than 0, for the minus in the Hamiltonian this gives a positive contribution and thus the state of the neuron will be flipped with probability 1. If *deltaE* is greater than 0, then the probability of finding the neuron in that state is calculated according to the usual formula associated with the Boltzmann-Gibbs distribution; then a random number is extracted in (0, 1) and the state of the neuron is inverted only if the number extracted is smaller than the probability calculated previously. This gives a chance to reverse the state of the neuron even though this new configuration does not lead to a decrease in energy. The sigma configuration that will emerge from these three chained cycles will correspond to the new configuration that will hopefully be equal to some fixed point that has been stored in the network.\\n\\n## Appendix C : Sherrington-Kirkpatrick Model\\n\\nThe Hamiltonian of the model is\\n\\n$$\\\\mathcal{H}_{N,J}^{S K}=-\\\\frac{1}{2\\\\sqrt{N}}\\\\sum_{i,j}J_{i j}\\\\sigma_{i}\\\\sigma_{j}$$\\n\\nwhere the synaptic weights are distributed as a standard Gaussian, i.e. Jij ∼ N (0, 1). Notice that the Hamiltonian depends on some random parameters whose probability is supposed to be known, which is why the model is well suited to analysing *disordered systems*. Normalisation with the square root is motivated by the fact that this gives *⟨H⟩ ∝* N. In the remainder of the appendix, we report the solution of the model using the *Replica Trick* with the *Replica* Symmetric Ansatz (see equation 37). We have\\n\\nE[Z n N,β,J ] = E \"X σ(1) ...X σ(n) exp −β Xn a=1 HN,β,J (σ (a)) !# = E  σ(1)...σ(n) exp      β 2 √N Xn a=1 X (i,j) Jijσ (a) i σ (a) j  X  = σ(1) ...X σ(n) Z\"Y i<j dJij √2π exp− J 2 ij 2 exp β Jij √N Xn a=1 σ (a) i σ (a) j !# = = X σ(1) ...X σ(n) Y i<j √2π √2π exp β 2 2N Xn a=1 Xn b=1 σ (a) i σ (a) j σ (b) i σ (b) j ! = X where in the last step we used the fact that\\nZdx exp−Ax2 + Bx= rπ A expB2 4A  with A = 1 2 and B = β √N Pn a=1 σ (a) i σ (a) j. Continuing the rewriting of the n-th moment of the partition function, we obtain that\\nE[Z n N,β,J ] = X σ(1) ...X σ(n) exp β 2 4N X i<j Xn a=1 Xn b=1 σ (a) i σ (a) j σ (b) i σ (b) j ! σ(1) ...X σ(n) exp β 2 4N X i,j Xn a=1 Xn 4N n 2N ! b=1 σ (a) i σ (a) j σ (b) i σ (b) j − β 2 = X σ(1) ...X σ(n) exp  4N n 2N  i σ (a) i σ (b) i !2 + β 2 2N X a̸=b  X  β 2 4N nN2 − β 2 = X  σ(1) ...X σ(n) Y a<b exp  i σ (a) i σ (b) i !2 2N  X = expβ 2  β 2 4 n(N − n) X \\nand we use the Gaussian result in the other direction with B = β', \"$$\\\\tau=\\\\beta^{2}\\\\sum_{i}\\\\sigma_{i}^{(a)}\\\\sigma_{i}^{(b)}\\\\ ,\\\\,A=\\\\frac{\\\\beta^{2}N}{2}\\\\ \\\\mathrm{so}$$\\n$$\\\\mathbb{E}[Z_{N,\\\\beta,I}^{u}]=\\\\exp\\\\biggl({\\\\frac{\\\\beta^{2}}{4}}n(N-n)\\\\biggr)\\\\int\\\\sum_{\\\\sigma^{(1)}}\\\\cdots\\\\sum_{\\\\sigma^{(N)}}\\\\prod_{a<b}{\\\\frac{d Q_{a b}}{\\\\sqrt{\\\\frac{\\\\beta\\\\pi}{\\\\beta^{2}}}}}\\\\exp\\\\biggl(-{\\\\frac{\\\\beta^{2}N}{2}}Q_{a b}^{2}\\\\biggr)\\\\exp\\\\biggl(\\\\beta^{2}Q_{a b}\\\\sum_{i}\\\\sigma_{i}^{(a)}\\\\sigma_{i}^{(b)}\\\\biggr).$$\\n\\n33 Continuing in this way we obtain\\n\\nσ(1) ...X σ(n) Y N E[Z n N,β,J ] = e β 2 4 n(N−n) Z Y a<b dQ q ab 2π β2 e − β 2N 2 Q2ab X i=1 e β 2 Pa<b Qabσ (a) iσ (b) i a<b dQ q ab 2π β2 e − β 2N 2 Q2ab Y N = e β 2 4 n(N−n) Z Y σ(1) ...X σ(n) e β 2 Pa<b Qabσ (a) iσ (b) i i=1 X σ(1) ...X σ(n) e β 2 Pa<b Qabσ (a) iσ (b) i !N a<b dQ q ab 2π β2 e − β 2N 2 Q2ab X = e β 2 4 n(N−n) Z Y = e β 2 4 n(N−n) Z Y a<b dQ q ab 2π β2   e − β 2N 2Pa<b Q2ab exp N logX σ(1) ...X σ(n) e β 2 Pa<b Qabσ (a) iσ (b) i ! .\\nIn conclusion, we derived that\\n\\n$$\\\\mathbb{E}[Z_{N,\\\\beta,J}^{n}]=\\\\int\\\\prod_{a<b}\\\\frac{d Q_{a b}}{\\\\sqrt{\\\\frac{2\\\\pi}{\\\\beta^{2}}}}\\\\exp\\\\{-N{\\\\mathcal{A}}[Q]\\\\}$$\\n\\nwhere the argument of the exponential is equal to\\n\\n$${\\\\mathcal{A}}[Q]=-{\\\\frac{\\\\beta^{2}}{4}}n(N-n)+{\\\\frac{\\\\beta^{2}}{2}}\\\\sum_{a<b}Q_{a b}^{2}-\\\\log\\\\left(\\\\sum_{\\\\sigma^{(1)}\\\\ldots\\\\sigma^{(1)}}e^{\\\\beta^{2}\\\\sum_{a<b}Q_{a b}\\\\sigma_{i}^{(a)}\\\\sigma_{i}^{(b)}}\\\\right).$$\\n\\nNow, we want to apply the RS-ansatz, i.e. Qab = 1(a = b) + q1(a ̸= b). Thus\\n\\n$${\\\\mathcal A}[Q]=-{\\\\frac{\\\\beta^{2}}{4}}n(N-n)+{\\\\frac{\\\\beta^{2}}{2}}\\\\sum_{a<b}q^{2}-\\\\log\\\\left(\\\\sum_{\\\\sigma^{(1)}\\\\ldots\\\\sigma^{(1)}}e^{\\\\beta^{2}q\\\\sum_{a<b}\\\\sigma_{i}^{(a)}\\\\sigma_{i}^{(b)}}\\\\right).$$\\n\\nIf we assume commutativity of the limits for N,n then we obtain\\n\\n$$A_{\\\\beta}^{Q}=\\\\operatorname*{lim}_{n\\\\to0}\\\\frac{1}{n}\\\\operatorname*{lim}_{N\\\\to\\\\infty}\\\\frac{1}{N}(\\\\mathbb{E}[Z_{N,\\\\beta,J}^{n}]-1)=-\\\\operatorname*{lim}_{n\\\\to0}\\\\frac{1}{n}\\\\mathcal{A}[Q^{*}]$$\\n\\nwhere we used Laplace's method and Q∗ = argmin A[Q]). By calculating the extremal point of A and doing the limit for n, we arrive at the formula\\n\\n$$A_{\\\\beta}^{Q,R S}=\\\\frac{\\\\beta^{2}}{4}(1-q^{R S})^{2}+\\\\log2+\\\\log\\\\int d\\\\mu(z)\\\\log\\\\cosh\\\\Bigl(\\\\beta z\\\\sqrt{q^{R S}}\\\\Bigr)$$\\n\\nwith q RS that satisfies the SC-equation\\n\\n$$q^{R S}=\\\\int d\\\\mu(z)\\\\operatorname{tanh}^{2}(\\\\beta z\\\\sqrt{q^{R S}}).$$\\n\\n## References\\n\\n[1] Agliari, E. (2023). Modelli di Reti Neurali. Universit`a degli Studi di Roma La Sapienza.\\n\\n[2] Feng, J., Tirozzi, B. (1996). Capacity of the Hopfield model. Statistics Group, The Babraham Institute, Cambridge and Mathematisches Institut, Universit¨at M¨unchen, University of Rome La Sapienza.\\n\\n[3] Albor`e, N., Tubito, A. (2022). Biology and Machine Learning. Universit`a degli Studi di Roma La Sapienza.\\n\\n[4] Bellina, A. (2021). Modello di Hopfield con Diluizione Sinaptica Simmetrica Casuale. Universit`a degli Studi di Roma La Sapienza.\\n\\n[5] Agliari, E., Alemanno, F., Barra, A., Facchetti, A. (2020). Generalized Guerra's interpolation schemes for dense associative neural networks. Dipartimento di Matematica Guido Castelnuovo, Dipartimento di Matematica e Fisica Ennio De Giorgi, Universit`a del Salento, C.N.R. Nanotec Lecce, I.N.F.N., Sezione di Lecce.\\n\\n[6] Patti, A. (2022). Retrieving mismatched memory patterns in the Hopfield model of neural networks. Universit`a degli Studi di Roma La Sapienza.\\n\\n[7] Haykin, S. (1999). Neural Networks and Learning Machines, 3rd ed. Wiley. [8] Negri, M., Lauditi, C., Perugini, G., Lucibello, C. and Malatesta, E. M. (2023). Storage and Learning Phase Transitions in the Random-Features Hopfield Model. University of Rome La Sapienza, Bocconi University.\"]\n"
     ]
    }
   ],
   "source": [
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3846, 3678, 3961, 3779, 3915, 3916, 3840, 3925, 3982, 3655, 3830, 3948, 3574, 3841, 3811, 3985, 3987, 3097, 3458, 3593, 3568]\n"
     ]
    }
   ],
   "source": [
    "print([len(chunk) for chunk in chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store into index_format (json temperary version 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def create_vecindex(source_path, index_path, chunk_size=8000, overlap=100):\n",
    "    # Search for markdown files in source_path recursively\n",
    "    markdown_files = []\n",
    "    for root, dirs, files in os.walk(source_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.md'):\n",
    "                markdown_files.append(os.path.join(root, file))\n",
    "    print(markdown_files)\n",
    "    \n",
    "    model = SentenceTransformer('Alibaba-NLP/gte-large-en-v1.5', trust_remote_code=True)\n",
    "    \n",
    "    if not os.path.exists(index_path):\n",
    "        os.makedirs(index_path)\n",
    "        \n",
    "    for md_file in markdown_files:\n",
    "        chunks = convert_markdown_to_chunks(md_file, chunk_size, overlap)\n",
    "        pure_filename = os.path.splitext(os.path.basename(md_file))[0]\n",
    "        chunk_filename = os.path.join(index_path, pure_filename + '_chunks.csv')\n",
    "        vector_filename = os.path.join(index_path, pure_filename + '_vectors.npy')\n",
    "        embeddings = []\n",
    "        for chunk_id in range(0, len((chunks)), 8):\n",
    "            embeddings.extend(model.encode(chunks[chunk_id:chunk_id+8]))\n",
    "        with open(chunk_filename, 'w', newline='') as csvfile:\n",
    "            fieldnames = ['id', 'chunk']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "            writer.writeheader()\n",
    "\n",
    "            chunk_format = lambda string: repr(string)\n",
    "\n",
    "            for i in range(len(chunks)):\n",
    "                writer.writerow({'id': i, 'chunk': chunk_format(chunks[i])})\n",
    "        np.save(vector_filename, embeddings)\n",
    "        \n",
    "        # store chunks into \n",
    "        torch.cuda.empty_cache() \n",
    "        \n",
    "    return csv_result\n",
    "\n",
    "    with open(index_file, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['id', 'chunk', 'embedding']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        writer.writeheader()\n",
    "\n",
    "        for md_file in markdown_files:\n",
    "            chunks = convert_markdown_to_chunks(md_file, chunk_size, overlap)\n",
    "            embeddings = get_embeddings(chunks)\n",
    "\n",
    "            for i in range(len(chunks)):\n",
    "                writer.writerow({'id': id, 'chunk': chunks[i], 'embedding': embeddings[i]})\n",
    "                id += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/ocr/ocr_2024-06-03/2405-20467_1/2405-20467_1.md', '../data/ocr/ocr_2024-06-03/2405-20662_1/2405-20662_1.md', '../data/ocr/ocr_2024-06-03/2405-20715_1/2405-20715_1.md', '../data/ocr/ocr_2024-06-03/2405-20750_1/2405-20750_1.md', '../data/ocr/ocr_2024-06-03/2405-20630_1/2405-20630_1.md', '../data/ocr/ocr_2024-06-03/2405-20525_1/2405-20525_1.md', '../data/ocr/ocr_2024-06-03/2405-21009_1/2405-21009_1.md', '../data/ocr/ocr_2024-06-03/2405-21051_1/2405-21051_1.md', '../data/ocr/ocr_2024-06-03/2405-20559_1/2405-20559_1.md', '../data/ocr/ocr_2024-06-03/2405-20524_1/2405-20524_1.md', '../data/ocr/ocr_2024-06-03/2405-21040_1/2405-21040_1.md', '../data/ocr/ocr_2024-06-03/2405-20423_1/2405-20423_1.md', '../data/ocr/ocr_2024-06-03/2405-20456_1/2405-20456_1.md', '../data/ocr/ocr_2024-06-03/2405-21066_1/2405-21066_1.md', '../data/ocr/ocr_2024-06-03/2405-20502_1/2405-20502_1.md', '../data/ocr/ocr_2024-06-03/2405-20603_1/2405-20603_1.md', '../data/ocr/ocr_2024-06-03/2405-20980_1/2405-20980_1.md', '../data/ocr/ocr_2024-06-03/2405-20967_1/2405-20967_1.md', '../data/ocr/ocr_2024-06-03/2405-20482_1/2405-20482_1.md', '../data/ocr/ocr_2024-06-03/2405-21027_1/2405-21027_1.md', '../data/ocr/ocr_2024-06-03/2405-20725_1/2405-20725_1.md', '../data/ocr/ocr_2024-06-03/2405-20972_1/2405-20972_1.md', '../data/ocr/ocr_2024-06-03/2405-21022_1/2405-21022_1.md', '../data/ocr/ocr_2024-06-03/2405-20983_1/2405-20983_1.md', '../data/ocr/ocr_2024-06-03/2405-20527_1/2405-20527_1.md', '../data/ocr/ocr_2024-06-03/2405-20574_1/2405-20574_1.md', '../data/ocr/ocr_2024-06-03/2405-20729_1/2405-20729_1.md', '../data/ocr/ocr_2024-06-03/2405-20354_1/2405-20354_1.md', '../data/ocr/ocr_2024-06-03/2405-20592_1/2405-20592_1.md', '../data/ocr/ocr_2024-06-03/2405-20877_1/2405-20877_1.md', '../data/ocr/ocr_2024-06-03/2405-20902_1/2405-20902_1.md', '../data/ocr/ocr_2024-06-03/2405-20678_1/2405-20678_1.md', '../data/ocr/ocr_2024-06-03/2405-20969_1/2405-20969_1.md', '../data/ocr/ocr_2024-06-03/2405-21021_1/2405-21021_1.md', '../data/ocr/ocr_2024-06-03/2405-20968_1/2405-20968_1.md', '../data/ocr/ocr_2024-06-03/2405-20455_1/2405-20455_1.md', '../data/ocr/ocr_2024-06-03/2405-20849_1/2405-20849_1.md', '../data/ocr/ocr_2024-06-03/2405-20806_1/2405-20806_1.md', '../data/ocr/ocr_2024-06-03/2405-20993_1/2405-20993_1.md', '../data/ocr/ocr_2024-06-03/2405-20776_1/2405-20776_1.md', '../data/ocr/ocr_2024-06-03/2405-20560_1/2405-20560_1.md', '../data/ocr/ocr_2024-06-03/2405-21018_1/2405-21018_1.md', '../data/ocr/ocr_2024-06-03/2405-20999_1/2405-20999_1.md', '../data/ocr/ocr_2024-06-03/2405-21025_1/2405-21025_1.md', '../data/ocr/ocr_2024-06-03/2405-20654_1/2405-20654_1.md', '../data/ocr/ocr_2024-06-03/2405-20481_1/2405-20481_1.md', '../data/ocr/ocr_2024-06-03/2405-20668_1/2405-20668_1.md', '../data/ocr/ocr_2024-06-03/2405-20685_1/2405-20685_1.md', '../data/ocr/ocr_2024-06-03/2405-20420_1/2405-20420_1.md', '../data/ocr/ocr_2024-06-03/2405-20905_1/2405-20905_1.md', '../data/ocr/ocr_2024-06-03/2405-20804_1/2405-20804_1.md', '../data/ocr/ocr_2024-06-03/2405-20748_1/2405-20748_1.md', '../data/ocr/ocr_2024-06-03/2405-21036_1/2405-21036_1.md', '../data/ocr/ocr_2024-06-03/2405-20664_1/2405-20664_1.md', '../data/ocr/ocr_2024-06-03/2405-20389_1/2405-20389_1.md', '../data/ocr/ocr_2024-06-03/2405-20775_1/2405-20775_1.md', '../data/ocr/ocr_2024-06-03/2405-20531_1/2405-20531_1.md', '../data/ocr/ocr_2024-06-03/2405-20778_1/2405-20778_1.md', '../data/ocr/ocr_2024-06-03/2405-20800_1/2405-20800_1.md', '../data/ocr/ocr_2024-06-03/2405-20701_1/2405-20701_1.md', '../data/ocr/ocr_2024-06-03/2405-20852_1/2405-20852_1.md', '../data/ocr/ocr_2024-06-03/2405-20387_1/2405-20387_1.md', '../data/ocr/ocr_2024-06-03/2405-20799_1/2405-20799_1.md', '../data/ocr/ocr_2024-06-03/2405-20863_1/2405-20863_1.md', '../data/ocr/ocr_2024-06-03/2405-20521_1/2405-20521_1.md', '../data/ocr/ocr_2024-06-03/2405-20620_1/2405-20620_1.md', '../data/ocr/ocr_2024-06-03/2405-20530_1/2405-20530_1.md', '../data/ocr/ocr_2024-06-03/2405-21003_1/2405-21003_1.md', '../data/ocr/ocr_2024-06-03/2405-20848_1/2405-20848_1.md', '../data/ocr/ocr_2024-06-03/2405-20786_1/2405-20786_1.md', '../data/ocr/ocr_2024-06-03/2405-20404_1/2405-20404_1.md', '../data/ocr/ocr_2024-06-03/2405-21015_1/2405-21015_1.md', '../data/ocr/ocr_2024-06-03/2405-20384_1/2405-20384_1.md', '../data/ocr/ocr_2024-06-03/2405-20562_1/2405-20562_1.md', '../data/ocr/ocr_2024-06-03/2405-20819_1/2405-20819_1.md', '../data/ocr/ocr_2024-06-03/2405-21034_1/2405-21034_1.md', '../data/ocr/ocr_2024-06-03/2405-20684_1/2405-20684_1.md', '../data/ocr/ocr_2024-06-03/2405-20859_1/2405-20859_1.md', '../data/ocr/ocr_2024-06-03/2405-20414_1/2405-20414_1.md', '../data/ocr/ocr_2024-06-03/2405-20447_1/2405-20447_1.md', '../data/ocr/ocr_2024-06-03/2405-21004_1/2405-21004_1.md', '../data/ocr/ocr_2024-06-03/2405-20805_1/2405-20805_1.md', '../data/ocr/ocr_2024-06-03/2405-20738_1/2405-20738_1.md', '../data/ocr/ocr_2024-06-03/2405-20915_1/2405-20915_1.md', '../data/ocr/ocr_2024-06-03/2405-20593_1/2405-20593_1.md', '../data/ocr/ocr_2024-06-03/2405-20830_1/2405-20830_1.md', '../data/ocr/ocr_2024-06-03/2405-20485_1/2405-20485_1.md', '../data/ocr/ocr_2024-06-03/2405-20682_1/2405-20682_1.md', '../data/ocr/ocr_2024-06-03/2405-20833_1/2405-20833_1.md', '../data/ocr/ocr_2024-06-03/2405-20413_1/2405-20413_1.md', '../data/ocr/ocr_2024-06-03/2405-20648_1/2405-20648_1.md', '../data/ocr/ocr_2024-06-03/2405-20633_1/2405-20633_1.md', '../data/ocr/ocr_2024-06-03/2405-20363_1/2405-20363_1.md', '../data/ocr/ocr_2024-06-03/2405-20988_1/2405-20988_1.md', '../data/ocr/ocr_2024-06-03/2405-20904_1/2405-20904_1.md', '../data/ocr/ocr_2024-06-03/2405-20981_1/2405-20981_1.md', '../data/ocr/ocr_2024-06-03/2405-20516_1/2405-20516_1.md', '../data/ocr/ocr_2024-06-03/2405-20882_1/2405-20882_1.md', '../data/ocr/ocr_2024-06-03/2405-20670_1/2405-20670_1.md', '../data/ocr/ocr_2024-06-03/2405-20733_1/2405-20733_1.md', '../data/ocr/ocr_2024-06-03/2405-20534_1/2405-20534_1.md', '../data/ocr/ocr_2024-06-03/2405-20368_1/2405-20368_1.md', '../data/ocr/ocr_2024-06-03/2405-20918_1/2405-20918_1.md', '../data/ocr/ocr_2024-06-03/2405-20380_1/2405-20380_1.md', '../data/ocr/ocr_2024-06-03/2405-20350_1/2405-20350_1.md', '../data/ocr/ocr_2024-06-03/2405-20446_1/2405-20446_1.md', '../data/ocr/ocr_2024-06-03/2405-20628_1/2405-20628_1.md', '../data/ocr/ocr_2024-06-03/2405-20505_1/2405-20505_1.md', '../data/ocr/ocr_2024-06-03/2405-21013_1/2405-21013_1.md', '../data/ocr/ocr_2024-06-03/2405-20687_1/2405-20687_1.md', '../data/ocr/ocr_2024-06-03/2405-20641_1/2405-20641_1.md', '../data/ocr/ocr_2024-06-03/2405-21005_1/2405-21005_1.md', '../data/ocr/ocr_2024-06-03/2405-20763_1/2405-20763_1.md', '../data/ocr/ocr_2024-06-03/2405-20727_1/2405-20727_1.md', '../data/ocr/ocr_2024-06-03/2405-21016_1/2405-21016_1.md', '../data/ocr/ocr_2024-06-03/2405-20471_1/2405-20471_1.md', '../data/ocr/ocr_2024-06-03/2405-20759_1/2405-20759_1.md', '../data/ocr/ocr_2024-06-03/2405-20868_1/2405-20868_1.md', '../data/ocr/ocr_2024-06-03/2405-20600_1/2405-20600_1.md', '../data/ocr/ocr_2024-06-03/2405-20860_1/2405-20860_1.md', '../data/ocr/ocr_2024-06-03/2405-20914_1/2405-20914_1.md', '../data/ocr/ocr_2024-06-03/2405-20820_1/2405-20820_1.md', '../data/ocr/ocr_2024-06-03/2405-20487_1/2405-20487_1.md', '../data/ocr/ocr_2024-06-03/2405-20815_1/2405-20815_1.md', '../data/ocr/ocr_2024-06-03/2405-20705_1/2405-20705_1.md', '../data/ocr/ocr_2024-06-03/2405-20573_1/2405-20573_1.md', '../data/ocr/ocr_2024-06-03/2405-20452_1/2405-20452_1.md', '../data/ocr/ocr_2024-06-03/2405-20653_1/2405-20653_1.md', '../data/ocr/ocr_2024-06-03/2405-20488_1/2405-20488_1.md', '../data/ocr/ocr_2024-06-03/2405-20618_1/2405-20618_1.md', '../data/ocr/ocr_2024-06-03/2405-20496_1/2405-20496_1.md', '../data/ocr/ocr_2024-06-03/2405-20561_1/2405-20561_1.md', '../data/ocr/ocr_2024-06-03/2405-20445_1/2405-20445_1.md', '../data/ocr/ocr_2024-06-03/2405-20355_1/2405-20355_1.md', '../data/ocr/ocr_2024-06-03/2405-20614_1/2405-20614_1.md', '../data/ocr/ocr_2024-06-03/2405-20513_1/2405-20513_1.md', '../data/ocr/ocr_2024-06-03/2405-20753_1/2405-20753_1.md', '../data/ocr/ocr_2024-06-03/2405-20584_1/2405-20584_1.md', '../data/ocr/ocr_2024-06-03/2405-20661_1/2405-20661_1.md', '../data/ocr/ocr_2024-06-03/2405-20785_1/2405-20785_1.md', '../data/ocr/ocr_2024-06-03/2405-20486_1/2405-20486_1.md', '../data/ocr/ocr_2024-06-03/2405-20771_1/2405-20771_1.md', '../data/ocr/ocr_2024-06-03/2405-20917_1/2405-20917_1.md', '../data/ocr/ocr_2024-06-03/2405-20642_1/2405-20642_1.md', '../data/ocr/ocr_2024-06-03/2405-20351_1/2405-20351_1.md', '../data/ocr/ocr_2024-06-03/2405-21045_1/2405-21045_1.md', '../data/ocr/ocr_2024-06-03/2405-20433_1/2405-20433_1.md', '../data/ocr/ocr_2024-06-03/2405-20777_1/2405-20777_1.md', '../data/ocr/ocr_2024-06-03/2405-20424_1/2405-20424_1.md', '../data/ocr/ocr_2024-06-03/2405-20671_1/2405-20671_1.md', '../data/ocr/ocr_2024-06-03/2405-20858_1/2405-20858_1.md', '../data/ocr/ocr_2024-06-03/2405-20881_1/2405-20881_1.md', '../data/ocr/ocr_2024-06-03/2405-20457_1/2405-20457_1.md', '../data/ocr/ocr_2024-06-03/2405-21046_1/2405-21046_1.md', '../data/ocr/ocr_2024-06-03/2405-20649_1/2405-20649_1.md', '../data/ocr/ocr_2024-06-03/2405-20916_1/2405-20916_1.md', '../data/ocr/ocr_2024-06-03/2405-21012_1/2405-21012_1.md', '../data/ocr/ocr_2024-06-03/2405-20503_1/2405-20503_1.md', '../data/ocr/ocr_2024-06-03/2405-20494_1/2405-20494_1.md', '../data/ocr/ocr_2024-06-03/2405-20970_1/2405-20970_1.md', '../data/ocr/ocr_2024-06-03/2405-20362_1/2405-20362_1.md', '../data/ocr/ocr_2024-06-03/2405-20681_1/2405-20681_1.md', '../data/ocr/ocr_2024-06-03/2405-20646_1/2405-20646_1.md', '../data/ocr/ocr_2024-06-03/2405-21048_1/2405-21048_1.md', '../data/ocr/ocr_2024-06-03/2405-20459_1/2405-20459_1.md', '../data/ocr/ocr_2024-06-03/2405-20458_1/2405-20458_1.md', '../data/ocr/ocr_2024-06-03/2405-20947_1/2405-20947_1.md', '../data/ocr/ocr_2024-06-03/2405-20449_1/2405-20449_1.md', '../data/ocr/ocr_2024-06-03/2405-20539_1/2405-20539_1.md', '../data/ocr/ocr_2024-06-03/2405-20470_1/2405-20470_1.md', '../data/ocr/ocr_2024-06-03/2405-20842_1/2405-20842_1.md', '../data/ocr/ocr_2024-06-03/2405-20512_1/2405-20512_1.md', '../data/ocr/ocr_2024-06-03/2405-20392_1/2405-20392_1.md', '../data/ocr/ocr_2024-06-03/2405-20892_1/2405-20892_1.md', '../data/ocr/ocr_2024-06-03/2405-20526_1/2405-20526_1.md', '../data/ocr/ocr_2024-06-03/2405-20887_1/2405-20887_1.md', '../data/ocr/ocr_2024-06-03/2405-20656_1/2405-20656_1.md', '../data/ocr/ocr_2024-06-03/2405-20582_1/2405-20582_1.md', '../data/ocr/ocr_2024-06-03/2405-21030_1/2405-21030_1.md', '../data/ocr/ocr_2024-06-03/2405-20838_1/2405-20838_1.md', '../data/ocr/ocr_2024-06-03/2405-20680_1/2405-20680_1.md', '../data/ocr/ocr_2024-06-03/2405-20565_1/2405-20565_1.md', '../data/ocr/ocr_2024-06-03/2405-20576_1/2405-20576_1.md', '../data/ocr/ocr_2024-06-03/2405-20489_1/2405-20489_1.md', '../data/ocr/ocr_2024-06-03/2405-20643_1/2405-20643_1.md', '../data/ocr/ocr_2024-06-03/2405-20867_1/2405-20867_1.md', '../data/ocr/ocr_2024-06-03/2405-20358_1/2405-20358_1.md', '../data/ocr/ocr_2024-06-03/2405-20590_1/2405-20590_1.md', '../data/ocr/ocr_2024-06-03/2405-20501_1/2405-20501_1.md', '../data/ocr/ocr_2024-06-03/2405-20782_1/2405-20782_1.md', '../data/ocr/ocr_2024-06-03/2405-20585_1/2405-20585_1.md', '../data/ocr/ocr_2024-06-03/2405-20697_1/2405-20697_1.md', '../data/ocr/ocr_2024-06-03/2405-20869_1/2405-20869_1.md', '../data/ocr/ocr_2024-06-03/2405-20834_1/2405-20834_1.md', '../data/ocr/ocr_2024-06-03/2405-20640_1/2405-20640_1.md', '../data/ocr/ocr_2024-06-03/2405-20797_1/2405-20797_1.md', '../data/ocr/ocr_2024-06-03/2405-20624_1/2405-20624_1.md', '../data/ocr/ocr_2024-06-03/2405-20933_1/2405-20933_1.md', '../data/ocr/ocr_2024-06-03/2405-21074_1/2405-21074_1.md', '../data/ocr/ocr_2024-06-03/2405-20779_1/2405-20779_1.md', '../data/ocr/ocr_2024-06-03/2405-20529_1/2405-20529_1.md', '../data/ocr/ocr_2024-06-03/2405-20364_1/2405-20364_1.md', '../data/ocr/ocr_2024-06-03/2405-20504_1/2405-20504_1.md', '../data/ocr/ocr_2024-06-03/2405-20818_1/2405-20818_1.md', '../data/ocr/ocr_2024-06-03/2405-20978_1/2405-20978_1.md', '../data/ocr/ocr_2024-06-03/2405-21028_1/2405-21028_1.md', '../data/ocr/ocr_2024-06-03/2405-20495_1/2405-20495_1.md', '../data/ocr/ocr_2024-06-03/2405-20607_1/2405-20607_1.md', '../data/ocr/ocr_2024-06-03/2405-20400_1/2405-20400_1.md', '../data/ocr/ocr_2024-06-03/2405-20568_1/2405-20568_1.md', '../data/ocr/ocr_2024-06-03/2405-20519_1/2405-20519_1.md', '../data/ocr/ocr_2024-06-03/2405-20468_1/2405-20468_1.md', '../data/ocr/ocr_2024-06-03/2405-20439_1/2405-20439_1.md', '../data/ocr/ocr_2024-06-03/2405-20985_1/2405-20985_1.md', '../data/ocr/ocr_2024-06-03/2405-20625_1/2405-20625_1.md', '../data/ocr/ocr_2024-06-03/2405-20851_1/2405-20851_1.md', '../data/ocr/ocr_2024-06-03/2405-20542_1/2405-20542_1.md', '../data/ocr/ocr_2024-06-03/2405-20508_1/2405-20508_1.md', '../data/ocr/ocr_2024-06-03/2405-20959_1/2405-20959_1.md', '../data/ocr/ocr_2024-06-03/2405-21064_1/2405-21064_1.md', '../data/ocr/ocr_2024-06-03/2405-21075_1/2405-21075_1.md', '../data/ocr/ocr_2024-06-03/2405-20962_1/2405-20962_1.md', '../data/ocr/ocr_2024-06-03/2405-20791_1/2405-20791_1.md', '../data/ocr/ocr_2024-06-03/2405-20710_1/2405-20710_1.md', '../data/ocr/ocr_2024-06-03/2405-20675_1/2405-20675_1.md', '../data/ocr/ocr_2024-06-03/2405-20652_1/2405-20652_1.md', '../data/ocr/ocr_2024-06-03/2405-20975_1/2405-20975_1.md', '../data/ocr/ocr_2024-06-03/2405-20810_1/2405-20810_1.md', '../data/ocr/ocr_2024-06-03/2405-20397_1/2405-20397_1.md', '../data/ocr/ocr_2024-06-03/2405-20465_1/2405-20465_1.md', '../data/ocr/ocr_2024-06-03/2405-20772_1/2405-20772_1.md', '../data/ocr/ocr_2024-06-03/2405-20884_1/2405-20884_1.md', '../data/ocr/ocr_2024-06-03/2405-20896_1/2405-20896_1.md', '../data/ocr/ocr_2024-06-03/2405-20847_1/2405-20847_1.md', '../data/ocr/ocr_2024-06-03/2405-21023_1/2405-21023_1.md', '../data/ocr/ocr_2024-06-03/2405-20795_1/2405-20795_1.md', '../data/ocr/ocr_2024-06-03/2405-20954_1/2405-20954_1.md', '../data/ocr/ocr_2024-06-03/2405-20883_1/2405-20883_1.md', '../data/ocr/ocr_2024-06-03/2405-20611_1/2405-20611_1.md', '../data/ocr/ocr_2024-06-03/2405-20540_1/2405-20540_1.md', '../data/ocr/ocr_2024-06-03/2405-20910_1/2405-20910_1.md', '../data/ocr/ocr_2024-06-03/2405-20745_1/2405-20745_1.md', '../data/ocr/ocr_2024-06-03/2405-20419_1/2405-20419_1.md', '../data/ocr/ocr_2024-06-03/2405-20441_1/2405-20441_1.md', '../data/ocr/ocr_2024-06-03/2405-20900_1/2405-20900_1.md', '../data/ocr/ocr_2024-06-03/2405-21068_1/2405-21068_1.md', '../data/ocr/ocr_2024-06-03/2405-20717_1/2405-20717_1.md', '../data/ocr/ocr_2024-06-03/2405-20669_1/2405-20669_1.md', '../data/ocr/ocr_2024-06-03/2405-20469_1/2405-20469_1.md', '../data/ocr/ocr_2024-06-03/2405-20692_1/2405-20692_1.md', '../data/ocr/ocr_2024-06-03/2405-21043_1/2405-21043_1.md', '../data/ocr/ocr_2024-06-03/2405-21044_1/2405-21044_1.md', '../data/ocr/ocr_2024-06-03/2405-20672_1/2405-20672_1.md', '../data/ocr/ocr_2024-06-03/2405-20879_1/2405-20879_1.md', '../data/ocr/ocr_2024-06-03/2405-20861_1/2405-20861_1.md', '../data/ocr/ocr_2024-06-03/2405-20704_1/2405-20704_1.md', '../data/ocr/ocr_2024-06-03/2405-20500_1/2405-20500_1.md', '../data/ocr/ocr_2024-06-03/2405-20622_1/2405-20622_1.md', '../data/ocr/ocr_2024-06-03/2405-20555_1/2405-20555_1.md', '../data/ocr/ocr_2024-06-03/2405-20986_1/2405-20986_1.md', '../data/ocr/ocr_2024-06-03/2405-20407_1/2405-20407_1.md', '../data/ocr/ocr_2024-06-03/2405-20426_1/2405-20426_1.md', '../data/ocr/ocr_2024-06-03/2405-20755_1/2405-20755_1.md', '../data/ocr/ocr_2024-06-03/2405-20435_1/2405-20435_1.md', '../data/ocr/ocr_2024-06-03/2405-20711_1/2405-20711_1.md', '../data/ocr/ocr_2024-06-03/2405-20990_1/2405-20990_1.md', '../data/ocr/ocr_2024-06-03/2405-21070_1/2405-21070_1.md', '../data/ocr/ocr_2024-06-03/2405-20690_1/2405-20690_1.md', '../data/ocr/ocr_2024-06-03/2405-20390_1/2405-20390_1.md', '../data/ocr/ocr_2024-06-03/2405-20605_1/2405-20605_1.md', '../data/ocr/ocr_2024-06-03/2405-20430_1/2405-20430_1.md', '../data/ocr/ocr_2024-06-03/2405-20429_1/2405-20429_1.md', '../data/ocr/ocr_2024-06-03/2405-20694_1/2405-20694_1.md', '../data/ocr/ocr_2024-06-03/2405-20348_1/2405-20348_1.md', '../data/ocr/ocr_2024-06-03/2405-20906_1/2405-20906_1.md', '../data/ocr/ocr_2024-06-03/2405-20735_1/2405-20735_1.md', '../data/ocr/ocr_2024-06-03/2405-20821_1/2405-20821_1.md', '../data/ocr/ocr_2024-06-03/2405-20774_1/2405-20774_1.md', '../data/ocr/ocr_2024-06-03/2405-20846_1/2405-20846_1.md', '../data/ocr/ocr_2024-06-03/2405-20448_1/2405-20448_1.md', '../data/ocr/ocr_2024-06-03/2405-20608_1/2405-20608_1.md', '../data/ocr/ocr_2024-06-03/2405-20982_1/2405-20982_1.md', '../data/ocr/ocr_2024-06-03/2405-20462_1/2405-20462_1.md', '../data/ocr/ocr_2024-06-03/2405-20412_1/2405-20412_1.md', '../data/ocr/ocr_2024-06-03/2405-21060_1/2405-21060_1.md', '../data/ocr/ocr_2024-06-03/2405-20538_1/2405-20538_1.md', '../data/ocr/ocr_2024-06-03/2405-21055_1/2405-21055_1.md', '../data/ocr/ocr_2024-06-03/2405-20770_1/2405-20770_1.md', '../data/ocr/ocr_2024-06-03/2405-20703_1/2405-20703_1.md', '../data/ocr/ocr_2024-06-03/2405-20612_1/2405-20612_1.md', '../data/ocr/ocr_2024-06-03/2405-20451_1/2405-20451_1.md', '../data/ocr/ocr_2024-06-03/2405-20567_1/2405-20567_1.md', '../data/ocr/ocr_2024-06-03/2405-20974_1/2405-20974_1.md', '../data/ocr/ocr_2024-06-03/2405-20623_1/2405-20623_1.md', '../data/ocr/ocr_2024-06-03/2405-20587_1/2405-20587_1.md', '../data/ocr/ocr_2024-06-03/2405-20768_1/2405-20768_1.md', '../data/ocr/ocr_2024-06-03/2405-20431_1/2405-20431_1.md', '../data/ocr/ocr_2024-06-03/2405-21056_1/2405-21056_1.md', '../data/ocr/ocr_2024-06-03/2405-20477_1/2405-20477_1.md', '../data/ocr/ocr_2024-06-03/2405-20790_1/2405-20790_1.md', '../data/ocr/ocr_2024-06-03/2405-21010_1/2405-21010_1.md', '../data/ocr/ocr_2024-06-03/2405-20987_1/2405-20987_1.md', '../data/ocr/ocr_2024-06-03/2405-20876_1/2405-20876_1.md', '../data/ocr/ocr_2024-06-03/2405-20951_1/2405-20951_1.md', '../data/ocr/ocr_2024-06-03/2405-20976_1/2405-20976_1.md', '../data/ocr/ocr_2024-06-03/2405-20708_1/2405-20708_1.md', '../data/ocr/ocr_2024-06-03/2405-20580_1/2405-20580_1.md', '../data/ocr/ocr_2024-06-03/2405-20991_1/2405-20991_1.md', '../data/ocr/ocr_2024-06-03/2405-20461_1/2405-20461_1.md', '../data/ocr/ocr_2024-06-03/2405-20724_1/2405-20724_1.md', '../data/ocr/ocr_2024-06-03/2405-20588_1/2405-20588_1.md', '../data/ocr/ocr_2024-06-03/2405-20880_1/2405-20880_1.md', '../data/ocr/ocr_2024-06-03/2405-20579_1/2405-20579_1.md', '../data/ocr/ocr_2024-06-03/2405-20878_1/2405-20878_1.md', '../data/ocr/ocr_2024-06-03/2405-20541_1/2405-20541_1.md', '../data/ocr/ocr_2024-06-03/2405-20700_1/2405-20700_1.md', '../data/ocr/ocr_2024-06-03/2405-20677_1/2405-20677_1.md', '../data/ocr/ocr_2024-06-03/2405-20773_1/2405-20773_1.md', '../data/ocr/ocr_2024-06-03/2405-20895_1/2405-20895_1.md', '../data/ocr/ocr_2024-06-03/2405-20650_1/2405-20650_1.md', '../data/ocr/ocr_2024-06-03/2405-20549_1/2405-20549_1.md', '../data/ocr/ocr_2024-06-03/2405-20973_1/2405-20973_1.md', '../data/ocr/ocr_2024-06-03/2405-20984_1/2405-20984_1.md', '../data/ocr/ocr_2024-06-03/2405-20718_1/2405-20718_1.md', '../data/ocr/ocr_2024-06-03/2405-21061_1/2405-21061_1.md', '../data/ocr/ocr_2024-06-03/2405-20602_1/2405-20602_1.md', '../data/ocr/ocr_2024-06-03/2405-20535_1/2405-20535_1.md', '../data/ocr/ocr_2024-06-03/2405-20583_1/2405-20583_1.md', '../data/ocr/ocr_2024-06-03/2405-20862_1/2405-20862_1.md', '../data/ocr/ocr_2024-06-03/2405-20935_1/2405-20935_1.md', '../data/ocr/ocr_2024-06-03/2405-20599_1/2405-20599_1.md', '../data/ocr/ocr_2024-06-03/2405-20743_1/2405-20743_1.md', '../data/ocr/ocr_2024-06-03/2405-20808_1/2405-20808_1.md', '../data/ocr/ocr_2024-06-03/2405-20693_1/2405-20693_1.md', '../data/ocr/ocr_2024-06-03/2405-20626_1/2405-20626_1.md', '../data/ocr/ocr_2024-06-03/2405-20631_1/2405-20631_1.md', '../data/ocr/ocr_2024-06-03/2405-20835_1/2405-20835_1.md', '../data/ocr/ocr_2024-06-03/2405-20443_1/2405-20443_1.md', '../data/ocr/ocr_2024-06-03/2405-20721_1/2405-20721_1.md', '../data/ocr/ocr_2024-06-03/2405-21047_1/2405-21047_1.md', '../data/ocr/ocr_2024-06-03/2405-20610_1/2405-20610_1.md', '../data/ocr/ocr_2024-06-03/2405-21063_1/2405-21063_1.md', '../data/ocr/ocr_2024-06-03/2405-20956_1/2405-20956_1.md', '../data/ocr/ocr_2024-06-03/2405-21059_1/2405-21059_1.md', '../data/ocr/ocr_2024-06-03/2405-20719_1/2405-20719_1.md', '../data/ocr/ocr_2024-06-03/2405-20510_1/2405-20510_1.md', '../data/ocr/ocr_2024-06-03/2405-20594_1/2405-20594_1.md', '../data/ocr/ocr_2024-06-03/2405-20769_1/2405-20769_1.md', '../data/ocr/ocr_2024-06-03/2405-20421_1/2405-20421_1.md', '../data/ocr/ocr_2024-06-03/2405-20402_1/2405-20402_1.md', '../data/ocr/ocr_2024-06-03/2405-20606_1/2405-20606_1.md', '../data/ocr/ocr_2024-06-03/2405-20434_1/2405-20434_1.md', '../data/ocr/ocr_2024-06-03/2405-20657_1/2405-20657_1.md', '../data/ocr/ocr_2024-06-03/2405-20761_1/2405-20761_1.md', '../data/ocr/ocr_2024-06-03/2405-20666_1/2405-20666_1.md', '../data/ocr/ocr_2024-06-03/2405-20713_1/2405-20713_1.md', '../data/ocr/ocr_2024-06-03/2405-20410_1/2405-20410_1.md', '../data/ocr/ocr_2024-06-03/2405-20596_1/2405-20596_1.md', '../data/ocr/ocr_2024-06-03/2405-20829_1/2405-20829_1.md', '../data/ocr/ocr_2024-06-03/2405-20551_1/2405-20551_1.md', '../data/ocr/ocr_2024-06-03/2405-20556_1/2405-20556_1.md', '../data/ocr/ocr_2024-06-03/2405-20825_1/2405-20825_1.md', '../data/ocr/ocr_2024-06-03/2405-20674_1/2405-20674_1.md', '../data/ocr/ocr_2024-06-03/2405-20824_1/2405-20824_1.md', '../data/ocr/ocr_2024-06-03/2405-20613_1/2405-20613_1.md', '../data/ocr/ocr_2024-06-03/2405-20416_1/2405-20416_1.md', '../data/ocr/ocr_2024-06-03/2405-20731_1/2405-20731_1.md', '../data/ocr/ocr_2024-06-03/2405-20853_1/2405-20853_1.md', '../data/ocr/ocr_2024-06-03/2405-20722_1/2405-20722_1.md', '../data/ocr/ocr_2024-06-03/2405-20787_1/2405-20787_1.md', '../data/ocr/ocr_2024-06-03/2405-20550_1/2405-20550_1.md', '../data/ocr/ocr_2024-06-03/2405-20994_1/2405-20994_1.md', '../data/ocr/ocr_2024-06-03/2405-20794_1/2405-20794_1.md', '../data/ocr/ocr_2024-06-03/2405-20609_1/2405-20609_1.md', '../data/ocr/ocr_2024-06-03/2405-20720_1/2405-20720_1.md', '../data/ocr/ocr_2024-06-03/2405-20971_1/2405-20971_1.md', '../data/ocr/ocr_2024-06-03/2405-21042_1/2405-21042_1.md', '../data/ocr/ocr_2024-06-03/2405-20591_1/2405-20591_1.md', '../data/ocr/ocr_2024-06-03/2405-20589_1/2405-20589_1.md', '../data/ocr/ocr_2024-06-03/2405-20543_1/2405-20543_1.md', '../data/ocr/ocr_2024-06-03/2402-04264_1/2402-04264_1.md', '../data/ocr/ocr_2024-06-03/2405-20836_1/2405-20836_1.md', '../data/ocr/ocr_2024-06-03/2405-20483_1/2405-20483_1.md', '../data/ocr/ocr_2024-06-03/2405-20450_1/2405-20450_1.md', '../data/ocr/ocr_2024-06-03/2405-20762_1/2405-20762_1.md', '../data/ocr/ocr_2024-06-03/2405-20509_1/2405-20509_1.md', '../data/ocr/ocr_2024-06-03/2405-20931_1/2405-20931_1.md', '../data/ocr/ocr_2024-06-03/2405-20764_1/2405-20764_1.md', '../data/ocr/ocr_2024-06-03/2405-21050_1/2405-21050_1.md', '../data/ocr/ocr_2024-06-03/2405-20347_1/2405-20347_1.md', '../data/ocr/ocr_2024-06-03/2405-20850_1/2405-20850_1.md']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chivier/opt/miniconda3/envs/darxiv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'csv_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_vecindex\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/ocr/ocr_2024-06-03\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/index/index_2024-06-03\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 47\u001b[0m, in \u001b[0;36mcreate_vecindex\u001b[0;34m(source_path, index_path, chunk_size, overlap)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# store chunks into \u001b[39;00m\n\u001b[1;32m     45\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache() \n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcsv_result\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(index_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m csvfile:\n\u001b[1;32m     50\u001b[0m     fieldnames \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'csv_result' is not defined"
     ]
    }
   ],
   "source": [
    "result = create_vecindex(\"../data/ocr/ocr_2024-06-03\", \"../data/index/index_2024-06-03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a string with a newline character: \\n\n"
     ]
    }
   ],
   "source": [
    "string_with_newline = \"Here is a string with a newline character: \\n\"\n",
    "escaped_string = string_with_newline.encode('unicode_escape').decode('utf-8')\n",
    "print(escaped_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a string with a newline character: \\n\n"
     ]
    }
   ],
   "source": [
    "print(r\"Here is a string with a newline character: \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "darxiv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
