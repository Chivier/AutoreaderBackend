{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/ocr/ocr_2024-06-03/2405-20467_1/2405-20467_1.md', '../data/ocr/ocr_2024-06-03/2405-20662_1/2405-20662_1.md', '../data/ocr/ocr_2024-06-03/2405-20715_1/2405-20715_1.md', '../data/ocr/ocr_2024-06-03/2405-20750_1/2405-20750_1.md', '../data/ocr/ocr_2024-06-03/2405-20630_1/2405-20630_1.md', '../data/ocr/ocr_2024-06-03/2405-20525_1/2405-20525_1.md', '../data/ocr/ocr_2024-06-03/2405-21009_1/2405-21009_1.md', '../data/ocr/ocr_2024-06-03/2405-21051_1/2405-21051_1.md', '../data/ocr/ocr_2024-06-03/2405-20559_1/2405-20559_1.md', '../data/ocr/ocr_2024-06-03/2405-20524_1/2405-20524_1.md', '../data/ocr/ocr_2024-06-03/2405-21040_1/2405-21040_1.md', '../data/ocr/ocr_2024-06-03/2405-20423_1/2405-20423_1.md', '../data/ocr/ocr_2024-06-03/2405-20456_1/2405-20456_1.md', '../data/ocr/ocr_2024-06-03/2405-21066_1/2405-21066_1.md', '../data/ocr/ocr_2024-06-03/2405-20502_1/2405-20502_1.md', '../data/ocr/ocr_2024-06-03/2405-20603_1/2405-20603_1.md', '../data/ocr/ocr_2024-06-03/2405-20980_1/2405-20980_1.md', '../data/ocr/ocr_2024-06-03/2405-20967_1/2405-20967_1.md', '../data/ocr/ocr_2024-06-03/2405-20482_1/2405-20482_1.md', '../data/ocr/ocr_2024-06-03/2405-21027_1/2405-21027_1.md', '../data/ocr/ocr_2024-06-03/2405-20725_1/2405-20725_1.md', '../data/ocr/ocr_2024-06-03/2405-20972_1/2405-20972_1.md', '../data/ocr/ocr_2024-06-03/2405-21022_1/2405-21022_1.md', '../data/ocr/ocr_2024-06-03/2405-20983_1/2405-20983_1.md', '../data/ocr/ocr_2024-06-03/2405-20527_1/2405-20527_1.md', '../data/ocr/ocr_2024-06-03/2405-20574_1/2405-20574_1.md', '../data/ocr/ocr_2024-06-03/2405-20729_1/2405-20729_1.md', '../data/ocr/ocr_2024-06-03/2405-20354_1/2405-20354_1.md', '../data/ocr/ocr_2024-06-03/2405-20592_1/2405-20592_1.md', '../data/ocr/ocr_2024-06-03/2405-20877_1/2405-20877_1.md', '../data/ocr/ocr_2024-06-03/2405-20902_1/2405-20902_1.md', '../data/ocr/ocr_2024-06-03/2405-20678_1/2405-20678_1.md', '../data/ocr/ocr_2024-06-03/2405-20969_1/2405-20969_1.md', '../data/ocr/ocr_2024-06-03/2405-21021_1/2405-21021_1.md', '../data/ocr/ocr_2024-06-03/2405-20968_1/2405-20968_1.md', '../data/ocr/ocr_2024-06-03/2405-20455_1/2405-20455_1.md', '../data/ocr/ocr_2024-06-03/2405-20849_1/2405-20849_1.md', '../data/ocr/ocr_2024-06-03/2405-20806_1/2405-20806_1.md', '../data/ocr/ocr_2024-06-03/2405-20993_1/2405-20993_1.md', '../data/ocr/ocr_2024-06-03/2405-20776_1/2405-20776_1.md', '../data/ocr/ocr_2024-06-03/2405-20560_1/2405-20560_1.md', '../data/ocr/ocr_2024-06-03/2405-21018_1/2405-21018_1.md', '../data/ocr/ocr_2024-06-03/2405-20999_1/2405-20999_1.md', '../data/ocr/ocr_2024-06-03/2405-21025_1/2405-21025_1.md', '../data/ocr/ocr_2024-06-03/2405-20654_1/2405-20654_1.md', '../data/ocr/ocr_2024-06-03/2405-20481_1/2405-20481_1.md', '../data/ocr/ocr_2024-06-03/2405-20668_1/2405-20668_1.md', '../data/ocr/ocr_2024-06-03/2405-20685_1/2405-20685_1.md', '../data/ocr/ocr_2024-06-03/2405-20420_1/2405-20420_1.md', '../data/ocr/ocr_2024-06-03/2405-20905_1/2405-20905_1.md', '../data/ocr/ocr_2024-06-03/2405-20804_1/2405-20804_1.md', '../data/ocr/ocr_2024-06-03/2405-20748_1/2405-20748_1.md', '../data/ocr/ocr_2024-06-03/2405-21036_1/2405-21036_1.md', '../data/ocr/ocr_2024-06-03/2405-20664_1/2405-20664_1.md', '../data/ocr/ocr_2024-06-03/2405-20389_1/2405-20389_1.md', '../data/ocr/ocr_2024-06-03/2405-20775_1/2405-20775_1.md', '../data/ocr/ocr_2024-06-03/2405-20531_1/2405-20531_1.md', '../data/ocr/ocr_2024-06-03/2405-20778_1/2405-20778_1.md', '../data/ocr/ocr_2024-06-03/2405-20800_1/2405-20800_1.md', '../data/ocr/ocr_2024-06-03/2405-20701_1/2405-20701_1.md', '../data/ocr/ocr_2024-06-03/2405-20852_1/2405-20852_1.md', '../data/ocr/ocr_2024-06-03/2405-20387_1/2405-20387_1.md', '../data/ocr/ocr_2024-06-03/2405-20799_1/2405-20799_1.md', '../data/ocr/ocr_2024-06-03/2405-20863_1/2405-20863_1.md', '../data/ocr/ocr_2024-06-03/2405-20521_1/2405-20521_1.md', '../data/ocr/ocr_2024-06-03/2405-20620_1/2405-20620_1.md', '../data/ocr/ocr_2024-06-03/2405-20530_1/2405-20530_1.md', '../data/ocr/ocr_2024-06-03/2405-21003_1/2405-21003_1.md', '../data/ocr/ocr_2024-06-03/2405-20848_1/2405-20848_1.md', '../data/ocr/ocr_2024-06-03/2405-20786_1/2405-20786_1.md', '../data/ocr/ocr_2024-06-03/2405-20404_1/2405-20404_1.md', '../data/ocr/ocr_2024-06-03/2405-21015_1/2405-21015_1.md', '../data/ocr/ocr_2024-06-03/2405-20384_1/2405-20384_1.md', '../data/ocr/ocr_2024-06-03/2405-20562_1/2405-20562_1.md', '../data/ocr/ocr_2024-06-03/2405-20819_1/2405-20819_1.md', '../data/ocr/ocr_2024-06-03/2405-21034_1/2405-21034_1.md', '../data/ocr/ocr_2024-06-03/2405-20684_1/2405-20684_1.md', '../data/ocr/ocr_2024-06-03/2405-20859_1/2405-20859_1.md', '../data/ocr/ocr_2024-06-03/2405-20414_1/2405-20414_1.md', '../data/ocr/ocr_2024-06-03/2405-20447_1/2405-20447_1.md', '../data/ocr/ocr_2024-06-03/2405-21004_1/2405-21004_1.md', '../data/ocr/ocr_2024-06-03/2405-20805_1/2405-20805_1.md', '../data/ocr/ocr_2024-06-03/2405-20738_1/2405-20738_1.md', '../data/ocr/ocr_2024-06-03/2405-20915_1/2405-20915_1.md', '../data/ocr/ocr_2024-06-03/2405-20593_1/2405-20593_1.md', '../data/ocr/ocr_2024-06-03/2405-20830_1/2405-20830_1.md', '../data/ocr/ocr_2024-06-03/2405-20485_1/2405-20485_1.md', '../data/ocr/ocr_2024-06-03/2405-20682_1/2405-20682_1.md', '../data/ocr/ocr_2024-06-03/2405-20833_1/2405-20833_1.md', '../data/ocr/ocr_2024-06-03/2405-20413_1/2405-20413_1.md', '../data/ocr/ocr_2024-06-03/2405-20648_1/2405-20648_1.md', '../data/ocr/ocr_2024-06-03/2405-20633_1/2405-20633_1.md', '../data/ocr/ocr_2024-06-03/2405-20363_1/2405-20363_1.md', '../data/ocr/ocr_2024-06-03/2405-20988_1/2405-20988_1.md', '../data/ocr/ocr_2024-06-03/2405-20904_1/2405-20904_1.md', '../data/ocr/ocr_2024-06-03/2405-20981_1/2405-20981_1.md', '../data/ocr/ocr_2024-06-03/2405-20516_1/2405-20516_1.md', '../data/ocr/ocr_2024-06-03/2405-20882_1/2405-20882_1.md', '../data/ocr/ocr_2024-06-03/2405-20670_1/2405-20670_1.md', '../data/ocr/ocr_2024-06-03/2405-20733_1/2405-20733_1.md', '../data/ocr/ocr_2024-06-03/2405-20534_1/2405-20534_1.md', '../data/ocr/ocr_2024-06-03/2405-20368_1/2405-20368_1.md', '../data/ocr/ocr_2024-06-03/2405-20918_1/2405-20918_1.md', '../data/ocr/ocr_2024-06-03/2405-20380_1/2405-20380_1.md', '../data/ocr/ocr_2024-06-03/2405-20350_1/2405-20350_1.md', '../data/ocr/ocr_2024-06-03/2405-20446_1/2405-20446_1.md', '../data/ocr/ocr_2024-06-03/2405-20628_1/2405-20628_1.md', '../data/ocr/ocr_2024-06-03/2405-20505_1/2405-20505_1.md', '../data/ocr/ocr_2024-06-03/2405-21013_1/2405-21013_1.md', '../data/ocr/ocr_2024-06-03/2405-20687_1/2405-20687_1.md', '../data/ocr/ocr_2024-06-03/2405-20641_1/2405-20641_1.md', '../data/ocr/ocr_2024-06-03/2405-21005_1/2405-21005_1.md', '../data/ocr/ocr_2024-06-03/2405-20763_1/2405-20763_1.md', '../data/ocr/ocr_2024-06-03/2405-20727_1/2405-20727_1.md', '../data/ocr/ocr_2024-06-03/2405-21016_1/2405-21016_1.md', '../data/ocr/ocr_2024-06-03/2405-20471_1/2405-20471_1.md', '../data/ocr/ocr_2024-06-03/2405-20759_1/2405-20759_1.md', '../data/ocr/ocr_2024-06-03/2405-20868_1/2405-20868_1.md', '../data/ocr/ocr_2024-06-03/2405-20600_1/2405-20600_1.md', '../data/ocr/ocr_2024-06-03/2405-20860_1/2405-20860_1.md', '../data/ocr/ocr_2024-06-03/2405-20914_1/2405-20914_1.md', '../data/ocr/ocr_2024-06-03/2405-20820_1/2405-20820_1.md', '../data/ocr/ocr_2024-06-03/2405-20487_1/2405-20487_1.md', '../data/ocr/ocr_2024-06-03/2405-20815_1/2405-20815_1.md', '../data/ocr/ocr_2024-06-03/2405-20705_1/2405-20705_1.md', '../data/ocr/ocr_2024-06-03/2405-20573_1/2405-20573_1.md', '../data/ocr/ocr_2024-06-03/2405-20452_1/2405-20452_1.md', '../data/ocr/ocr_2024-06-03/2405-20653_1/2405-20653_1.md', '../data/ocr/ocr_2024-06-03/2405-20488_1/2405-20488_1.md', '../data/ocr/ocr_2024-06-03/2405-20618_1/2405-20618_1.md', '../data/ocr/ocr_2024-06-03/2405-20496_1/2405-20496_1.md', '../data/ocr/ocr_2024-06-03/2405-20561_1/2405-20561_1.md', '../data/ocr/ocr_2024-06-03/2405-20445_1/2405-20445_1.md', '../data/ocr/ocr_2024-06-03/2405-20355_1/2405-20355_1.md', '../data/ocr/ocr_2024-06-03/2405-20614_1/2405-20614_1.md', '../data/ocr/ocr_2024-06-03/2405-20513_1/2405-20513_1.md', '../data/ocr/ocr_2024-06-03/2405-20753_1/2405-20753_1.md', '../data/ocr/ocr_2024-06-03/2405-20584_1/2405-20584_1.md', '../data/ocr/ocr_2024-06-03/2405-20661_1/2405-20661_1.md', '../data/ocr/ocr_2024-06-03/2405-20785_1/2405-20785_1.md', '../data/ocr/ocr_2024-06-03/2405-20486_1/2405-20486_1.md', '../data/ocr/ocr_2024-06-03/2405-20771_1/2405-20771_1.md', '../data/ocr/ocr_2024-06-03/2405-20917_1/2405-20917_1.md', '../data/ocr/ocr_2024-06-03/2405-20642_1/2405-20642_1.md', '../data/ocr/ocr_2024-06-03/2405-20351_1/2405-20351_1.md', '../data/ocr/ocr_2024-06-03/2405-21045_1/2405-21045_1.md', '../data/ocr/ocr_2024-06-03/2405-20433_1/2405-20433_1.md', '../data/ocr/ocr_2024-06-03/2405-20777_1/2405-20777_1.md', '../data/ocr/ocr_2024-06-03/2405-20424_1/2405-20424_1.md', '../data/ocr/ocr_2024-06-03/2405-20671_1/2405-20671_1.md', '../data/ocr/ocr_2024-06-03/2405-20858_1/2405-20858_1.md', '../data/ocr/ocr_2024-06-03/2405-20881_1/2405-20881_1.md', '../data/ocr/ocr_2024-06-03/2405-20457_1/2405-20457_1.md', '../data/ocr/ocr_2024-06-03/2405-21046_1/2405-21046_1.md', '../data/ocr/ocr_2024-06-03/2405-20649_1/2405-20649_1.md', '../data/ocr/ocr_2024-06-03/2405-20916_1/2405-20916_1.md', '../data/ocr/ocr_2024-06-03/2405-21012_1/2405-21012_1.md', '../data/ocr/ocr_2024-06-03/2405-20503_1/2405-20503_1.md', '../data/ocr/ocr_2024-06-03/2405-20494_1/2405-20494_1.md', '../data/ocr/ocr_2024-06-03/2405-20970_1/2405-20970_1.md', '../data/ocr/ocr_2024-06-03/2405-20362_1/2405-20362_1.md', '../data/ocr/ocr_2024-06-03/2405-20681_1/2405-20681_1.md', '../data/ocr/ocr_2024-06-03/2405-20646_1/2405-20646_1.md', '../data/ocr/ocr_2024-06-03/2405-21048_1/2405-21048_1.md', '../data/ocr/ocr_2024-06-03/2405-20459_1/2405-20459_1.md', '../data/ocr/ocr_2024-06-03/2405-20458_1/2405-20458_1.md', '../data/ocr/ocr_2024-06-03/2405-20947_1/2405-20947_1.md', '../data/ocr/ocr_2024-06-03/2405-20449_1/2405-20449_1.md', '../data/ocr/ocr_2024-06-03/2405-20539_1/2405-20539_1.md', '../data/ocr/ocr_2024-06-03/2405-20470_1/2405-20470_1.md', '../data/ocr/ocr_2024-06-03/2405-20842_1/2405-20842_1.md', '../data/ocr/ocr_2024-06-03/2405-20512_1/2405-20512_1.md', '../data/ocr/ocr_2024-06-03/2405-20392_1/2405-20392_1.md', '../data/ocr/ocr_2024-06-03/2405-20892_1/2405-20892_1.md', '../data/ocr/ocr_2024-06-03/2405-20526_1/2405-20526_1.md', '../data/ocr/ocr_2024-06-03/2405-20887_1/2405-20887_1.md', '../data/ocr/ocr_2024-06-03/2405-20656_1/2405-20656_1.md', '../data/ocr/ocr_2024-06-03/2405-20582_1/2405-20582_1.md', '../data/ocr/ocr_2024-06-03/2405-21030_1/2405-21030_1.md', '../data/ocr/ocr_2024-06-03/2405-20838_1/2405-20838_1.md', '../data/ocr/ocr_2024-06-03/2405-20680_1/2405-20680_1.md', '../data/ocr/ocr_2024-06-03/2405-20565_1/2405-20565_1.md', '../data/ocr/ocr_2024-06-03/2405-20576_1/2405-20576_1.md', '../data/ocr/ocr_2024-06-03/2405-20489_1/2405-20489_1.md', '../data/ocr/ocr_2024-06-03/2405-20643_1/2405-20643_1.md', '../data/ocr/ocr_2024-06-03/2405-20867_1/2405-20867_1.md', '../data/ocr/ocr_2024-06-03/2405-20358_1/2405-20358_1.md', '../data/ocr/ocr_2024-06-03/2405-20590_1/2405-20590_1.md', '../data/ocr/ocr_2024-06-03/2405-20501_1/2405-20501_1.md', '../data/ocr/ocr_2024-06-03/2405-20782_1/2405-20782_1.md', '../data/ocr/ocr_2024-06-03/2405-20585_1/2405-20585_1.md', '../data/ocr/ocr_2024-06-03/2405-20697_1/2405-20697_1.md', '../data/ocr/ocr_2024-06-03/2405-20869_1/2405-20869_1.md', '../data/ocr/ocr_2024-06-03/2405-20834_1/2405-20834_1.md', '../data/ocr/ocr_2024-06-03/2405-20640_1/2405-20640_1.md', '../data/ocr/ocr_2024-06-03/2405-20797_1/2405-20797_1.md', '../data/ocr/ocr_2024-06-03/2405-20624_1/2405-20624_1.md', '../data/ocr/ocr_2024-06-03/2405-20933_1/2405-20933_1.md', '../data/ocr/ocr_2024-06-03/2405-21074_1/2405-21074_1.md', '../data/ocr/ocr_2024-06-03/2405-20779_1/2405-20779_1.md', '../data/ocr/ocr_2024-06-03/2405-20529_1/2405-20529_1.md', '../data/ocr/ocr_2024-06-03/2405-20364_1/2405-20364_1.md', '../data/ocr/ocr_2024-06-03/2405-20504_1/2405-20504_1.md', '../data/ocr/ocr_2024-06-03/2405-20818_1/2405-20818_1.md', '../data/ocr/ocr_2024-06-03/2405-20978_1/2405-20978_1.md', '../data/ocr/ocr_2024-06-03/2405-21028_1/2405-21028_1.md', '../data/ocr/ocr_2024-06-03/2405-20495_1/2405-20495_1.md', '../data/ocr/ocr_2024-06-03/2405-20607_1/2405-20607_1.md', '../data/ocr/ocr_2024-06-03/2405-20400_1/2405-20400_1.md', '../data/ocr/ocr_2024-06-03/2405-20568_1/2405-20568_1.md', '../data/ocr/ocr_2024-06-03/2405-20519_1/2405-20519_1.md', '../data/ocr/ocr_2024-06-03/2405-20468_1/2405-20468_1.md', '../data/ocr/ocr_2024-06-03/2405-20439_1/2405-20439_1.md', '../data/ocr/ocr_2024-06-03/2405-20985_1/2405-20985_1.md', '../data/ocr/ocr_2024-06-03/2405-20625_1/2405-20625_1.md', '../data/ocr/ocr_2024-06-03/2405-20851_1/2405-20851_1.md', '../data/ocr/ocr_2024-06-03/2405-20542_1/2405-20542_1.md', '../data/ocr/ocr_2024-06-03/2405-20508_1/2405-20508_1.md', '../data/ocr/ocr_2024-06-03/2405-20959_1/2405-20959_1.md', '../data/ocr/ocr_2024-06-03/2405-21064_1/2405-21064_1.md', '../data/ocr/ocr_2024-06-03/2405-21075_1/2405-21075_1.md', '../data/ocr/ocr_2024-06-03/2405-20962_1/2405-20962_1.md', '../data/ocr/ocr_2024-06-03/2405-20791_1/2405-20791_1.md', '../data/ocr/ocr_2024-06-03/2405-20710_1/2405-20710_1.md', '../data/ocr/ocr_2024-06-03/2405-20675_1/2405-20675_1.md', '../data/ocr/ocr_2024-06-03/2405-20652_1/2405-20652_1.md', '../data/ocr/ocr_2024-06-03/2405-20975_1/2405-20975_1.md', '../data/ocr/ocr_2024-06-03/2405-20810_1/2405-20810_1.md', '../data/ocr/ocr_2024-06-03/2405-20397_1/2405-20397_1.md', '../data/ocr/ocr_2024-06-03/2405-20465_1/2405-20465_1.md', '../data/ocr/ocr_2024-06-03/2405-20772_1/2405-20772_1.md', '../data/ocr/ocr_2024-06-03/2405-20884_1/2405-20884_1.md', '../data/ocr/ocr_2024-06-03/2405-20896_1/2405-20896_1.md', '../data/ocr/ocr_2024-06-03/2405-20847_1/2405-20847_1.md', '../data/ocr/ocr_2024-06-03/2405-21023_1/2405-21023_1.md', '../data/ocr/ocr_2024-06-03/2405-20795_1/2405-20795_1.md', '../data/ocr/ocr_2024-06-03/2405-20954_1/2405-20954_1.md', '../data/ocr/ocr_2024-06-03/2405-20883_1/2405-20883_1.md', '../data/ocr/ocr_2024-06-03/2405-20611_1/2405-20611_1.md', '../data/ocr/ocr_2024-06-03/2405-20540_1/2405-20540_1.md', '../data/ocr/ocr_2024-06-03/2405-20910_1/2405-20910_1.md', '../data/ocr/ocr_2024-06-03/2405-20745_1/2405-20745_1.md', '../data/ocr/ocr_2024-06-03/2405-20419_1/2405-20419_1.md', '../data/ocr/ocr_2024-06-03/2405-20441_1/2405-20441_1.md', '../data/ocr/ocr_2024-06-03/2405-20900_1/2405-20900_1.md', '../data/ocr/ocr_2024-06-03/2405-21068_1/2405-21068_1.md', '../data/ocr/ocr_2024-06-03/2405-20717_1/2405-20717_1.md', '../data/ocr/ocr_2024-06-03/2405-20669_1/2405-20669_1.md', '../data/ocr/ocr_2024-06-03/2405-20469_1/2405-20469_1.md', '../data/ocr/ocr_2024-06-03/2405-20692_1/2405-20692_1.md', '../data/ocr/ocr_2024-06-03/2405-21043_1/2405-21043_1.md', '../data/ocr/ocr_2024-06-03/2405-21044_1/2405-21044_1.md', '../data/ocr/ocr_2024-06-03/2405-20672_1/2405-20672_1.md', '../data/ocr/ocr_2024-06-03/2405-20879_1/2405-20879_1.md', '../data/ocr/ocr_2024-06-03/2405-20861_1/2405-20861_1.md', '../data/ocr/ocr_2024-06-03/2405-20704_1/2405-20704_1.md', '../data/ocr/ocr_2024-06-03/2405-20500_1/2405-20500_1.md', '../data/ocr/ocr_2024-06-03/2405-20622_1/2405-20622_1.md', '../data/ocr/ocr_2024-06-03/2405-20555_1/2405-20555_1.md', '../data/ocr/ocr_2024-06-03/2405-20986_1/2405-20986_1.md', '../data/ocr/ocr_2024-06-03/2405-20407_1/2405-20407_1.md', '../data/ocr/ocr_2024-06-03/2405-20426_1/2405-20426_1.md', '../data/ocr/ocr_2024-06-03/2405-20755_1/2405-20755_1.md', '../data/ocr/ocr_2024-06-03/2405-20435_1/2405-20435_1.md', '../data/ocr/ocr_2024-06-03/2405-20711_1/2405-20711_1.md', '../data/ocr/ocr_2024-06-03/2405-20990_1/2405-20990_1.md', '../data/ocr/ocr_2024-06-03/2405-21070_1/2405-21070_1.md', '../data/ocr/ocr_2024-06-03/2405-20690_1/2405-20690_1.md', '../data/ocr/ocr_2024-06-03/2405-20390_1/2405-20390_1.md', '../data/ocr/ocr_2024-06-03/2405-20605_1/2405-20605_1.md', '../data/ocr/ocr_2024-06-03/2405-20430_1/2405-20430_1.md', '../data/ocr/ocr_2024-06-03/2405-20429_1/2405-20429_1.md', '../data/ocr/ocr_2024-06-03/2405-20694_1/2405-20694_1.md', '../data/ocr/ocr_2024-06-03/2405-20348_1/2405-20348_1.md', '../data/ocr/ocr_2024-06-03/2405-20906_1/2405-20906_1.md', '../data/ocr/ocr_2024-06-03/2405-20735_1/2405-20735_1.md', '../data/ocr/ocr_2024-06-03/2405-20821_1/2405-20821_1.md', '../data/ocr/ocr_2024-06-03/2405-20774_1/2405-20774_1.md', '../data/ocr/ocr_2024-06-03/2405-20846_1/2405-20846_1.md', '../data/ocr/ocr_2024-06-03/2405-20448_1/2405-20448_1.md', '../data/ocr/ocr_2024-06-03/2405-20608_1/2405-20608_1.md', '../data/ocr/ocr_2024-06-03/2405-20982_1/2405-20982_1.md', '../data/ocr/ocr_2024-06-03/2405-20462_1/2405-20462_1.md', '../data/ocr/ocr_2024-06-03/2405-20412_1/2405-20412_1.md', '../data/ocr/ocr_2024-06-03/2405-21060_1/2405-21060_1.md', '../data/ocr/ocr_2024-06-03/2405-20538_1/2405-20538_1.md', '../data/ocr/ocr_2024-06-03/2405-21055_1/2405-21055_1.md', '../data/ocr/ocr_2024-06-03/2405-20770_1/2405-20770_1.md', '../data/ocr/ocr_2024-06-03/2405-20703_1/2405-20703_1.md', '../data/ocr/ocr_2024-06-03/2405-20612_1/2405-20612_1.md', '../data/ocr/ocr_2024-06-03/2405-20451_1/2405-20451_1.md', '../data/ocr/ocr_2024-06-03/2405-20567_1/2405-20567_1.md', '../data/ocr/ocr_2024-06-03/2405-20974_1/2405-20974_1.md', '../data/ocr/ocr_2024-06-03/2405-20623_1/2405-20623_1.md', '../data/ocr/ocr_2024-06-03/2405-20587_1/2405-20587_1.md', '../data/ocr/ocr_2024-06-03/2405-20768_1/2405-20768_1.md', '../data/ocr/ocr_2024-06-03/2405-20431_1/2405-20431_1.md', '../data/ocr/ocr_2024-06-03/2405-21056_1/2405-21056_1.md', '../data/ocr/ocr_2024-06-03/2405-20477_1/2405-20477_1.md', '../data/ocr/ocr_2024-06-03/2405-20790_1/2405-20790_1.md', '../data/ocr/ocr_2024-06-03/2405-21010_1/2405-21010_1.md', '../data/ocr/ocr_2024-06-03/2405-20987_1/2405-20987_1.md', '../data/ocr/ocr_2024-06-03/2405-20876_1/2405-20876_1.md', '../data/ocr/ocr_2024-06-03/2405-20951_1/2405-20951_1.md', '../data/ocr/ocr_2024-06-03/2405-20976_1/2405-20976_1.md', '../data/ocr/ocr_2024-06-03/2405-20708_1/2405-20708_1.md', '../data/ocr/ocr_2024-06-03/2405-20580_1/2405-20580_1.md', '../data/ocr/ocr_2024-06-03/2405-20991_1/2405-20991_1.md', '../data/ocr/ocr_2024-06-03/2405-20461_1/2405-20461_1.md', '../data/ocr/ocr_2024-06-03/2405-20724_1/2405-20724_1.md', '../data/ocr/ocr_2024-06-03/2405-20588_1/2405-20588_1.md', '../data/ocr/ocr_2024-06-03/2405-20880_1/2405-20880_1.md', '../data/ocr/ocr_2024-06-03/2405-20579_1/2405-20579_1.md', '../data/ocr/ocr_2024-06-03/2405-20878_1/2405-20878_1.md', '../data/ocr/ocr_2024-06-03/2405-20541_1/2405-20541_1.md', '../data/ocr/ocr_2024-06-03/2405-20700_1/2405-20700_1.md', '../data/ocr/ocr_2024-06-03/2405-20677_1/2405-20677_1.md', '../data/ocr/ocr_2024-06-03/2405-20773_1/2405-20773_1.md', '../data/ocr/ocr_2024-06-03/2405-20895_1/2405-20895_1.md', '../data/ocr/ocr_2024-06-03/2405-20650_1/2405-20650_1.md', '../data/ocr/ocr_2024-06-03/2405-20549_1/2405-20549_1.md', '../data/ocr/ocr_2024-06-03/2405-20973_1/2405-20973_1.md', '../data/ocr/ocr_2024-06-03/2405-20984_1/2405-20984_1.md', '../data/ocr/ocr_2024-06-03/2405-20718_1/2405-20718_1.md', '../data/ocr/ocr_2024-06-03/2405-21061_1/2405-21061_1.md', '../data/ocr/ocr_2024-06-03/2405-20602_1/2405-20602_1.md', '../data/ocr/ocr_2024-06-03/2405-20535_1/2405-20535_1.md', '../data/ocr/ocr_2024-06-03/2405-20583_1/2405-20583_1.md', '../data/ocr/ocr_2024-06-03/2405-20862_1/2405-20862_1.md', '../data/ocr/ocr_2024-06-03/2405-20935_1/2405-20935_1.md', '../data/ocr/ocr_2024-06-03/2405-20599_1/2405-20599_1.md', '../data/ocr/ocr_2024-06-03/2405-20743_1/2405-20743_1.md', '../data/ocr/ocr_2024-06-03/2405-20808_1/2405-20808_1.md', '../data/ocr/ocr_2024-06-03/2405-20693_1/2405-20693_1.md', '../data/ocr/ocr_2024-06-03/2405-20626_1/2405-20626_1.md', '../data/ocr/ocr_2024-06-03/2405-20631_1/2405-20631_1.md', '../data/ocr/ocr_2024-06-03/2405-20835_1/2405-20835_1.md', '../data/ocr/ocr_2024-06-03/2405-20443_1/2405-20443_1.md', '../data/ocr/ocr_2024-06-03/2405-20721_1/2405-20721_1.md', '../data/ocr/ocr_2024-06-03/2405-21047_1/2405-21047_1.md', '../data/ocr/ocr_2024-06-03/2405-20610_1/2405-20610_1.md', '../data/ocr/ocr_2024-06-03/2405-21063_1/2405-21063_1.md', '../data/ocr/ocr_2024-06-03/2405-20956_1/2405-20956_1.md', '../data/ocr/ocr_2024-06-03/2405-21059_1/2405-21059_1.md', '../data/ocr/ocr_2024-06-03/2405-20719_1/2405-20719_1.md', '../data/ocr/ocr_2024-06-03/2405-20510_1/2405-20510_1.md', '../data/ocr/ocr_2024-06-03/2405-20594_1/2405-20594_1.md', '../data/ocr/ocr_2024-06-03/2405-20769_1/2405-20769_1.md', '../data/ocr/ocr_2024-06-03/2405-20421_1/2405-20421_1.md', '../data/ocr/ocr_2024-06-03/2405-20402_1/2405-20402_1.md', '../data/ocr/ocr_2024-06-03/2405-20606_1/2405-20606_1.md', '../data/ocr/ocr_2024-06-03/2405-20434_1/2405-20434_1.md', '../data/ocr/ocr_2024-06-03/2405-20657_1/2405-20657_1.md', '../data/ocr/ocr_2024-06-03/2405-20761_1/2405-20761_1.md', '../data/ocr/ocr_2024-06-03/2405-20666_1/2405-20666_1.md', '../data/ocr/ocr_2024-06-03/2405-20713_1/2405-20713_1.md', '../data/ocr/ocr_2024-06-03/2405-20410_1/2405-20410_1.md', '../data/ocr/ocr_2024-06-03/2405-20596_1/2405-20596_1.md', '../data/ocr/ocr_2024-06-03/2405-20829_1/2405-20829_1.md', '../data/ocr/ocr_2024-06-03/2405-20551_1/2405-20551_1.md', '../data/ocr/ocr_2024-06-03/2405-20556_1/2405-20556_1.md', '../data/ocr/ocr_2024-06-03/2405-20825_1/2405-20825_1.md', '../data/ocr/ocr_2024-06-03/2405-20674_1/2405-20674_1.md', '../data/ocr/ocr_2024-06-03/2405-20824_1/2405-20824_1.md', '../data/ocr/ocr_2024-06-03/2405-20613_1/2405-20613_1.md', '../data/ocr/ocr_2024-06-03/2405-20416_1/2405-20416_1.md', '../data/ocr/ocr_2024-06-03/2405-20731_1/2405-20731_1.md', '../data/ocr/ocr_2024-06-03/2405-20853_1/2405-20853_1.md', '../data/ocr/ocr_2024-06-03/2405-20722_1/2405-20722_1.md', '../data/ocr/ocr_2024-06-03/2405-20787_1/2405-20787_1.md', '../data/ocr/ocr_2024-06-03/2405-20550_1/2405-20550_1.md', '../data/ocr/ocr_2024-06-03/2405-20994_1/2405-20994_1.md', '../data/ocr/ocr_2024-06-03/2405-20794_1/2405-20794_1.md', '../data/ocr/ocr_2024-06-03/2405-20609_1/2405-20609_1.md', '../data/ocr/ocr_2024-06-03/2405-20720_1/2405-20720_1.md', '../data/ocr/ocr_2024-06-03/2405-20971_1/2405-20971_1.md', '../data/ocr/ocr_2024-06-03/2405-21042_1/2405-21042_1.md', '../data/ocr/ocr_2024-06-03/2405-20591_1/2405-20591_1.md', '../data/ocr/ocr_2024-06-03/2405-20589_1/2405-20589_1.md', '../data/ocr/ocr_2024-06-03/2405-20543_1/2405-20543_1.md', '../data/ocr/ocr_2024-06-03/2402-04264_1/2402-04264_1.md', '../data/ocr/ocr_2024-06-03/2405-20836_1/2405-20836_1.md', '../data/ocr/ocr_2024-06-03/2405-20483_1/2405-20483_1.md', '../data/ocr/ocr_2024-06-03/2405-20450_1/2405-20450_1.md', '../data/ocr/ocr_2024-06-03/2405-20762_1/2405-20762_1.md', '../data/ocr/ocr_2024-06-03/2405-20509_1/2405-20509_1.md', '../data/ocr/ocr_2024-06-03/2405-20931_1/2405-20931_1.md', '../data/ocr/ocr_2024-06-03/2405-20764_1/2405-20764_1.md', '../data/ocr/ocr_2024-06-03/2405-21050_1/2405-21050_1.md', '../data/ocr/ocr_2024-06-03/2405-20347_1/2405-20347_1.md', '../data/ocr/ocr_2024-06-03/2405-20850_1/2405-20850_1.md']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chivier/opt/miniconda3/envs/darxiv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Convert Markdown to Embeddings\n",
    "\n",
    "from DailyVec import *\n",
    "\n",
    "ocr_result_path = \"../data/ocr/ocr_2024-06-03\"\n",
    "index_path = \"../data/index/index_2024-06-03-fix\"\n",
    "convert_embeddings(ocr_result_path, index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read vectors\n",
    "\n",
    "vector_path = \"../data/index/index_2024-06-03-fix/\"\n",
    "\n",
    "# Load all vectors\n",
    "\n",
    "# find all npy files in the directory\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def load_vectors(vector_path):\n",
    "    vectors = {}\n",
    "    for file in os.listdir(vector_path):\n",
    "        if file.endswith(\".npy\"):\n",
    "            local_vectors = np.load(vector_path + file)\n",
    "            key_prefix = file.split(\".\")[0].replace(\"_vectors\", \"\")\n",
    "            chunk_id = 0\n",
    "            for vector in local_vectors:\n",
    "                chunk_id += 1\n",
    "                vector_key = key_prefix + \"_\" + str(chunk_id)\n",
    "                vectors[vector_key] = vector\n",
    "    return vectors\n",
    "\n",
    "vectors = load_vectors(vector_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5431\n"
     ]
    }
   ],
   "source": [
    "print(len(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"LLM model serving on distrubuted systems or serverless architecture. Related to VLLM or OLLAMA.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_embeddings(sentences):\n",
    "    model = SentenceTransformer(\"Alibaba-NLP/gte-large-en-v1.5\", trust_remote_code=True)\n",
    "    embeddings = model.encode(sentences)\n",
    "    # release the memory of the model\n",
    "    torch.cuda.empty_cache()\n",
    "    return embeddings\n",
    "\n",
    "question_embeddings = get_embeddings([question])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024,)\n"
     ]
    }
   ],
   "source": [
    "print(question_embeddings[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2_distance(vector1, vector2):\n",
    "    return np.linalg.norm(vector1 - vector2)\n",
    "\n",
    "def cos_sim(vector1, vector2):\n",
    "    return -np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "\n",
    "def get_closest_vector(vectors, target_vector, top_k=3):\n",
    "    # find the closest vector\n",
    "    top_kv = []\n",
    "    for key, vector in vectors.items():\n",
    "        # print(key, L2_distance(vector, target_vector))\n",
    "        top_kv.append((key, cos_sim(vector, target_vector)))\n",
    "        top_kv.sort(key=lambda x: x[1])\n",
    "        if len(top_kv) > top_k:\n",
    "            top_kv.pop()\n",
    "    return top_kv\n",
    "    \n",
    "question_result = get_closest_vector(vectors, question_embeddings[0], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"We are not aware of any other serverless framework that supports a Wasm runtime and allows both the controller and the workers to be deployed on resource-constrained edge nodes.\\n\\nFocusing on serverless platforms supporting Wasm runtimes, Hall and Ramachandran were among the first to advocate WebAssembly as the enabling technology to avoid the overhead of containers, which would substantially weigh on the limited hardware resources of edge computing environments [10]. They presented a serverless platform that runs WebAssembly code within the V8 JavaScript engine for execution and sandboxing of functions. Differently from FunLess they use a NodeJS runtime which embeds V8 for the execution of WebAssembly code. As the authors note [10],\\nthe nesting of these layers takes a conspicuous toll on the performance of the system.\\n\\nGadepalli et al. [11] use WebAssembly to run and sandbox serverless functions. They target only single-host deployments, requiring the deployment of the entire platform on one node only. Moreover, they do not support WASI [14], thus making their system potentially less portable.\\n\\nGackstatter et al. [25] propose WOW, a WebAssemblybased runtime environment for serverless edge computing integrated within the Apache OpenWhisk platform. The authors introduce a new layer between OpenWhisk and different Wasm runtimes which enable the execution of Wasm functions.\\n\\nCompared to FunLess, WOW requires the deployment of a full installation (of a custom version) of the OpenWhisk platform which precludes the installation of the controller to low-power and memory-restricted edge devices.16 Lucet [27] was used by Fastly to run Wasm on their commercial Compute platform. Lucet translated WebAssembly to native code, which was then executed using Lucet's runtime also on edge devices. Unfortunately, Lucet has reached endof-life and is no longer maintained. Cloudflare Workers [28]\\nis also a commercial serverless platform that supports the possibility of defining functions in Wasm and has native support for WASI since 2022.17 Although the runtime part of this project has recently been made open-source,18, the serverless platform is proprietary and closed-source.\\n\\nIt is worth mentioning the work by Shillaker and Pietzuch [29] that, tangential to our proposal, concerns a Wasmbased serverless runtime that uses Wasm to achieve state sharing across functions—they allow the execution of functions that share memory regions in the same address space for possible performance benefits. On a similar note, Zhao et al. [30] present an OpenWhisk extension for confidential serverless computing that integrates a Wasm runtime. The authors propose a solution to construct reusable enclaves that enable rapid enclave reset and robust security to reduce cold start times. Although these kinds of proposals are orthogonal to FunLess, we see them as interesting future optimisations that the usage of a Wasm function runtime can unlock for FunLess.\\n\\nKjorveziroski and Filiposka [31] focus on serverless orchestration using Wasm and introduce a variant of Kubernetes that can orchestrate Wasm modules that are executed without containers. Interestingly, also Kjorveziroski and Filiposka report that Wasm tasks enjoy faster deployment times (two-fold) and at least one order of magnitude smaller artefact sizes, while still offering comparable execution performance.\"\n",
      "\n",
      "\n",
      "\n",
      "'# Small Language Models For Application Interactions: A Case Study\\n\\nBeibin Li, Yi Zhang, Sebastien Bubeck, Jeevan Pathuri, Ishai Menache ´\\nMicrosoft, Redmond May 2024\\n\\n## Abstract\\n\\nWe study the efficacy of Small Language Models (SLMs) in facilitating application usage through natural language interactions. Our focus here is on a particular internal application used in Microsoft for cloud supply chain fulfilment. Our experiments show that small models can outperform much larger ones in terms of both accuracy and running time, even when fine-tuned on small datasets. Alongside these results, we also highlight SLM-based system design considerations.\\n\\n## 1 Introduction\\n\\nLarge Language Models (LLMs) are becoming pervasive in assisting humans with a wide variety of tasks, such as writing documents, presenting work, coding and health assistant. Generative LLMs are being rapidly integrated in user-facing software, for answering questions and increasing productivity through simple, language based interactions with technology. One of the key operating principles behind LLMs is exploiting their ability to generalize to unseen tasks by providing examples through the prompt itself - an approach commonly known as in-context learning. While LLMs are being designed to support larger prompt sizes, processing very large prompts might be expensive and incur non-negligible latencies.\\n\\nIn this paper, we consider the alternative of using Small Language Models (SLMs), which are being developed nowadays and open-sourced by several companies. While SLMs may fall behind LLMs for general unseen tasks, we examine using fine-tuned versions thereof to handle a specific and *fixed* set of *tasks*. In particular, we consider using SLMs in order to enable natural-language interactions with a given *application*; an application can be simply viewed as software that includes a set of functions (or APIs) that are invoked by the user to carry out different requirements.\\n\\nOrganizations deploy customized applications in a variety of sectors, including healthcare [1], supply chains\\n[2], and agriculture [3]. Our focus here on a particular application that has been developed in Microsoft for managing the server fulfillment process for Azure\\'s supply chain - namely, the process of sending hardware from warehouses to the data centers. The APIs of this application include adding business and operational constraints for fulfilment, generating a fulfillment plan and extracting details and insights about the plan. We build a language model system on top of this application to enable users to accomplish essential tasks, each of which utilizing one or more APIs. For example, users may interact with our system in plain English to generate a new plan, or answer what-if queries (e.g., \"what are the implications of disabling a certain warehouse\"?). In this paper, we experiment with different language model technologies that can be used in our system. Our experiments show that SLMs such as Phi-3, Llama 3 and Mistral v0.2 can achieve higher degrees of accuracy than state-of-the-art LLMs, while providing outputs significantly faster. Similar observations regarding the potential benefits of SLMs have been recently reported in [4] for a variety of benchmarks. Importantly, our paper shows that SLMs may reach these superior capabilities with a modest size of training data (see Figure 1).\\n\\n![1_image_0.png](1_image_0.png)\\n\\nOur results, while corresponding to only a single case study, may point to an attractive option of using SLMs for interacting with applications. Aside from the potential performance benefits, the SLMs can be hosted locally on a single machine. Such system architectures can be particularly appealing for edge computations (in warehouses, farms, vehicles, etc.) where data transfers and Internet connectivity might be a bottleneck.\\n\\n## 2 Background And Motivation 2.1 Small Language Models'\n",
      "\n",
      "\n",
      "\n",
      "\"Finally, Tzenetopoulos et al. [32] analyse the performance of *Lean OpenWhisk*, an edge-focused variant of the Apache OpenWhisk serverless platform. Their variant of the platform coalesces the scheduling and execution components in a single entity, remove the message broker (Apache Kafka) from the deployment, and introduce changes to reduce OpenWhisk's overhead, making it better suited for resource-constrained devices. Unfortunately, we could not perform a quantitative comparison of Lean OpenWhisk because the project seems no longer maintained, and we could not deploy the platform due to problems linked to years-old container images (NodeJS\\n6) and outdated dependencies that reached the end of life (Ansible 2.7.9).\\n\\n## Vi. Conclusion\\n\\nWe present FunLess, a FaaS platform tailored to respond to recent trends in serverless computing that advocate for extending FaaS to cover private edge cloud systems, including Internet-of-Things devices. The motivation behind the shift towards private edge cloud systems includes reduced latency, enhanced security, and improved resource usage. Unlike existing solutions that rely on containers and container orchestration technologies for function invocation, FunLess leverages Wasm as its function-execution runtime environment. The reason behind this choice is to reduce performance overheads that can prevent resource-constrained devices from running FaaS\\nsystems. Wasm's fundamental feature exploited by FunLess is its lightweight, sandboxed runtime, which allows the platform to run efficiently functions in isolation on constrained devices at the edge. Thus, Wasm provides a portable, homogeneous way for developers to implement and deploy their functions among clusters of heterogeneous devices (write once, run everywhere), simplifying platform deployments, offering flexibility in deployment options, and mitigating cold start issues.\\n\\nTo validate FunLess, we consider different deployment scenarios spanning from the pure edge case to the pure cloud one, with/without container orchestration technologies to ease the deployment. To draw our comparison, we select three alternatives from production-ready, widely adopted opensource FaaS platforms—OpenFaaS, Fission, and Knative—and run representative cloud and edge FaaS benchmarks.\\n\\nOur benchmarks confirm that FunLess is a viable solution for FaaS private edge cloud systems. Our platform outperforms the considered alternatives in terms of memory footprint and support for heterogeneous devices.\\n\\nAs future work, we plan to integrate the new versions of Wasmtime and, with it, native support for HTTP and other optimisations and features of the new releases and support for the WASI runtime. Indeed, many current Wasm runtime implementations miss features like interface types, networking support in WASI multi-threading, atomics, and garbage collectors. Besides Wasmtime, other projects are developing new, optimised, and extended Wasm runtimes, which FunLess can leverage to increase its performance (and adapt it to different application contexts). As an example, we conjecture that by using a Wasm runtime that natively supports HTTP\\nrequests, FunLess will perform better in the network benchmark (c.f. Section IV) while the support for garbage collection can support improved JavaScript runtimes and increase the performance of the matrix multiplication benchmark.\\n\\nWe also plan to improve the reliability of the platform, allowing the support of retry policies for failed invocations, at-least-once message delivery, and the replication of *Core* components. Following the principles of simplicity and versatility that guided the development of FunLess, we propose to tackle these extensions as optional features to support flexible deployments, adaptable to the different application contexts\\n(cloud, edge, on resource-constrained devices).\"\n",
      "\n",
      "\n",
      "\n",
      "'## Ii. Preliminaries\\n\\nWe dedicate this section to providing the preliminary notions useful to contextualise our contribution. Specifically, we introduce WebAssembly—the technology underpinning the FunLess execution runtime (cf. Section III)—and briefly present the serverless platforms we compare against in our evaluation (cf. Section IV).\\n\\n## A. Webassembly\\n\\nThe WebAssembly [13] technology, Wasm for short, is a W3C standard since 2019, maintained with contributions from Apple, Google, Microsoft, Mozilla, and other companies.\\n\\nThe idea behind Wasm is to provide a simple assemblylike instruction set which can run efficiently within a browser.\\n\\nAt its core, Wasm includes a binary instruction format and a stack-based Virtual Machine that supports functions and control flow abstractions like loops and conditionals.\\n\\nAlthough browsers are the main target of Wasm, recent initiatives, like WebAssembly System Interface [14] (WASI)\\nnormed the implementation of Wasm runtimes that support the execution of Wasm code outside the browser with a set of APIs that provide POSIX capabilities (e.g., file system, network, and process management). Some examples of WASIcompliant runtimes, either open-source or proprietary, are Wasmtime [15], Wasmer [16], and WasmEdge [17].\\n\\nFocussing on FaaS, Wasm provides a sandboxed runtime environment for functions, akin to containers. However, while one needs to build a container (for the same function) for each targeted architecture, the same Wasm binary can run on different architectures thanks to the hardware abstraction provided by the Wasm runtime. Moreover, Wasm binaries tend to be more lightweight than containers, thanks to the fact that they do not need to include a pre-packaged filesystem.\\n\\n## B. Alternative Open-Source Serverless Platforms\\n\\nAs part of our platform\\'s performance assessment, we select and characterise three popular alternative open-source serverless solutions we compare FunLess against.\\n\\nWe aimed to select widely adopted open-source solutions for serverless. These are not necessarily adapted for the private edge cloud case since, given the recent emergence of this case, there are no popular open-source proposals yet.\\n\\nTo select the candidates, we searched GitHub for the keyword \"faas\" (which, at the time of writing returned 3.9k matches) and we followed four inclusion criteria for the selection: production-ready (used in industry, verified by looking at the commercial testimonials found on the project\\'s webpages),\\npopular (above 5k stars on GitHub), actively developed (commits within the last quarter and with at least 100 contributors),\\nand able to run on both AMD64 and ARM hardware (i.e., the most common hardware found in the cloud and edge devices).\\n\\nWe ranked the results by popularity (GitHub stars) and selected the first three. The selected platforms, in ranking order, are OpenFaaS (24k+ stars), Fission (8k+ stars), and Knative (5k+\\nstars).6 We conclude this section with the main traits of the alternatives.\\n\\na) OpenFaaS: OpenFaaS builds on Kubernetes, and it takes advantage of Kubernetes\\' scheduler for function allocation and scaling—specifically, functions are pods, i.e., application containers that enclose the function\\'s code and runtime environment. Since pods are generic containers, OpenFaaS supports different languages by providing language-specific template containers with example source files that users can extend to implement their functions and include the necessary dependencies.'\n",
      "\n",
      "\n",
      "\n",
      "'b) Fission: Like OpenFaaS, also Fission builds on Kubernetes. However, Fission does not rely on user-built containers for functions, allowing users to directly upload their source code. Functions run through the use of \"environments\", which essentially define which pre-built containers the platforms shall use to run the functions\\' sources (as compiled binaries or via an interpreter). One of the strengths of Fission is the small cold-start times it affords for functions deployed as source code (i.e., not binary executables). To achieve this result, Fission initialises \"general-purpose\" containers for the language environment of the deployed functions (e.g., if the user deploys NodeJS functions, Fission prepares a pool of NodeJS containers at each execution machine). At function invocation, Fission uses one of these \"warm\" containers by injecting and running therein the code of the function.\\n\\nc) Knative: Knative is also a Kubernetes-based serverless platform. The main difference with OpenFaaS and Fission is that Knative adopts a more low-level approach to function development. Essentially, developers implement their functions as containerised microservices (the main contemporary programming style complementary to serverless for cloudbased distributed software [18]), which Knative executes in a serverless-like fashion (managing event-based allocation and scaling).\\n\\n## Iii. Platform Architecture\\n\\nWe present the principles and technologies behind FunLess, its architecture and discuss our design choices (trade-offs and limitations).\\n\\nThe main principles behind the design of FunLess are the simplicity of both function development and platform deployment and the flexibility of hardware and deployment automation. In particular, FunLess is independent of the underlying deployment orchestrators (if any), which avoids potential overheads and allows users to install the entire platform on resource-constrained, low-power edge devices. For the implementation of the platform, we used Elixir [19], which is a functional, concurrent, high-level general-purpose programming language that runs on the BEAM virtual machine [20]\\n6Resp. found at https://github.com/OpenFaaS/faas, https://github.com/\\nfission/fission, and https://github.com/knative. Note that the above selection lacks Apache OpenWhisk, a popular (6k+ stars), serverless platform which we excluded because the container images of its main components (Controller and Invoker) are not available for ARM architectures, thus precluding its installation on many edge devices.\\n\\n![3_image_0.png](3_image_0.png)\\n\\n(used by the Erlang language). Specifically, Elixir and the BEAM allowed us to simplify the creation and deployment of a distributed application without relying on container orchestration technologies, while retaining high performance, faulttolerance, and resilience (provided by the BEAM\\'s scheduler and lightweight processes, famous for being optimised for concurrent and distributed systems).\\n\\nWe represent in Figure 1 both the components that make up the platform\\'s architecture and the typical flow developers and users follow to create and invoke functions. Architecturewise, FunLess consists of mainly two components: the *Core* and the *Worker*, which we detail in the next parts of this section. Briefly, the *Core* acts as an user-facing API to i) create, fetch, update, and delete functions and ii) schedule functions on workers. The *Worker* is the component deployed on every node tasked to run the functions; in the remainder, we refer to these nodes as *Worker*s. In addition to these two components, FunLess includes a *Postgres* database to store functions and metadata and *Prometheus* to collect metrics from the platform.7 FunLess is an open-source project and both its source code [21] and documentation [22] are publicly available.'\n",
      "\n",
      "\n",
      "\n",
      "'In this paper, we address the challenge of supporting FaaS\\nin private edge cloud systems. Off-the-shelf solutions to this challenge consist of deploying popular open-source FaaS platforms (e.g., OpenFaas, Knative, Fission, OpenWhisk) on top of container orchestration technologies (e.g., Kubernetes). However, these technologies, which usually rely on containers and container orchestration solutions, entail performance and resource overheads which can create issues on devices with constrained resources—they might not have enough memory to host containers or computational power to effectively run functions in low-latency application contexts.\\n\\nThese problems motivated researchers and practitioners to consider alternatives and propose runtimes that provide the isolation and parallel execution of existing FaaS platforms yet mediate the heavy toll of the mentioned more complex runtimes. Examples of these proposals include using virtual machines like that of Java [5] and Python [6] or embedding functions in unikernels [7]. Unfortunately, while these solutions achieve the goal of reducing the overhead of containers, they respectively miss fundamental features. Java/Python VMs do not provide high-performing runtimes [8] and properly isolate functions (e.g., exposing the users to security risks). Unikernels are still a niche technology whose usage requires specific engineering knowledge (e.g., to define the minimal OS stack needed to run high-level functions).\\n\\nA promising alternative is leveraging WebAssembly3\\n(Wasm) for lightweight FaaS environments [9] (introduced in more detail in Section II). Indeed, Wasm comes with a stack-based virtual machine designed for running programs in a sandbox environment with performance close to native code and fast load times. Wasm proved to be a valid candidate for FaaS, providing lightweight sandboxing at the edge with 3https://webassembly.org/.\\n\\nboth small latencies and startup times [10], [11]—recently, providers like Cloudflare proposed closed-source solutions based on Wasm4.\\n\\nFunLess. Building on these encouraging results, we propose FunLess, a FaaS platform designed for private and hybrid edge-cloud systems powered by a Wasm-based function execution engine. The advantages of leveraging Wasm in FunLess are multifaceted:\\n- *Security.* Wasm\\'s inherent security and isolation mechanisms make it well-suited for scenarios where data integrity and confidentiality are critical.\\n\\n- *Memory and CPU footprint.* FunLess does not require a container runtime (e.g., Docker) and orchestrator (e.g.,\\nKubernetes). Hence, the \"bare-metal\" deployment of FunLess frees resources essential for running functions on memory-constrained or low-power edge devices.\\n\\n- *Cold starts.* FunLess leverages Wasm to mitigate the problem of cold starts [12], i.e., delays in function execution due to the overhead of loading and initialising functions; an issue that constrained-resource edge devices can accentuate. Indeed, cold-start mitigations usually rely on caching and even keeping \"warm\" function instances and the memory footprint of containers can make these solutions unfeasible on constrained-resource devices. Contrarily, the small size of Wasm functions makes caching (and even fetch-and-load roundtrips) affordable in FunLess. This possibility, together with the fact that Wasm runtimes are optimised for fast startup times (note that Wasm\\'s main use case is in-browser execution, where responsiveness is crucial) allow FunLess to achieve small cold-start overheads.\\n\\n- *Consistent function development and deployment environment.* Since Wasm abstracts away the hardware and environment it runs within, FunLess provides a consistent development and deployment experience across the diverse private edge architectures, offering a built-in solution for the challenges of variability in hardware and software environments of private edge-cloud scenarios.'\n",
      "\n",
      "\n",
      "\n",
      "\"# Decentralized Ai: Permissionless Llm Inference On Pokt Network\\n\\nDaniel Olshansky Grove Inc.\\n\\nRamiro Rodr´ıguez Colmeiro Pocket Scan Technologies LLC\\nBowen Li POKT Network Foundation Abstract**—POKT Network's decentralized Remote Procedure** Call (RPC) infrastructure, surpassing 740 billion requests since launching on MainNet in 2020, is well-positioned to extend into providing AI inference services with minimal design or implementation modifications. This litepaper illustrates how the network's open-source and permissionless design aligns incentives among model researchers, hardware operators, API providers and users whom we term model Sources, Suppliers, Gateways and Applications respectively. Through its Relay Mining algorithm, POKT creates a transparent marketplace where costs and earnings directly reflect cryptographically verified usage. This decentralized framework offers large model AI researchers a new avenue to disseminate their work and generate revenue without the complexities of maintaining infrastructure or building end-user products. Supply scales naturally with demand, as evidenced in recent years and the protocol's free market dynamics. POKT Gateways facilitate network growth, evolution, adoption, and quality by acting as application-facing load balancers, providing value-added features without managing LLM nodes directly. This vertically decoupled network, battle tested over several years, is set up to accelerate the adoption, operation, innovation and financialization of open-source models. It is the first mature permissionless network whose quality of service competes with centralized entities set up to provide application grade inference.\\n\\n## 1. Introduction 1.1. Llm Inference & Web3 Full Nodes\\n\\nThe advent of OpenAI's ChatGPT brought foundation models into the mainstream. With it, the ecosystem of finetuning, distributing, evaluating and optimizing models has become ubiquitous. Companies like Meta are training and open-sourcing [1] models ranging from 8B (small) to over 400B (large) parameters, often referred to as Language Models (LMs), Large Language Models (LLMs), or Large Multimodal Models (LMMs). Platforms like HuggingFace have become central hubs for sharing and discovering new models, hosting hundreds of thousands [2] of open-source models from institutions and independent researchers.\\n\\nAlthough some models can be hosted on personal devices [3], most AI engineers [4] rely on third-party services with less resource-constrained hardware for reliable and cost-effective inference maintained by dedicated teams.\\n\\nThese LLM API Providers [5] create a disjoint and inconsistent ecosystem that varies in models offered, APIs, tooling with little visibility into what drives their cost structure or how new models are added to their offering list.\\n\\nDelegating infrastructure maintenance to third parties has been common practice in Web3 for years. As the resource requirements for maintaining Full Nodes [6] increased - Solana [7] recommending a minimum of 512GB\\nof RAM for baseline functionality - the industry began relying on outsourced Remote Procedure Call (RPC) Nodes maintained by full-time DevOps teams. Today, there are dozens of major RPC providers [8], each servicing multiple blockchains.\\n\\n## 1.2. Pokt Network Background\\n\\nPOKT Network [9] has been live on MainNet since 2020, serving hundreds of millions of daily RPC requests [10] across dozens of blockchains via a heterogeneous and independent set of hardware operators. As a Decentralized Physical Infrastructure Network (DePIN), its permissionless and incentive-driven economics drive the organic addition and weeding out of supported blockchains based on customer demand and usage.\"\n",
      "\n",
      "\n",
      "\n",
      "\"Machinery.\\n              \\n[25] Albert Greenberg, James R. Hamilton, Navendu Jain, Srikanth Kandula,\\n                                                                  \\n    Changhoon Kim, Parantap Lahiri, David A. Maltz, Parveen Patel, and\\n                                                                 \\n    Sudipta Sengupta. Vl2: a scalable and flexible data center network.\\n                                                                  \\n    In Proceedings of the ACM SIGCOMM 2009 Conference on Data Communication, SIGCOMM '09, page 51–62, New York, NY, USA, 2009.\\n    Association for Computing Machinery.\\n                                       \\n[26] Mingyang Zhang, Radhika Niranjan Mysore, Sucha Supittayapornpong, and Ramesh Govindan. Understanding lifecycle management\\n                                                                 \\n    complexity of datacenter topologies. In 16th USENIX Symposium on\\n    Networked Systems Design and Implementation (NSDI 19), pages 235–\\n    254, Boston, MA, February 2019. USENIX Association.\\n                                                    \\n[27] Maciej Besta and Torsten Hoefler. Slim fly: A cost effective lowdiameter network topology. In SC '14: Proceedings of the International\\n    Conference for High Performance Computing, Networking, Storage and\\n    Analysis, pages 348–359, 2014.\\n[28] Vincent Liu, Daniel Halperin, Arvind Krishnamurthy, and Thomas\\n                                                                 \\n    Anderson. F10: A Fault-Tolerant engineered network. In 10th USENIX\\n    Symposium on Networked Systems Design and Implementation (NSDI\\n    13), pages 399–412, Lombard, IL, April 2013. USENIX Association.\\n[29] Chuanxiong Guo, Guohan Lu, Dan Li, Haitao Wu, Xuan Zhang, Yunfeng Shi, Chen Tian, Yongguang Zhang, and Songwu Lu. Bcube: a\\n                                                                 \\n    high performance, server-centric network architecture for modular\\n                                                                 \\n    data centers. In Proceedings of the ACM SIGCOMM 2009 Conference on\\n    Data Communication, SIGCOMM '09, page 63–74, New York, NY, USA,\\n    2009. Association for Computing Machinery.\\n                                            \\n[30] Xia Zhou, Zengbin Zhang, Yibo Zhu, Yubo Li, Saipriya Kumar, Amin\\n                                                                 \\n    Vahdat, Ben Y. Zhao, and Haitao Zheng. Mirror mirror on the ceiling:\\n                                                                  \\n    flexible wireless links for data centers. SIGCOMM Comput. Commun.\\n    Rev., 42(4):443–454, aug 2012.\\n[31] Srikanth Kandula, Jitendra Padhye, and Paramvir Bahl. Flyways to\\n                                                                 \\n    de-congest data center networks. In HotNets. ACM SIGCOMM, 2009.\\n[32] Navid Hamedazimi, Zafar Qazi, Himanshu Gupta, Vyas Sekar, Samir R.\\n                                                                  \\n    Das, Jon P. Longtin, Himanshu Shah, and Ashish Tanwer. Firefly: a\\n                                                                 \\n    reconfigurable wireless data center fabric using free-space optics. In\\n                                                                 \\n    Proceedings of the 2014 ACM Conference on SIGCOMM, SIGCOMM '14,\\n    page 319–330, New York, NY, USA, 2014. Association for Computing\\n                                                                 \\n    Machinery.\\n              \\n[33] Li Chen, Kai Chen, Zhonghua Zhu, Minlan Yu, George Porter, Chunming Qiao, and Shan Zhong. Enabling Wide-Spread communications\\n                                                                 \\n    on optical fabric with MegaSwitch. In 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17), pages 577–593,\\n    Boston, MA, March 2017. USENIX Association.\\n                                              \\n[34] Yunpeng James Liu, Peter Xiang Gao, Bernard Wong, and Srinivasan\"\n",
      "\n",
      "\n",
      "\n",
      "\"The application. We consider the fulfillment management application which has been designed internally at Microsoft for managing cloud supply chain fulfillment, a critical operational phase in cloud resource management in light of the constant growth of the cloud (see [20] and references therein); we henceforth refer to that application simply as App, as our design methodology is general and can be applied to other applications. At a high level, App manages the decision making process for satisfying the demand for cloud hardware. A demand request comes from internal Microsoft organizations, such as Microsoft 365 or Azure, and it is specified in racks of server units, alongside a desired dock (or deployment) date. App is in charge of generating a *fulfilment plan* for a configurable time-horizon (typically in the order of weeks). For each demand request, the plan includes (i) *supplier.* The hardware supplier (including warehouse location) that will be used to fulfill the demand, (ii) *Shipping.* The shipping method that will be used to transfer the racks of hardware from the warehouse, and the target datacenter that will host these racks, and (iii) *Scheduling.* The timing of the shipment; note that the a combination of shipping and scheduling determines the dock date. App uses an optimization algorithm to automatically generate a plan while accounting for different business and operational constraints [20]. Motivation behind language models. App is consumed by *planners* who periodically generate plans and oversee that the execution of the decisions is completed as planned. Planners may be interested in obtaining insights from historical plan data, understanding certain decisions, as well asking what-if questions. What-if questions may correspond to understanding the potential impact on the plan if certain conditions are changed (e.g., a supplier becomes unavailable, a certain shipping date must be enforced, etc.). Before the LLM disruption, planners would interact with engineering teams for understanding issues and answering what-if questions [20]. As we elaborate below, the role of language models is to facilitate the planners' job by enabling direct and efficient interaction with App.\\n\\nTasks. App may be used in order to carry out different *tasks*. The task types can be divided into three categories:\\ndata extraction, plan generation, and what-if analysis. Each task is carried out through one or more APIs (App has over forty different APIs). We provide some task examples below. Data extraction:\\n- [supplier] What is the standard deviation of supplier S's inventory in the last T weeks? - [shipping] What was the fraction of cross-geographical shipments in the last T weeks? - [scheduling] Will demand D be deployed in its ideal dock date?\\n\\n## Plan Generation:\\n\\n- Optimize plan while taking into account a set of new constraints C.\\n\\n## What-If Analysis:\\n\\n- [supplier] What are the cost implications of not using suppliers from region R on Month M?\\n\\n- [shipping] By how much would a plan cost increase if we force priority shipping for demand D? - [scheduling] Is it possible to dock demand D in its ideal dock-date?\"\n",
      "\n",
      "\n",
      "\n",
      "'| human           | Llama2 7B   | Falcon 7B   | OPT 7B   | Mistral 7B   | llama 7B (base)   |       |\\n|-----------------|-------------|-------------|----------|--------------|-------------------|-------|\\n| Llama2 7B       | 1.086       | 0.003       | 0.001    | 0.003        | 0.005             | 0.006 |\\n| Falcon 7B       | 0.388       | 0.006       | 0.000    | 0.001        | 0.009             | 0.007 |\\n| OPT 7B          | 0.485       | 0.006       | 0.001    | 0.000        | 0.009             | 0.009 |\\n| Mistral 7B      | 0.964       | 0.004       | 0.002    | 0.003        | 0.003             | 0.007 |\\n| llama 7B (base) | 1.022       | 0.005       | 0.001    | 0.002        | 0.008             | 0.007 |\\n\\nTable 11: Evaluation of instruction based LLMs on the Wikipedia test dataset. These LLMs are instruction tuned, except for Llama 7b (base).\\n\\nTable 12: Evaluation of the proposed SPOT method on LLMs tackling medical related prompts.\\n\\nhuman Llama 7B Falc 7B Dv2 3B GPT-J 6B Mistral 7B OPT 6.7B\\n\\nLlama 7B 0.695 0.004 0.002 0.005 0.005 0.007 0.009\\n\\nFalcon 7B 0.562 0.007 0.000 0.005 0.002 0.006 0.036\\n\\nDv2 3B 0.595 0.034 0.005 0.003 0.005 0.027 0.024\\n\\nGPT-J 6B 0.574 0.019 0.003 0.005 0.002 0.016 0.003\\n\\nMistral 7B 0.681 0.007 0.002 0.005 0.003 0.007 0.011\\n\\nOPT 6.7B 0.621 0.024 0.002 0.005 0.004 0.016 0.001\\n\\nTable 13: Evaluation on actual emails sent and received by workers.\\n\\nhuman Llama 7B Falc 7B Dv2 3B GPT-J 6B Mistral 7B OPT 6.7B\\n\\nllama 7B 1.073 0.004 0.002 0.005 0.005 0.007 0.009\\n\\nfalcon 7B 0.307 0.007 0.000 0.005 0.002 0.006 0.036\\n\\ndollyv2 3B 0.527 0.034 0.005 0.003 0.005 0.027 0.024\\n\\nGPT-J 6B 0.349 0.019 0.003 0.005 0.002 0.016 0.003\\n\\nMistral 7B 1.057 0.007 0.002 0.005 0.003 0.007 0.011\\n\\nOPT 6.7B 0.394 0.024 0.002 0.005 0.004 0.016 0.001\\n\\nhuman and LLMs through SPOT. However, this rule was under the assumption of working with limited support distribution. Consequently, our results on medical data could be attributed to their low representation in LLMs training sets with respect to the diversity of possible medical content. In summary, SPOT is sensitive to the difficulty or specificity of the task for which the text is generated as long as the models are specifically trained for said tasks. Nonetheless, we argue that this property does not prevent the usage of SPOT in generic and broader practical applications such as e-mail filtering.\\n\\n## 7 3.4 Application To Mails\\n\\nIn Table 13, we evaluate SPOT on actual examples of mails. We observe a similar behaviour as on Wikipedia. This suggests that the diversity in possible mails, *i.e.* the spread of distribution, is large enough to enable SPOT to discern human made mails from LLM ones. However, we remark that the results are less stable across different LLM architectures, as Falcon 7B and GPT-J 6B have a harder time classifying text inputs. This phenomenon cannot be attributed to their higher performance in terms of perplexity, as they are both outperformed by Mistral 7B. Similarly, we can rule out the explanation regarding training data, as none of these models were specifically trained on such messages. The reason as to why the difference in performance between models remains an open question to us. Still, SPOT manages to attribute an originality scores 8.53× to 119.22× larger to humans than to LLMs. Consequently, we suggest considering SPOT for practical applications.\\n\\nHowever, such applications often involve neural network compression, which we have not considered so far.\\n\\n## 3.5 Robustness To Deployment Conditions\\n\\nIn Table 14, we evaluate Llama models compressed using the popular OPTQ [26] quantization technique. This method depends on two hyperparameters: the bit-width (which defines the compression rate) and the group-size (the larger the group size, the higher the fidelity to the uncompressed model).'\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def read_source_from_key(source_path, key):\n",
    "    chunk_id = key.split(\"_\")[-1]\n",
    "    # file_id = key remove \"_{chunk_id}\"\n",
    "    file_id = key[:-(len(chunk_id) + 1)]\n",
    "    # print(chunk_id, file_id)\n",
    "    with open(source_path + file_id + \"_chunks.csv\", \"r\") as f:\n",
    "        # read #chunk_id line from the file\n",
    "        reader = csv.reader(f)\n",
    "        for i, row in enumerate(reader):\n",
    "            if i == int(chunk_id):\n",
    "                return file_id, row[1]\n",
    "        \n",
    "    \n",
    "for key, distance in question_result:\n",
    "    # print(key, distance)\n",
    "    source = read_source_from_key(\"../data/index/index_2024-06-03/\", key)\n",
    "    print(source[1])\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "darxiv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
